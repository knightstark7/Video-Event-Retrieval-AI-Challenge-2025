{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6764bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from scenedetect import detect, AdaptiveDetector, split_video_ffmpeg\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_FOLDER = '/mnt/d/AI Challenge/Data/video'\n",
    "SCENE_FOLDER = '/mnt/d/AI Challenge/Data/scene'\n",
    "os.makedirs(SCENE_FOLDER, exist_ok=True)\n",
    "video_list = glob.glob(os.path.join(DOWNLOAD_FOLDER, '*/*.mp4'))\n",
    "len(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f38a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /mnt/d/AI Challenge/Data/video/L21/L21_V001.mp4 (1/4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Detected: 55 | Progress:  21%|██        | 7871/37849 [00:04<00:16, 1778.82frames/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(video_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     scene_list = \u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAdaptiveDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     df = pd.DataFrame(\n\u001b[32m     13\u001b[39m         [\n\u001b[32m     14\u001b[39m             {\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m         columns=[\u001b[33m\"\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mend\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     21\u001b[39m     )\n\u001b[32m     23\u001b[39m     out_name = os.path.splitext(os.path.basename(video_path))[\u001b[32m0\u001b[39m] + \u001b[33m\"\u001b[39m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Video-Event-Retrieval-AI-Challenge-2025/.venv/lib/python3.12/site-packages/scenedetect/__init__.py:165\u001b[39m, in \u001b[36mdetect\u001b[39m\u001b[34m(video_path, detector, stats_file_path, show_progress, start_time, end_time, start_in_scene)\u001b[39m\n\u001b[32m    163\u001b[39m scene_manager = SceneManager(StatsManager() \u001b[38;5;28;01mif\u001b[39;00m stats_file_path \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    164\u001b[39m scene_manager.add_detector(detector)\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m \u001b[43mscene_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetect_scenes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scene_manager.stats_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    171\u001b[39m     scene_manager.stats_manager.save_to_csv(csv_file=stats_file_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Video-Event-Retrieval-AI-Challenge-2025/.venv/lib/python3.12/site-packages/scenedetect/scene_manager.py:1342\u001b[39m, in \u001b[36mSceneManager.detect_scenes\u001b[39m\u001b[34m(self, video, duration, end_time, frame_skip, show_progress, callback, frame_source)\u001b[39m\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1341\u001b[39m     frame_im = next_frame\n\u001b[32m-> \u001b[39m\u001b[32m1342\u001b[39m new_cuts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mframe_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_im\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1344\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m new_cuts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Video-Event-Retrieval-AI-Challenge-2025/.venv/lib/python3.12/site-packages/scenedetect/scene_manager.py:1189\u001b[39m, in \u001b[36mSceneManager._process_frame\u001b[39m\u001b[34m(self, frame_num, frame_im, callback)\u001b[39m\n\u001b[32m   1187\u001b[39m \u001b[38;5;28mself\u001b[39m._frame_buffer = \u001b[38;5;28mself\u001b[39m._frame_buffer[-(\u001b[38;5;28mself\u001b[39m._frame_buffer_size + \u001b[32m1\u001b[39m) :]\n\u001b[32m   1188\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m detector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._detector_list:\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m     cuts = \u001b[43mdetector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_im\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1190\u001b[39m     \u001b[38;5;28mself\u001b[39m._cutting_list += cuts\n\u001b[32m   1191\u001b[39m     new_cuts = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m cuts \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Video-Event-Retrieval-AI-Challenge-2025/.venv/lib/python3.12/site-packages/scenedetect/detectors/adaptive_detector.py:133\u001b[39m, in \u001b[36mAdaptiveDetector.process_frame\u001b[39m\u001b[34m(self, frame_num, frame_img)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Process the next frame. `frame_num` is assumed to be sequential.\u001b[39;00m\n\u001b[32m    120\u001b[39m \n\u001b[32m    121\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m \u001b[33;03m    or more frames in the list, and not necessarily the same as frame_num.\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# TODO(#283): Merge this with ContentDetector and turn it on by default.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_num\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_img\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Initialize last scene cut point at the beginning of the frames of interest.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._last_cut \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Video-Event-Retrieval-AI-Challenge-2025/.venv/lib/python3.12/site-packages/scenedetect/detectors/content_detector.py:202\u001b[39m, in \u001b[36mContentDetector.process_frame\u001b[39m\u001b[34m(self, frame_num, frame_img)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_frame\u001b[39m(\u001b[38;5;28mself\u001b[39m, frame_num: \u001b[38;5;28mint\u001b[39m, frame_img: numpy.ndarray) -> List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    191\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Process the next frame. `frame_num` is assumed to be sequential.\u001b[39;00m\n\u001b[32m    192\u001b[39m \n\u001b[32m    193\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    200\u001b[39m \u001b[33;03m        or more frames in the list, and not necessarily the same as frame_num.\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28mself\u001b[39m._frame_score = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_calculate_frame_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._frame_score \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    204\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Video-Event-Retrieval-AI-Challenge-2025/.venv/lib/python3.12/site-packages/scenedetect/detectors/content_detector.py:156\u001b[39m, in \u001b[36mContentDetector._calculate_frame_score\u001b[39m\u001b[34m(self, frame_num, frame_img)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calculate score representing relative amount of motion in `frame_img` compared to\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[33;03mthe last time the function was called (returns 0.0 on the first call).\"\"\"\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# TODO: Add option to enable motion estimation before calculating score components.\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# TODO: Investigate methods of performing cheaper alternatives, e.g. shifting or resizing\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# the frame to simulate camera movement, using optical flow, etc...\u001b[39;00m\n\u001b[32m    154\u001b[39m \n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Convert image into HSV colorspace.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m hue, sat, lum = cv2.split(\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOLOR_BGR2HSV\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Performance: Only calculate edges if we have to.\u001b[39;00m\n\u001b[32m    159\u001b[39m calculate_edges: \u001b[38;5;28mbool\u001b[39m = (\u001b[38;5;28mself\u001b[39m._weights.delta_edges > \u001b[32m0.0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stats_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Detected: 57 | Progress:  21%|██        | 8040/37849 [00:19<00:16, 1778.82frames/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scenedetect import detect, AdaptiveDetector\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for i, video_path in enumerate(video_list, 0):\n",
    "    print(f\"Processing: {video_path} ({i+1}/{len(video_list)})\")\n",
    "    try:\n",
    "        scene_list = detect(video_path, AdaptiveDetector(), show_progress=True)\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"start\": round(start.get_seconds(), 2),\n",
    "                    \"end\": round(end.get_seconds(), 2),\n",
    "                }\n",
    "                for start, end in scene_list\n",
    "            ],\n",
    "            columns=[\"start\", \"end\"]\n",
    "        )\n",
    "\n",
    "        out_name = os.path.splitext(os.path.basename(video_path))[0] + \".csv\"\n",
    "        out_path = os.path.join(SCENE_FOLDER, out_name)\n",
    "        df.to_csv(out_path, index=False)\n",
    "\n",
    "        print(f\"Saved: {out_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {video_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
