{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12770694,"sourceType":"datasetVersion","datasetId":8073301}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install open_clip_torch -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:31:12.665997Z","iopub.execute_input":"2025-08-15T08:31:12.666270Z","iopub.status.idle":"2025-08-15T08:32:27.889235Z","shell.execute_reply.started":"2025-08-15T08:31:12.666251Z","shell.execute_reply":"2025-08-15T08:32:27.888132Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport open_clip\nimport glob\nfrom collections import defaultdict\n\n# --- Configuration ---\nbase_dir = '/kaggle/input/keyframe-real/keyframes'   # Each folder = 1 video\noutput_dir = '/kaggle/working/features'    # Save 1 .npy per image\nos.makedirs(output_dir, exist_ok=True)\n\n# --- Load OpenCLIP ---\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel, _, preprocess = open_clip.create_model_and_transforms(\n    model_name='ViT-H-14-quickgelu',\n    pretrained='dfn5b',\n    device=device\n)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:56:43.821883Z","iopub.execute_input":"2025-08-15T08:56:43.822122Z","iopub.status.idle":"2025-08-15T08:56:56.673697Z","shell.execute_reply.started":"2025-08-15T08:56:43.822106Z","shell.execute_reply":"2025-08-15T08:56:56.673069Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"CLIP(\n  (visual): VisionTransformer(\n    (conv1): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), bias=False)\n    (patch_dropout): Identity()\n    (ln_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    (transformer): Transformer(\n      (resblocks): ModuleList(\n        (0-31): 32 x ResidualAttentionBlock(\n          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n          )\n          (ls_1): Identity()\n          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n          )\n          (ls_2): Identity()\n        )\n      )\n    )\n    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  )\n  (transformer): Transformer(\n    (resblocks): ModuleList(\n      (0-23): 24 x ResidualAttentionBlock(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n        )\n        (ls_1): Identity()\n        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n        )\n        (ls_2): Identity()\n      )\n    )\n  )\n  (token_embedding): Embedding(49408, 1024)\n  (ln_final): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"videos = glob.glob(os.path.join(base_dir,'*','*','*.jpg'))\nif not videos:\n    print(\"[ERROR] No videos found.\")\n\nprint(f\"[INFO] Found {len(videos)} videos to process.\")\nvideos[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:38:55.019578Z","iopub.execute_input":"2025-08-15T08:38:55.020109Z","iopub.status.idle":"2025-08-15T08:38:55.317726Z","shell.execute_reply.started":"2025-08-15T08:38:55.020089Z","shell.execute_reply":"2025-08-15T08:38:55.317163Z"}},"outputs":[{"name":"stdout","text":"[INFO] Found 17585 videos to process.\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"['/kaggle/input/keyframe-real/keyframes/L26/L26_V215/L26_V215_2449.jpg',\n '/kaggle/input/keyframe-real/keyframes/L26/L26_V215/L26_V215_5149.jpg',\n '/kaggle/input/keyframe-real/keyframes/L26/L26_V215/L26_V215_124.jpg',\n '/kaggle/input/keyframe-real/keyframes/L26/L26_V215/L26_V215_2388.jpg',\n '/kaggle/input/keyframe-real/keyframes/L26/L26_V215/L26_V215_5033.jpg',\n '/kaggle/input/keyframe-real/keyframes/L26/L26_V215/L26_V215_5749.jpg',\n '/kaggle/input/keyframe-real/keyframes/L26/L26_V215/L26_V215_7055.jpg',\n '/kaggle/input/keyframe-real/keyframes/L26/L26_V215/L26_V215_6209.jpg',\n '/kaggle/input/keyframe-real/keyframes/L26/L26_V215/L26_V215_2773.jpg',\n '/kaggle/input/keyframe-real/keyframes/L26/L26_V215/L26_V215_5269.jpg']"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"batch_size=256\n\nfolder_to_images = defaultdict(list)\nfor img_path in videos:\n    folder_to_images[os.path.dirname(img_path)].append(img_path)\n\nfor folder_path, image_files in tqdm(folder_to_images.items(), desc=\"Processing folders\"):\n    # Create matching output subfolder\n    rel_path = os.path.relpath(folder_path, base_dir)\n    save_folder = os.path.join(output_dir, rel_path)\n    os.makedirs(save_folder, exist_ok=True)\n\n    # Sort image paths for consistent processing\n    image_files = sorted(image_files)\n\n    # Process in batches\n    for i in range(0, len(image_files), batch_size):\n        batch_files = image_files[i:i + batch_size]\n        batch_images = []\n        valid_files = []\n\n        for img_path in batch_files:\n            try:\n                image = Image.open(img_path).convert('RGB')\n                image_tensor = preprocess(image)\n                batch_images.append(image_tensor)\n                valid_files.append(img_path)\n            except Exception as e:\n                print(f\"Error loading {img_path}: {e}\")\n\n        if not batch_images:\n            continue\n\n        # Encode features\n        input_tensor = torch.stack(batch_images).to(device)\n        with torch.no_grad():\n            features = model.encode_image(input_tensor)\n            features = features / features.norm(dim=-1, keepdim=True)\n\n        # Save each feature with same relative path\n        for img_path, feat in zip(valid_files, features):\n            rel_img_path = os.path.relpath(img_path, base_dir)\n            save_path = os.path.join(output_dir, os.path.splitext(rel_img_path)[0] + '.npy')\n            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n            np.save(save_path, feat.cpu().numpy())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T08:57:12.290532Z","iopub.execute_input":"2025-08-15T08:57:12.290828Z"}},"outputs":[{"name":"stderr","text":"Processing folders:   8%|▊         | 14/175 [01:37<17:33,  6.55s/it]","output_type":"stream"}],"execution_count":null}]}