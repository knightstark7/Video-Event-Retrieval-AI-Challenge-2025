{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fastapi uvicorn sentence-transformers open-clip-torch qdrant-client \\\n    llama-index llama-index-vector-stores-qdrant llama-index-core pyngrok -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:49:36.720692Z","iopub.execute_input":"2025-08-27T09:49:36.721233Z","iopub.status.idle":"2025-08-27T09:51:18.661290Z","shell.execute_reply.started":"2025-08-27T09:49:36.721208Z","shell.execute_reply":"2025-08-27T09:51:18.660333Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from fastapi import FastAPI, UploadFile, File, Form, Depends, APIRouter\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom qdrant_client import QdrantClient, models\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.core import VectorStoreIndex\nfrom collections import defaultdict\nimport heapq\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nimport re\nfrom pydantic import PrivateAttr\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom typing import List, Optional\nimport open_clip\nfrom pydantic import BaseModel\nimport json\nfrom PIL import Image\nimport hashlib\nimport io","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:51:18.662971Z","iopub.execute_input":"2025-08-27T09:51:18.663281Z","iopub.status.idle":"2025-08-27T09:51:51.690643Z","shell.execute_reply.started":"2025-08-27T09:51:18.663252Z","shell.execute_reply":"2025-08-27T09:51:51.690027Z"}},"outputs":[{"name":"stderr","text":"2025-08-27 09:51:35.202860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756288295.373799      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756288295.418959      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class CLIPEmbedding(BaseEmbedding):\n    _model = PrivateAttr()\n    _preprocess = PrivateAttr()\n    _tokenizer = PrivateAttr()\n    _device = PrivateAttr()\n\n    def __init__(self, model_name: str):\n        super().__init__()\n        self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self._model, _, self._preprocess = open_clip.create_model_and_transforms(\n            model_name='ViT-H-14-quickgelu',\n            pretrained='dfn5b',\n            device=self._device\n        )\n        self._tokenizer = open_clip.get_tokenizer('ViT-H-14-quickgelu')\n        self._model = self._model.to(self._device).eval()\n\n    # --- Text embeddings ---\n    def _encode_text(self, text: str) -> List[float]:\n        tokens = self._tokenizer([text]).to(self._device)\n        with torch.no_grad():\n            emb = self._model.encode_text(tokens)\n            emb = emb / emb.norm(dim=-1, keepdim=True) \n        return emb[0].cpu().numpy().tolist()\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        return self._encode_text(query)\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        return self._encode_text(text)\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        return self._get_text_embedding(text)\n\n    # --- Image embeddings ---\n    def _encode_image(self, image: Image.Image) -> List[float]:\n        image_tensor = self._preprocess(image).unsqueeze(0).to(self._device)\n        with torch.no_grad():\n            emb = self._model.encode_image(image_tensor)\n            emb = emb / emb.norm(dim=-1, keepdim=True)\n        return emb[0].cpu().numpy().tolist()\n        \n    def _get_image_embedding(self, image: Image.Image) -> List[float]:\n            return self._encode_image(image)\n    \n    async def _aget_image_embedding(self, image: Image.Image) -> List[float]:\n        return self._get_image_embedding(image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:51:51.691789Z","iopub.execute_input":"2025-08-27T09:51:51.692173Z","iopub.status.idle":"2025-08-27T09:51:51.704031Z","shell.execute_reply.started":"2025-08-27T09:51:51.692142Z","shell.execute_reply":"2025-08-27T09:51:51.703299Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class BGEEmbedding(BaseEmbedding):\n    _model: SentenceTransformer = PrivateAttr()\n\n    def __init__(self, model_name: str = \"BAAI/bge-small-en\", device: str = \"cpu\"):\n        super().__init__()\n        self._model = SentenceTransformer(model_name, device=device)\n        self._model = self._model.eval()\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        return self._model.encode(query).tolist()\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        return self._model.encode(text).tolist()\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        return self._get_text_embedding(text)\n\nclass DocumentEmbedding(BaseEmbedding):\n    _model: SentenceTransformer = PrivateAttr()\n\n    def __init__(self, model_name: str = \"dangvantuan/vietnamese-document-embedding\", device: str = \"cpu\"):\n        super().__init__()\n        self._model = SentenceTransformer(model_name, device=device, trust_remote_code=True)\n        self._model = self._model.eval()\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        return self._model.encode(query, convert_to_numpy=True).tolist()\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        return self._model.encode(text, convert_to_numpy=True).tolist()\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        return self._get_text_embedding(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:51:51.705717Z","iopub.execute_input":"2025-08-27T09:51:51.705897Z","iopub.status.idle":"2025-08-27T09:51:51.730772Z","shell.execute_reply.started":"2025-08-27T09:51:51.705883Z","shell.execute_reply":"2025-08-27T09:51:51.730030Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Translator:\n    def __init__(self, model_name: str = \"VietAI/envit5-translation\", device: str = 'cpu'):\n        self.device = device\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n\n    def _clean_prefix(self, text: str) -> str:\n        return re.sub(r\"^(en|vi)\\s*:\\s*\", \"\", text.strip(), flags=re.IGNORECASE)\n    \n    def translate(self, text: str, source_lang: str = \"en\", max_length: int = 128) -> str:\n        content = f\"{source_lang}: {text}\"\n        inputs = self.tokenizer(\n            content, \n            return_tensors=\"pt\", \n            truncation=True, \n            max_length=max_length).to(self.device)\n        with torch.no_grad():\n            outputs = self.model.generate(**inputs, max_length=max_length)\n        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        return self._clean_prefix(decoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:51:51.731578Z","iopub.execute_input":"2025-08-27T09:51:51.731830Z","iopub.status.idle":"2025-08-27T09:51:51.753634Z","shell.execute_reply.started":"2025-08-27T09:51:51.731798Z","shell.execute_reply":"2025-08-27T09:51:51.752811Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Configuration with Kaggle secrets support\nimport os\n\n# Use Kaggle secrets or fallback to hardcoded values\nQDRANT_URL = os.getenv('QDRANT_URL', \"https://09a6d049-00c4-4b77-8e95-1dcc9ea5df34.eu-west-1-0.aws.cloud.qdrant.io:6333\")\nQDRANT_API_KEY = os.getenv('QDRANT_API_KEY', \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.-ZPZib9FxehqbTuqxsk7QdVjBQd0LlQEq7dpjF1b4PI\")\nNGROK_AUTH_TOKEN = os.getenv('NGROK_AUTH_TOKEN', \"28k6uZmtZlrKVCzyVQTfjtRSIDd_6GHtfHcwNEojEk9WjkTmv\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")\n\nqdrant_client = QdrantClient(\n    url=QDRANT_URL,\n    api_key=QDRANT_API_KEY,\n)\n\nCORS_SETTINGS = {\n    \"allow_origins\": [\"*\"],\n    \"allow_credentials\": True,\n    \"allow_methods\": [\"*\"],\n    \"allow_headers\": [\"*\"],\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:51:51.754575Z","iopub.execute_input":"2025-08-27T09:51:51.754845Z","iopub.status.idle":"2025-08-27T09:51:52.216955Z","shell.execute_reply.started":"2025-08-27T09:51:51.754816Z","shell.execute_reply":"2025-08-27T09:51:52.216375Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n\nclip_embed_model = CLIPEmbedding(model_name='ViT-H-14-quickgelu')\nclip_vector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"Image\")\nclip_index = VectorStoreIndex.from_vector_store(vector_store=clip_vector_store, embed_model=clip_embed_model)\n\n# bge_embed_model = BGEEmbedding(model_name=\"AITeamVN/Vietnamese_Embedding_v2\", device=DEVICE)\nbge_embed_model = DocumentEmbedding(device=DEVICE)\nbge_vector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"Demo\")\nbge_index = VectorStoreIndex.from_vector_store(vector_store=bge_vector_store, embed_model=bge_embed_model)\n\ntranslator = Translator(device=DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:51:52.217661Z","iopub.execute_input":"2025-08-27T09:51:52.217872Z","iopub.status.idle":"2025-08-27T09:52:51.311966Z","shell.execute_reply.started":"2025-08-27T09:51:52.217855Z","shell.execute_reply":"2025-08-27T09:52:51.310956Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"open_clip_pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a53e9f1207ee43419c284f43baeda6ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"008b56d198624199af9576bef04e8d0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/171 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51a4a24cfd1a4ba9bb8ff4674b2bacf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f70740a1601349759c06490173c84496"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fab383547e4440f0879e58f297cf2e42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa4237a0198c425ab4a068f0562e8106"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72282fa4bb064425bfe2c367a2260ae8"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dangvantuan/Vietnamese_impl:\n- configuration.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"986b21f2ab9048c99e629753589bb212"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dangvantuan/Vietnamese_impl:\n- modeling.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c518e73cf464ade89e4a3f51d2d9ff5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b49efb0d61f34bbc96f8deb42213f945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d1ceb3505564c1491b94decbe5c02f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d38faa4fad04f88ad7c53badeab8cca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d585d9f98124e029ce920875278aeda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48949fe8989f4154aff7c6a1fdbf142c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78f1e1afec8d4ac6912ec1a0f50cc4f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f71f48f488f64c678c95a627b75f014c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50d814e7554a4f24b72bfe35f759186b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/721 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2dc477f9278443e90a7071a19fcb375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44a3204df2114224a343f650b1f73ced"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb02cfef01ed42cb9346501ae92db689"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def retrieve_frame(query: str, topK: int, mode: str = \"hybrid\", alpha: float = 0.5):\n\n    \n    if mode == \"clip\":\n        # CLIP-only search\n        prompt = translator.translate(query, source_lang=\"vi\")\n        clip_nodes = clip_index.as_retriever(similarity_top_k=topK).retrieve(prompt)\n        results = [\n            {\"image\": node.metadata[\"id\"].strip(), \"caption\": f\"{node.metadata['id']} | Score: {node.score:.2f}\"}\n            for node in clip_nodes\n        ]\n        return results\n    \n    elif mode == \"vintern\":\n        # Vintern-only search (using Demo collection)\n        bge_nodes = bge_index.as_retriever(similarity_top_k=topK).retrieve(query)\n        results = [\n            {\"image\": node.metadata[\"id\"].strip(), \"caption\": f\"{node.metadata['id']} | Score: {node.score:.2f}\"}\n            for node in bge_nodes\n        ]\n        return results\n    \n    else:\n        prompt = translator.translate(query, source_lang=\"vi\")\n        # Hybrid search (original logic)\n        bge_nodes = bge_index.as_retriever(similarity_top_k=topK).retrieve(query)\n        clip_nodes = clip_index.as_retriever(similarity_top_k=topK).retrieve(prompt)\n        \n        combined_scores = defaultdict(float)\n        for node in bge_nodes:\n            combined_scores[node.metadata[\"id\"]] += node.score * alpha\n\n        for node in clip_nodes:\n            combined_scores[node.metadata[\"id\"]] += node.score * (1 - alpha)\n\n        top_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:topK]\n\n        return [\n            {\"image\": video_id.strip(), \"caption\": f\"{video_id} | Score: {score:.2f}\"}\n            for video_id, score in top_results\n        ]\n\ndef retrieve_from_image(contents: bytes, topK: int):\n    image_id = hashlib.md5(contents).hexdigest()\n    image = Image.open(io.BytesIO(contents)).convert(\"RGB\")\n    vector_query = clip_embed_model._get_image_embedding(image)\n\n    clip_nodes = qdrant_client.search(\n        collection_name=\"Image\",\n        query_vector=vector_query,\n        limit=topK,\n        with_payload=True\n    )\n\n    results = [\n        {\n            \"image\": node.payload.get(\"id\", \"\").strip(),\n            \"caption\": f\"{node.payload.get('id', '')} | Score: {node.score:.2f}\"\n        }\n        for node in clip_nodes\n    ]\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:58:03.257774Z","iopub.execute_input":"2025-08-27T09:58:03.258312Z","iopub.status.idle":"2025-08-27T09:58:03.269995Z","shell.execute_reply.started":"2025-08-27T09:58:03.258272Z","shell.execute_reply":"2025-08-27T09:58:03.268982Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"app = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=CORS_SETTINGS[\"allow_origins\"],\n    allow_credentials=CORS_SETTINGS[\"allow_credentials\"],\n    allow_methods=CORS_SETTINGS[\"allow_methods\"],\n    allow_headers=CORS_SETTINGS[\"allow_headers\"],\n)\n\nrouter = APIRouter()\n\n@router.post(\"/search\")\nasync def api_search(\n    query: Optional[str] = Form(None),\n    topK: int = Form(...),\n    mode: str = Form(\"hybrid\"),\n    file: UploadFile = File(None)\n):\n    if mode == \"image\":\n        if file is None:\n            return {\"error\": \"No file uploaded for image mode\"}\n        contents = await file.read()\n        results = retrieve_from_image(contents=contents, topK=topK)\n    else:\n        if query is None:\n            return {\"error\": \"No query provided for text mode\"}\n        results = retrieve_frame(query=query, topK=topK, mode=mode)\n\n    return {\"results\": results}\n\napp.include_router(router)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:58:12.775414Z","iopub.execute_input":"2025-08-27T09:58:12.775742Z","iopub.status.idle":"2025-08-27T09:58:12.787404Z","shell.execute_reply.started":"2025-08-27T09:58:12.775721Z","shell.execute_reply":"2025-08-27T09:58:12.786436Z"}},"outputs":[{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 422 Unprocessable Entity\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b212363c4be42a08f80f998833fc476"}},"metadata":{}},{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/913444450.py:48: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n  clip_nodes = qdrant_client.search(\n","output_type":"stream"},{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07c02112aad1427fbc9fc75636a51ac7"}},"metadata":{}},{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\nINFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"OPTIONS /search HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ebe53e916b745c088b129830c8551e9"}},"metadata":{}},{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os, time, threading, socket\nfrom pyngrok import ngrok\nimport uvicorn\n\n# === Config ===\nPORT = 8000\nHOST = \"0.0.0.0\"\n\nNGROK_AUTH_TOKEN = os.getenv(\"NGROK_AUTH_TOKEN\", globals().get(\"NGROK_AUTH_TOKEN\"))\n\nif NGROK_AUTH_TOKEN:\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\nelse:\n    print(\"âš ï¸  NGROK_AUTH_TOKEN chÆ°a Ä‘Æ°á»£c thiáº¿t láº­p.\")\n\ndef is_port_in_use(port: int, host=\"127.0.0.1\") -> bool:\n    \"\"\"Check if a local TCP port is already in use.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        return s.connect_ex((host, port)) == 0\n\ndef run_server():\n    \"\"\"Run FastAPI server in background thread\"\"\"\n    uvicorn.run(app, host=HOST, port=PORT, log_level=\"info\")\n\n# 1) Start server only if not already running\nif not is_port_in_use(PORT):\n    server_thread = threading.Thread(target=run_server, daemon=True)\n    server_thread.start()\n    # Äá»£i server khá»Ÿi Ä‘á»™ng\n    time.sleep(1)\nelse:\n    print(f\"ğŸ” Server Ä‘Ã£ cháº¡y trÃªn http://localhost:{PORT}, bá» qua bÆ°á»›c khá»Ÿi Ä‘á»™ng.\")\n\n# 2) Reuse/clean tunnels\nfor t in ngrok.get_tunnels():\n    addr = (t.config or {}).get(\"addr\", \"\")\n    if str(PORT) in addr:\n        try:\n            ngrok.disconnect(t.public_url)\n        except Exception:\n            pass\n\ntry:\n    if len(ngrok.get_tunnels()) >= 3:\n        ngrok.kill()\n\n    tunnel = ngrok.connect(addr=PORT, proto=\"http\", bind_tls=True)  # sáº½ tráº£ vá» URL https\n    PUBLIC_URL = tunnel.public_url  # https://...\n    print(f\"ğŸŒ Public URL: {PUBLIC_URL}\")\n    print(f\"ğŸ“ API Documentation: {PUBLIC_URL}/docs\")\n    print(f\"ğŸ’¡ Local URL: http://localhost:{PORT}\")\n\n    # LÆ°u vÃ o biáº¿n global Ä‘á»ƒ dÃ¹ng sau\n    globals()[\"PUBLIC_URL\"] = PUBLIC_URL\n\nexcept Exception as e:\n    print(f\"âŒ ngrok tunnel failed: {e}\")\n    print(f\"ğŸ”§ Falling back to local access only: http://localhost:{PORT}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:52:56.049716Z","iopub.execute_input":"2025-08-27T09:52:56.050023Z","iopub.status.idle":"2025-08-27T09:52:59.097177Z","shell.execute_reply.started":"2025-08-27T09:52:56.049996Z","shell.execute_reply":"2025-08-27T09:52:59.096514Z"}},"outputs":[{"name":"stdout","text":"                                                                                                    \r","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [36]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"ğŸŒ Public URL: https://081d5c369305.ngrok-free.app\nğŸ“ API Documentation: https://081d5c369305.ngrok-free.app/docs\nğŸ’¡ Local URL: http://localhost:8000\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Test the API with public URL\nimport requests\nimport json\n\ndef test_search_api(query=\"con chÃ³ mÃ u Ä‘en\", topK=10, mode=\"hybrid\", use_public_url=True):\n    \"\"\"Test the search API with public or local URL\"\"\"\n    \n    # Use public URL if available, otherwise fallback to localhost\n    if use_public_url and 'PUBLIC_URL' in globals():\n        base_url = globals()['PUBLIC_URL']\n    else:\n        base_url = \"http://localhost:8000\"\n    \n    url = f\"{base_url}/search\"\n    payload = {\n        \"query\": query,\n        \"topK\": topK,\n        \"mode\": mode\n    }\n    \n    print(f\"ğŸ” Testing endpoint: {url}\")\n    print(f\"ğŸ¯ Search mode: {mode}\")\n    \n    try:\n        response = requests.post(url, json=payload, timeout=30)\n        if response.status_code == 200:\n            results = response.json()\n            print(f\"âœ… Search successful for query: '{query}'\")\n            print(f\"ğŸ“Š Found {len(results['results'])} results:\")\n            for i, result in enumerate(results['results'][:5], 1):\n                print(f\"  {i}. {result['caption']}\")\n            return results\n        else:\n            print(f\"âŒ Error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"âŒ Request failed: {str(e)}\")\n        return None\n\n# Test all search modes\nprint(\"ğŸ§ª Testing different search modes...\")\n\nprint(\"\\n1. Testing HYBRID mode:\")\ntest_results = test_search_api(mode=\"hybrid\")\n\nprint(\"\\n2. Testing CLIP-only mode:\")\ntest_results = test_search_api(mode=\"clip\")\n\nprint(\"\\n3. Testing VINTERN-only mode:\")\ntest_results = test_search_api(mode=\"vintern\")\n\n# Also provide curl commands for external testing\nif 'PUBLIC_URL' in globals():\n    public_url = globals()['PUBLIC_URL']\n    print(f\"\\nğŸ”§ External curl test commands:\")\n    print(f'# Hybrid mode:')\n    print(f'curl -X POST \"{public_url}/search\" -H \"Content-Type: application/json\" -d \\'{{\"query\": \"khung hÃ¬nh bÃ³ng Ä‘Ã¡\", \"topK\": 10, \"mode\": \"hybrid\"}}\\'')\n    print(f'# CLIP-only mode:')\n    print(f'curl -X POST \"{public_url}/search\" -H \"Content-Type: application/json\" -d \\'{{\"query\": \"khung hÃ¬nh bÃ³ng Ä‘Ã¡\", \"topK\": 10, \"mode\": \"clip\"}}\\'')\n    print(f'# Vintern-only mode:')\n    print(f'curl -X POST \"{public_url}/search\" -H \"Content-Type: application/json\" -d \\'{{\"query\": \"khung hÃ¬nh bÃ³ng Ä‘Ã¡\", \"topK\": 10, \"mode\": \"vintern\"}}\\'')\nelse:\n    print(\"\\nâš ï¸  No public URL available. Use local testing only.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T09:52:59.097998Z","iopub.execute_input":"2025-08-27T09:52:59.098277Z","iopub.status.idle":"2025-08-27T09:52:59.656901Z","shell.execute_reply.started":"2025-08-27T09:52:59.098251Z","shell.execute_reply":"2025-08-27T09:52:59.656302Z"}},"outputs":[{"name":"stdout","text":"ğŸ§ª Testing different search modes...\n\n1. Testing HYBRID mode:\nğŸ” Testing endpoint: https://081d5c369305.ngrok-free.app/search\nğŸ¯ Search mode: hybrid\nINFO:     35.229.68.54:0 - \"POST /search HTTP/1.1\" 422 Unprocessable Entity\nâŒ Error 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"topK\"],\"msg\":\"Field required\",\"input\":null}]}\n\n2. Testing CLIP-only mode:\nğŸ” Testing endpoint: https://081d5c369305.ngrok-free.app/search\nğŸ¯ Search mode: clip\nINFO:     35.229.68.54:0 - \"POST /search HTTP/1.1\" 422 Unprocessable Entity\nâŒ Error 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"topK\"],\"msg\":\"Field required\",\"input\":null}]}\n\n3. Testing VINTERN-only mode:\nğŸ” Testing endpoint: https://081d5c369305.ngrok-free.app/search\nğŸ¯ Search mode: vintern\nINFO:     35.229.68.54:0 - \"POST /search HTTP/1.1\" 422 Unprocessable Entity\nâŒ Error 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"topK\"],\"msg\":\"Field required\",\"input\":null}]}\n\nğŸ”§ External curl test commands:\n# Hybrid mode:\ncurl -X POST \"https://081d5c369305.ngrok-free.app/search\" -H \"Content-Type: application/json\" -d '{\"query\": \"khung hÃ¬nh bÃ³ng Ä‘Ã¡\", \"topK\": 10, \"mode\": \"hybrid\"}'\n# CLIP-only mode:\ncurl -X POST \"https://081d5c369305.ngrok-free.app/search\" -H \"Content-Type: application/json\" -d '{\"query\": \"khung hÃ¬nh bÃ³ng Ä‘Ã¡\", \"topK\": 10, \"mode\": \"clip\"}'\n# Vintern-only mode:\ncurl -X POST \"https://081d5c369305.ngrok-free.app/search\" -H \"Content-Type: application/json\" -d '{\"query\": \"khung hÃ¬nh bÃ³ng Ä‘Ã¡\", \"topK\": 10, \"mode\": \"vintern\"}'\nINFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"OPTIONS /search HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/913444450.py:48: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n  clip_nodes = qdrant_client.search(\n","output_type":"stream"},{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\nINFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"OPTIONS /search HTTP/1.1\" 200 OK\nINFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 422 Unprocessable Entity\nINFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 422 Unprocessable Entity\nINFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 422 Unprocessable Entity\nINFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}