{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fastapi uvicorn sentence-transformers open-clip-torch qdrant-client \\\n    llama-index llama-index-vector-stores-qdrant llama-index-core pyngrok -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:40:30.777258Z","iopub.execute_input":"2025-08-30T23:40:30.777595Z","iopub.status.idle":"2025-08-30T23:42:32.224622Z","shell.execute_reply.started":"2025-08-30T23:40:30.777571Z","shell.execute_reply":"2025-08-30T23:42:32.223491Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from fastapi import FastAPI, UploadFile, File, Form, Depends, APIRouter\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom qdrant_client import QdrantClient, models\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.core import VectorStoreIndex\nfrom collections import defaultdict\nimport heapq\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nimport re\nfrom pydantic import PrivateAttr\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom typing import List, Optional\nimport open_clip\nfrom pydantic import BaseModel\nimport json\nfrom PIL import Image\nimport hashlib\nimport io\nimport json\nfrom collections import defaultdict\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:42:32.227266Z","iopub.execute_input":"2025-08-30T23:42:32.227699Z","iopub.status.idle":"2025-08-30T23:43:19.579188Z","shell.execute_reply.started":"2025-08-30T23:42:32.227654Z","shell.execute_reply":"2025-08-30T23:43:19.577814Z"}},"outputs":[{"name":"stderr","text":"2025-08-30 23:42:55.868598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756597376.132298      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756597376.206184      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class CLIPEmbedding(BaseEmbedding):\n    _model = PrivateAttr()\n    _preprocess = PrivateAttr()\n    _tokenizer = PrivateAttr()\n    _device = PrivateAttr()\n\n    def __init__(self, model_name: str = \"ViT-H-14-quickgelu\", device: str = \"cpu\"):\n        super().__init__()\n        self._device = device\n        self._model, _, self._preprocess = open_clip.create_model_and_transforms(\n            model_name=model_name,\n            pretrained=\"dfn5b\",\n            device=self._device\n        )\n        self._tokenizer = open_clip.get_tokenizer(model_name)\n        self._model = self._model.to(self._device).eval()\n\n    def _encode_text(self, text: str) -> List[float]:\n        tokens = self._tokenizer([text]).to(self._device)\n        with torch.no_grad():\n            emb = self._model.encode_text(tokens)\n            emb = emb / emb.norm(dim=-1, keepdim=True) \n        return emb[0].cpu().numpy().tolist()\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        return self._encode_text(query)\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        return self._encode_text(text)\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        return self._get_text_embedding(text)\n\n    def _encode_image(self, image: Image.Image) -> List[float]:\n        image_tensor = self._preprocess(image).unsqueeze(0).to(self._device)\n        with torch.no_grad():\n            emb = self._model.encode_image(image_tensor)\n            emb = emb / emb.norm(dim=-1, keepdim=True)\n        return emb[0].cpu().numpy().tolist()\n        \n    def _get_image_embedding(self, image: Image.Image) -> List[float]:\n            return self._encode_image(image)\n    \n    async def _aget_image_embedding(self, image: Image.Image) -> List[float]:\n        return self._get_image_embedding(image)\n\nclass CaptionEmbedding(BaseEmbedding):\n    _model: SentenceTransformer = PrivateAttr()\n\n    def __init__(self, model_name: str = \"BAAI/bge-small-en\", device: str = \"cpu\", trust_remote_code: bool = False):\n        super().__init__()\n        print(f\"Loading model: {model_name}\")\n        self._model = SentenceTransformer(model_name, device=device, \n                                          trust_remote_code=trust_remote_code)\n        self._model = self._model.eval()\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        return self._model.encode(query, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False).tolist()\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        return self._model.encode(text, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False).tolist()\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        return self._get_text_embedding(text)\n\nclass Translator:\n    def __init__(self, model_name: str = \"VietAI/envit5-translation\", device: str = 'cpu'):\n        self.device = device\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n\n    def _clean_prefix(self, text: str) -> str:\n        return re.sub(r\"^(en|vi)\\s*:\\s*\", \"\", text.strip(), flags=re.IGNORECASE)\n    \n    def translate(self, text: str, source_lang: str = \"vi\", max_length: int = 128) -> str:\n        content = f\"{source_lang}: {text}\"\n        inputs = self.tokenizer(\n            content, \n            return_tensors=\"pt\", \n            truncation=True, \n            max_length=max_length).to(self.device)\n        with torch.no_grad():\n            outputs = self.model.generate(**inputs, max_length=max_length)\n        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        return self._clean_prefix(decoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:43:19.580387Z","iopub.execute_input":"2025-08-30T23:43:19.580820Z","iopub.status.idle":"2025-08-30T23:43:19.609185Z","shell.execute_reply.started":"2025-08-30T23:43:19.580785Z","shell.execute_reply":"2025-08-30T23:43:19.608162Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Configuration with Kaggle secrets support\nimport os\n\n# Use Kaggle secrets or fallback to environment variables\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    QDRANT_URL = user_secrets.get_secret(\"QDRANT_URL\")\n    QDRANT_API_KEY = user_secrets.get_secret(\"QDRANT_API_KEY\") \n    NGROK_AUTH_TOKEN = user_secrets.get_secret(\"NGROK_AUTH_TOKEN\")\n    print(\"✅ Using Kaggle secrets for configuration\")\nexcept Exception as e:\n    print(f\"⚠️  Kaggle secrets not available: {e}\")\n    print(\"🔧 Falling back to hardcoded values (update these with your credentials)\")\n    # Fallback to hardcoded values - UPDATE THESE WITH YOUR ACTUAL CREDENTIALS\n    QDRANT_URL = \"https://09a6d049-00c4-4b77-8e95-1dcc9ea5df34.eu-west-1-0.aws.cloud.qdrant.io:6333\"\n    QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.-ZPZib9FxehqbTuqxsk7QdVjBQd0LlQEq7dpjF1b4PI\"\n    NGROK_AUTH_TOKEN = \"320qV3SQqd4mes6sIS1q9TQTMim_6ruaWyvPqaaWUH7WMkuAB\"\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"🚀 Using device: {DEVICE}\")\n\n# Initialize Qdrant client\nqdrant_client = QdrantClient(\n    url=QDRANT_URL,\n    api_key=QDRANT_API_KEY,\n)\n\nCORS_SETTINGS = {\n    \"allow_origins\": [\"*\"],\n    \"allow_credentials\": True,\n    \"allow_methods\": [\"*\"],\n    \"allow_headers\": [\"*\"],\n}\n\n# Collection names\nCLIP_collection = \"Image\"\nBGE_collection = \"BGE_Caption\"\nGTE_collection = \"GTE_Caption\"\n\nprint(f\"📊 Collections: {CLIP_collection}, {BGE_collection}, {GTE_collection}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:43:19.611632Z","iopub.execute_input":"2025-08-30T23:43:19.611981Z","iopub.status.idle":"2025-08-30T23:43:20.046226Z","shell.execute_reply.started":"2025-08-30T23:43:19.611957Z","shell.execute_reply":"2025-08-30T23:43:20.045123Z"}},"outputs":[{"name":"stdout","text":"⚠️  Kaggle secrets not available: Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 90800749 and label QDRANT_URL.'], 'error': {'code': 5}, 'wasSuccessful': False}.\n🔧 Falling back to hardcoded values (update these with your credentials)\n🚀 Using device: cpu\n📊 Collections: Image, BGE_Caption, GTE_Caption\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Initialize models\nprint(\"🔧 Initializing models...\")\n\ntranslator = Translator(device=DEVICE)\nprint(\"✅ Translator loaded\")\n\nclip_embed_model = CLIPEmbedding(device=DEVICE)\nclip_vector_store = QdrantVectorStore(client=qdrant_client,\n                                      collection_name=CLIP_collection)\nclip_index = VectorStoreIndex.from_vector_store(vector_store=clip_vector_store,\n                                                embed_model=clip_embed_model)\nprint(\"✅ CLIP model and index loaded\")\n\nbge_embed_model = CaptionEmbedding(model_name=\"AITeamVN/Vietnamese_Embedding_v2\", device=DEVICE)\nbge_vector_store = QdrantVectorStore(client=qdrant_client, \n                                     collection_name=BGE_collection)\nbge_index = VectorStoreIndex.from_vector_store(vector_store=bge_vector_store,\n                                               embed_model=bge_embed_model)\nprint(\"✅ BGE Vietnamese model loaded\")\n\ngte_embed_model = CaptionEmbedding(model_name=\"dangvantuan/vietnamese-document-embedding\",\n                                   device=DEVICE, trust_remote_code=True)\ngte_vector_store = QdrantVectorStore(client=qdrant_client,\n                                     collection_name=GTE_collection)\ngte_index = VectorStoreIndex.from_vector_store(vector_store=gte_vector_store,\n                                               embed_model=gte_embed_model)\nprint(\"✅ GTE Document model loaded\")\n\nprint(\"🎉 All models initialized successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:43:20.047273Z","iopub.execute_input":"2025-08-30T23:43:20.047620Z","iopub.status.idle":"2025-08-30T23:45:06.353591Z","shell.execute_reply.started":"2025-08-30T23:43:20.047589Z","shell.execute_reply":"2025-08-30T23:45:06.352421Z"}},"outputs":[{"name":"stdout","text":"🔧 Initializing models...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c11e70d246f4705ba57d7ed79d6870a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6092f4925e5542f493e85122f6a4a3c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce36cb1dd45244d096b30690ac196285"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21b7aae99a51473eadc320259df32162"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/721 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22ea78c1328b4a43bd16664a13692a66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"929098a82f694df488b75e0d2eb8e0ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aedbc27ab194727a8cbbd7b73ff81f5"}},"metadata":{}},{"name":"stdout","text":"✅ Translator loaded\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"open_clip_pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7d86ffa91e84ef0b3ec84f85a06289a"}},"metadata":{}},{"name":"stdout","text":"✅ CLIP model and index loaded\nLoading model: AITeamVN/Vietnamese_Embedding_v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f1497226a1b4240853d57fa2cf0e066"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/171 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3db007af5884b4bad8d0b1c28da37c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed7de66d3c2a4dfb92460eeb0d54bc7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee0f9c8dfcab4372bcf0b1f7730d6218"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2624da050a1d449e85d4359bf2615d14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2200e955c4c49efb66e395ef16daaf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a168ab6fc4ec49c3ab600a7bd78f336e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0dbeec9325a48188f7fa1fb0def3a44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"664d9d77305540f6a3caa2d4cf0aef02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abf65de67c384dd39529e08bfc976d37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e427afbaaf52444ca61f3ded8635079f"}},"metadata":{}},{"name":"stdout","text":"✅ BGE Vietnamese model loaded\nLoading model: dangvantuan/vietnamese-document-embedding\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0573c380df5b498db89e1ce9a3ef52ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/171 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f1051360fa64539b45deb9e89934f8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"927cfcd42f1341d881d4feb20af3c04a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"029ffcb9de0f43d99c085d9e805f2740"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c4c7bf84a454100b7173516ddb70b65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61cee9309f6748f38299bc9a2c39ea17"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dangvantuan/Vietnamese_impl:\n- configuration.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e78e852338994406a3b601ebf0127c47"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dangvantuan/Vietnamese_impl:\n- modeling.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c21f12e9119d485dbd3dee96690842dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc2a53e8bf794409867add5f7ff2fa90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a49bfc4f8bd948cda50dce200f3327ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb1766cb92ae4493a187701035269b6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6a46f6034964dc7a983f6a7da9f7191"}},"metadata":{}},{"name":"stdout","text":"✅ GTE Document model loaded\n🎉 All models initialized successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Build frame mappings for temporal search\nprint(\"📊 Building frame mappings for temporal search...\")\n\nFRAME_NAMES = []\noffset = None\nbatch_count = 0\n\nwhile True:\n    result, offset = qdrant_client.scroll(\n        collection_name=CLIP_collection,\n        scroll_filter=None,\n        with_payload=True,\n        limit=1000,\n        offset=offset\n    )\n    \n    batch_count += 1\n    print(f\"  Batch {batch_count}: {len(result)} frames\")\n    \n    for point in result:\n        if \"id\" in point.payload:\n            FRAME_NAMES.append(point.payload[\"id\"])\n    \n    if offset is None:\n        break\n\nFRAME_NAMES = sorted(set(FRAME_NAMES))\n\nVIDEO_TO_FRAMES = defaultdict(list)\nfor f in FRAME_NAMES:\n    vid = \"_\".join(f.split(\"_\")[:2]) \n    VIDEO_TO_FRAMES[vid].append(f)\n\nprint(f\"✅ Loaded {len(FRAME_NAMES)} frame names from {len(VIDEO_TO_FRAMES)} videos\")\nprint(f\"📹 Sample videos: {list(VIDEO_TO_FRAMES.keys())[:5]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:45:06.355014Z","iopub.execute_input":"2025-08-30T23:45:06.355318Z","iopub.status.idle":"2025-08-30T23:45:13.161022Z","shell.execute_reply.started":"2025-08-30T23:45:06.355284Z","shell.execute_reply":"2025-08-30T23:45:13.159562Z"}},"outputs":[{"name":"stdout","text":"📊 Building frame mappings for temporal search...\n  Batch 1: 1000 frames\n  Batch 2: 1000 frames\n  Batch 3: 1000 frames\n  Batch 4: 1000 frames\n  Batch 5: 1000 frames\n  Batch 6: 1000 frames\n  Batch 7: 1000 frames\n  Batch 8: 1000 frames\n  Batch 9: 1000 frames\n  Batch 10: 1000 frames\n  Batch 11: 1000 frames\n  Batch 12: 1000 frames\n  Batch 13: 1000 frames\n  Batch 14: 1000 frames\n  Batch 15: 1000 frames\n  Batch 16: 1000 frames\n  Batch 17: 1000 frames\n  Batch 18: 1000 frames\n  Batch 19: 1000 frames\n  Batch 20: 1000 frames\n  Batch 21: 1000 frames\n  Batch 22: 1000 frames\n  Batch 23: 1000 frames\n  Batch 24: 1000 frames\n  Batch 25: 1000 frames\n  Batch 26: 1000 frames\n  Batch 27: 1000 frames\n  Batch 28: 1000 frames\n  Batch 29: 1000 frames\n  Batch 30: 1000 frames\n  Batch 31: 1000 frames\n  Batch 32: 1000 frames\n  Batch 33: 1000 frames\n  Batch 34: 1000 frames\n  Batch 35: 1000 frames\n  Batch 36: 1000 frames\n  Batch 37: 1000 frames\n  Batch 38: 1000 frames\n  Batch 39: 1000 frames\n  Batch 40: 1000 frames\n  Batch 41: 1000 frames\n  Batch 42: 1000 frames\n  Batch 43: 1000 frames\n  Batch 44: 1000 frames\n  Batch 45: 1000 frames\n  Batch 46: 1000 frames\n  Batch 47: 1000 frames\n  Batch 48: 1000 frames\n  Batch 49: 1000 frames\n  Batch 50: 1000 frames\n  Batch 51: 1000 frames\n  Batch 52: 1000 frames\n  Batch 53: 1000 frames\n  Batch 54: 1000 frames\n  Batch 55: 1000 frames\n  Batch 56: 1000 frames\n  Batch 57: 1000 frames\n  Batch 58: 1000 frames\n  Batch 59: 1000 frames\n  Batch 60: 1000 frames\n  Batch 61: 1000 frames\n  Batch 62: 1000 frames\n  Batch 63: 1000 frames\n  Batch 64: 1000 frames\n  Batch 65: 1000 frames\n  Batch 66: 1000 frames\n  Batch 67: 1000 frames\n  Batch 68: 1000 frames\n  Batch 69: 1000 frames\n  Batch 70: 1000 frames\n  Batch 71: 1000 frames\n  Batch 72: 1000 frames\n  Batch 73: 1000 frames\n  Batch 74: 1000 frames\n  Batch 75: 1000 frames\n  Batch 76: 1000 frames\n  Batch 77: 1000 frames\n  Batch 78: 1000 frames\n  Batch 79: 1000 frames\n  Batch 80: 1000 frames\n  Batch 81: 1000 frames\n  Batch 82: 1000 frames\n  Batch 83: 1000 frames\n  Batch 84: 1000 frames\n  Batch 85: 1000 frames\n  Batch 86: 1000 frames\n  Batch 87: 1000 frames\n  Batch 88: 1000 frames\n  Batch 89: 1000 frames\n  Batch 90: 1000 frames\n  Batch 91: 1000 frames\n  Batch 92: 1000 frames\n  Batch 93: 1000 frames\n  Batch 94: 1000 frames\n  Batch 95: 1000 frames\n  Batch 96: 1000 frames\n  Batch 97: 545 frames\n✅ Loaded 96545 frame names from 866 videos\n📹 Sample videos: ['L21_V001', 'L21_V002', 'L21_V003', 'L21_V005', 'L21_V006']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def retrieve(query: str, topK: int, frame_ids: Optional[List] = None,\n             mode: str = \"clip\", caption_mode: str = \"bge\"):\n    \"\"\"\n    Universal retrieve function with frame filtering support\n    \"\"\"\n    if mode == \"clip\":\n        embed_model = clip_embed_model\n        index = clip_index\n        collection_name = CLIP_collection\n        query_text = translator.translate(query, source_lang=\"vi\")\n    elif mode == \"vintern\":\n        if caption_mode == \"bge\":\n            embed_model = bge_embed_model\n            index = bge_index\n            collection_name = BGE_collection\n            query_text = query\n        else:  # gte\n            embed_model = gte_embed_model\n            index = gte_index\n            collection_name = GTE_collection\n            query_text = query\n        \n    if frame_ids: \n        # Direct query with frame filtering\n        vector_query = embed_model._get_text_embedding(query_text)\n        nodes = qdrant_client.query_points(\n            collection_name=collection_name,\n            query=vector_query,\n            limit=topK,\n            with_payload=True,\n            query_filter=models.Filter(must=[\n                models.FieldCondition(\n                    key=\"id\",\n                    match=models.MatchAny(any=frame_ids)\n                )\n            ])\n        ).points\n        results = [\n            {\"id\": node.payload[\"id\"].strip(), \"score\": node.score}\n            for node in nodes\n        ]\n    else:\n        # Use index for full search\n        retriever = index.as_retriever(similarity_top_k=topK)\n        nodes = retriever.retrieve(query_text)\n        results = [\n            {\"id\": node.metadata.get(\"id\", \"\").strip(), \"score\": node.score}\n            for node in nodes\n        ]\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:45:13.162306Z","iopub.execute_input":"2025-08-30T23:45:13.162733Z","iopub.status.idle":"2025-08-30T23:45:13.175107Z","shell.execute_reply.started":"2025-08-30T23:45:13.162688Z","shell.execute_reply":"2025-08-30T23:45:13.173810Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def retrieve_frame(query: str, topK: int, mode: str = \"hybrid\", caption_mode: str = \"bge\",\n                   alpha: float = 0.5, frame_ids: Optional[List] = None):\n    \"\"\"\n    Enhanced retrieve_frame with caption mode support\n    \"\"\"\n    if mode == \"clip\":\n        clip_nodes = retrieve(query, topK, frame_ids, \"clip\")\n        results = [\n            {\"image\": node[\"id\"], \"caption\": f\"{node['id']} | Score: {node['score']:.2f}\"}\n            for node in clip_nodes\n        ]\n        return results\n    \n    elif mode == \"vintern\":\n        caption_nodes = retrieve(query, topK, frame_ids, \"vintern\", caption_mode)\n        results = [\n            {\"image\": node[\"id\"], \"caption\": f\"{node['id']} | Score: {node['score']:.2f}\"}\n            for node in caption_nodes\n        ]\n        return results\n    \n    else:  # hybrid mode\n        clip_nodes = retrieve(query, topK, frame_ids, \"clip\")\n        caption_nodes = retrieve(query, topK, frame_ids, \"vintern\", caption_mode)\n        \n        combined_scores = defaultdict(float)\n        for node in caption_nodes:\n            combined_scores[node[\"id\"]] += node[\"score\"] * alpha\n\n        for node in clip_nodes:\n            combined_scores[node[\"id\"]] += node[\"score\"] * (1 - alpha)\n\n        top_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:topK]\n\n        return [\n            {\"image\": video_id.strip(), \"caption\": f\"{video_id} | Score: {score:.2f}\"}\n            for video_id, score in top_results\n        ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:45:13.176158Z","iopub.execute_input":"2025-08-30T23:45:13.176525Z","iopub.status.idle":"2025-08-30T23:45:13.579728Z","shell.execute_reply.started":"2025-08-30T23:45:13.176492Z","shell.execute_reply":"2025-08-30T23:45:13.578213Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def retrieve_from_image(contents: bytes, topK: int):\n    \"\"\"\n    Image-based search using CLIP embeddings\n    \"\"\"\n    image = Image.open(io.BytesIO(contents)).convert(\"RGB\")\n    vector_query = clip_embed_model._get_image_embedding(image)\n\n    clip_nodes = qdrant_client.query_points(\n        collection_name=CLIP_collection,\n        query=vector_query,\n        limit=topK,\n        with_payload=True\n    ).points\n\n    results = [\n        {\n            \"image\": node.payload.get(\"id\", \"\").strip(),\n            \"caption\": f\"{node.payload.get('id', '')} | Score: {node.score:.2f}\"\n        }\n        for node in clip_nodes\n    ]\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:45:13.581067Z","iopub.execute_input":"2025-08-30T23:45:13.581488Z","iopub.status.idle":"2025-08-30T23:45:13.609802Z","shell.execute_reply.started":"2025-08-30T23:45:13.581428Z","shell.execute_reply":"2025-08-30T23:45:13.607983Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def get_vid(image_name: str) -> str:\n    \"\"\"Extract video ID from frame name\"\"\"\n    parts = image_name.split(\"_\", 2)\n    return f\"{parts[0]}_{parts[1]}\"\n\ndef temporal_search(events: List[str], topK: int = 100, \n                    mode: str = \"hybrid\", caption_mode: str = \"bge\",\n                    alpha: float = 0.5, search_mode: str = \"progressive\"):\n\n    print(f\"🔍 Starting {search_mode} temporal search with {len(events)} events...\")\n    \n    if search_mode == \"progressive\":\n        # Original progressive filtering approach\n        frame_ids = None\n        final_results = []\n        \n        for i, event in enumerate(events):\n            print(f\"  Event {i+1}: {event[:50]}...\")\n            \n            results = retrieve_frame(query=event, topK=topK, mode=mode, \n                                     caption_mode=caption_mode, alpha=alpha, frame_ids=frame_ids)\n            final_results.append(results)\n            \n            # Extract video IDs to narrow search space\n            video_ids = {\"_\".join(item['image'].split(\"_\")[:2]) for item in results}\n            print(f\"    → Found {len(results)} results from {len(video_ids)} videos\")\n            \n            # Update frame_ids for next iteration\n            frame_ids = [f for vid in video_ids for f in VIDEO_TO_FRAMES[vid]]\n            print(f\"    → Narrowed search space to {len(frame_ids)} frames\")\n        \n        print(f\"✅ Progressive temporal search completed!\")\n        return final_results\n    \n    else:  # consolidated mode\n        # New consolidated approach - find common videos with best frames per event\n        event_results = []\n        \n        # Get results for each event independently\n        for i, event in enumerate(events):\n            print(f\"  Event {i+1}: {event[:50]}...\")\n            results = retrieve_frame(query=event, topK=topK*3, mode=mode,\n                                     caption_mode=caption_mode, alpha=alpha)\n            event_results.append(results)\n            print(f\"    → Found {len(results)} results\")\n        \n        # Find videos that appear in ALL events\n        video_sets = []\n        for results in event_results:\n            video_set = {get_vid(item['image']) for item in results}\n            video_sets.append(video_set)\n        \n        common_videos = set.intersection(*video_sets) if video_sets else set()\n        print(f\"  📹 Found {len(common_videos)} videos appearing in all events\")\n        \n        if not common_videos:\n            print(\"  ⚠️ No common videos found, returning top results from final event\")\n            return event_results[-1] if event_results else []\n        \n        # For each common video, get best frame per event\n        video_timeline_results = []\n        \n        for video_id in sorted(common_videos):\n            video_frames_per_event = []\n            total_score = 0\n            \n            for event_idx, results in enumerate(event_results):\n                # Find best frame for this video in this event\n                video_frames = [item for item in results if get_vid(item['image']) == video_id]\n                if video_frames:\n                    best_frame = max(video_frames, key=lambda x: float(x['caption'].split('Score: ')[1]))\n                    video_frames_per_event.append(best_frame['image'])\n                    total_score += float(best_frame['caption'].split('Score: ')[1])\n                else:\n                    video_frames_per_event.append(f\"{video_id}_0000\")\n            \n            avg_score = total_score / len(events)\n            video_timeline_results.append({\n                \"video_id\": video_id,\n                \"image\": video_frames_per_event,\n                \"caption\": [f\"Event {i+1}\" for i in range(len(events))],\n                \"score\": total_score\n            })\n        \n        # Sort by average score and return top results\n        video_timeline_results.sort(key=lambda x: x['score'], reverse=True)\n        final_consolidated = video_timeline_results[:topK]\n        \n        print(f\"✅ Consolidated temporal search completed! Returning {len(final_consolidated)} video timelines\")\n        return final_consolidated","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T00:19:12.269195Z","iopub.execute_input":"2025-08-31T00:19:12.271081Z","iopub.status.idle":"2025-08-31T00:19:12.293539Z","shell.execute_reply.started":"2025-08-31T00:19:12.271026Z","shell.execute_reply":"2025-08-31T00:19:12.291901Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def visualize_video_results(results, root_dir=\"/kaggle/input/aic-batch-1/keyframes/keyframes\", top_k=10):\n    \"\"\"\n    Enhanced visualization function for TRAKE mode temporal search results\n    Displays video timelines with frames from each event in sequence\n    \n    Args:\n        results: Results from temporal_search with search_mode=\"consolidated\"\n        root_dir: Root directory containing keyframe images\n        top_k: Number of top video timelines to display\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import matplotlib.image as mpimg\n    from matplotlib.patches import Rectangle\n    import numpy as np\n    import os\n    \n    if not results:\n        print(\"❌ No results to visualize\")\n        return\n    \n    # Handle both progressive and consolidated result formats\n    if isinstance(results[0], dict) and 'video_id' in results[0]:\n        # Consolidated format - video timelines\n        video_results = results[:top_k]\n        print(f\"🎬 Visualizing {len(video_results)} video timelines for TRAKE mode\")\n        \n        fig, axes = plt.subplots(len(video_results), len(video_results[0]['image']), \n                                figsize=(4 * len(video_results[0]['image']), 3 * len(video_results)))\n        fig.suptitle(f'TRAKE Mode: Top {len(video_results)} Video Event Sequences', fontsize=16, fontweight='bold')\n        \n        for video_idx, video_info in enumerate(video_results):\n            frames = video_info[\"image\"]\n            video_id = video_info[\"video_id\"] \n            total_score = video_info[\"score\"]\n            \n            for event_idx, frame_name in enumerate(frames):\n                if len(video_results) == 1:\n                    ax = axes[event_idx] if len(frames) > 1 else axes\n                else:\n                    ax = axes[video_idx, event_idx] if len(frames) > 1 else axes[video_idx]\n                \n                # Construct image path\n                img_path = os.path.join(root_dir, frame_name + '.jpg')\n                \n                try:\n                    if os.path.exists(img_path):\n                        img = mpimg.imread(img_path)\n                        ax.imshow(img)\n                    else:\n                        # Create placeholder if image not found\n                        ax.text(0.5, 0.5, f'Image not found:\\\\n{frame_name}', \n                               ha='center', va='center', transform=ax.transAxes,\n                               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n                        \n                except Exception as e:\n                    ax.text(0.5, 0.5, f'Error loading:\\\\n{frame_name}', \n                           ha='center', va='center', transform=ax.transAxes,\n                           bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"))\n                \n                # Add frame info and event marker\n                ax.set_title(f'Event {event_idx + 1}\\\\n{frame_name}', fontsize=10)\n                ax.axis('off')\n                \n                # Add colored border for event sequence\n                colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n                border_color = colors[event_idx % len(colors)]\n                rect = Rectangle((0, 0), 1, 1, transform=ax.transAxes, \n                               linewidth=3, edgecolor=border_color, facecolor='none')\n                ax.add_patch(rect)\n            \n            # Add video info as row label\n            if len(video_results) > 1:\n                axes[video_idx, 0].text(-0.1, 0.5, \n                                       f'{video_id}\\\\nScore: {total_score:.3f}', \n                                       ha='right', va='center', transform=axes[video_idx, 0].transAxes,\n                                       fontsize=12, fontweight='bold',\n                                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n        \n        plt.tight_layout()\n        plt.show()\n        \n       \n        \n    else:\n        # Progressive format - convert to simple grid visualization\n        print(f\"📋 Visualizing progressive search results ({len(results)} events)\")\n        \n        # Take top frames from final event for visualization\n        final_event_results = results[-1][:top_k] if results else []\n        \n        if not final_event_results:\n            print(\"❌ No final results to visualize\")\n            return\n            \n        cols = min(5, len(final_event_results))\n        rows = (len(final_event_results) + cols - 1) // cols\n        \n        fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n        fig.suptitle(f'Progressive Search: Final Results ({len(results)} events)', fontsize=14)\n        \n        if rows == 1 and cols == 1:\n            axes = [axes]\n        elif rows == 1:\n            axes = [axes]\n        else:\n            axes = axes.flatten() if rows > 1 else axes\n            \n        for idx, result in enumerate(final_event_results):\n            if idx >= len(axes):\n                break\n                \n            ax = axes[idx]\n            frame_name = result['image']\n            img_path = os.path.join(root_dir, frame_name + '.jpg')\n            \n            try:\n                if os.path.exists(img_path):\n                    img = mpimg.imread(img_path)\n                    ax.imshow(img)\n                else:\n                    ax.text(0.5, 0.5, f'Image not found:\\\\n{frame_name}', \n                           ha='center', va='center', transform=ax.transAxes)\n            except Exception as e:\n                ax.text(0.5, 0.5, f'Error loading:\\\\n{frame_name}', \n                       ha='center', va='center', transform=ax.transAxes)\n            \n            ax.set_title(f'{frame_name}\\\\n{result[\"caption\"].split(\"|\")[1]}', fontsize=9)\n            ax.axis('off')\n        \n        # Hide unused subplots\n        for idx in range(len(final_event_results), len(axes)):\n            axes[idx].set_visible(False)\n        \n        plt.tight_layout()\n        plt.show()\n\nprint(\"✅ Video visualization function\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T00:19:23.453426Z","iopub.execute_input":"2025-08-31T00:19:23.453834Z","iopub.status.idle":"2025-08-31T00:19:23.482010Z","shell.execute_reply.started":"2025-08-31T00:19:23.453808Z","shell.execute_reply":"2025-08-31T00:19:23.480307Z"}},"outputs":[{"name":"stdout","text":"✅ Video visualization function added for enhanced TRAKE mode\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# FastAPI Application\napp = FastAPI(title=\"Video Event Retrieval API v2.0\", \n              description=\"Enhanced multimodal search with temporal capabilities\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=CORS_SETTINGS[\"allow_origins\"],\n    allow_credentials=CORS_SETTINGS[\"allow_credentials\"],\n    allow_methods=CORS_SETTINGS[\"allow_methods\"],\n    allow_headers=CORS_SETTINGS[\"allow_headers\"],\n)\n\nrouter = APIRouter()\n\n@router.post(\"/search\")\nasync def api_search(\n    query: Optional[str] = Form(None),\n    topK: int = Form(...),\n    mode: str = Form(\"hybrid\"),\n    caption_mode: str = Form(\"bge\"),\n    alpha: float = Form(0.5),\n    file: UploadFile = File(None)\n):\n    \"\"\"\n    Enhanced search API with caption mode support\n    - mode: hybrid, clip, vintern, image\n    - caption_mode: bge, gte (for vintern and hybrid modes)\n    - alpha: text/visual balance for hybrid mode (0.1-0.9)\n    \"\"\"\n    start_time = time.time()\n    \n    try:\n        if mode == \"image\":\n            if file is None:\n                return {\"error\": \"No file uploaded for image mode\"}\n            contents = await file.read()\n            results = retrieve_from_image(contents=contents, topK=topK)\n            search_info = f\"IMAGE search\"\n        else:\n            if query is None or query.strip() == \"\":\n                return {\"error\": \"No query provided for text mode\"}\n            results = retrieve_frame(query=query, topK=topK, mode=mode, \n                                    caption_mode=caption_mode, alpha=alpha)\n            search_info = f\"{mode.upper()} mode with {caption_mode.upper()} model\"\n        \n        duration = time.time() - start_time\n        \n        return {\n            \"results\": results,\n            \"search_info\": {\n                \"mode\": mode,\n                \"caption_mode\": caption_mode if mode in [\"hybrid\", \"vintern\"] else None,\n                \"alpha\": alpha if mode == \"hybrid\" else None,\n                \"duration\": round(duration, 3),\n                \"count\": len(results),\n                \"description\": search_info\n            }\n        }\n    except Exception as e:\n        return {\"error\": f\"Search failed: {str(e)}\"}\n\n@router.post(\"/temporal_search\")\nasync def api_temporal_search(\n    events: str = Form(...),  # JSON string of event list\n    topK: int = Form(100),\n    mode: str = Form(\"hybrid\"),\n    caption_mode: str = Form(\"bge\"),\n    alpha: float = Form(0.5),\n    search_mode: str = Form(\"progressive\")  # New parameter: \"progressive\" or \"consolidated\"\n):\n    \"\"\"\n    Enhanced Temporal search API for TRAKE mode\n    - events: JSON array of sequential event descriptions\n    - search_mode: \"progressive\" (frontend compatible) or \"consolidated\" (TRAKE visualization)\n    - Returns different result formats based on search_mode\n    \"\"\"\n    start_time = time.time()\n    \n    try:\n        events_list = json.loads(events)\n        if not isinstance(events_list, list) or len(events_list) == 0:\n            return {\"error\": \"Events must be a non-empty list\"}\n        \n        # Filter out empty events\n        valid_events = [e.strip() for e in events_list if e.strip()]\n        if len(valid_events) == 0:\n            return {\"error\": \"No valid events provided\"}\n        \n        results = temporal_search(events=valid_events, topK=topK, mode=mode,\n                                 caption_mode=caption_mode, alpha=alpha, \n                                 search_mode=search_mode)\n        \n        duration = time.time() - start_time\n        \n        # Different response format based on search mode\n        if search_mode == \"consolidated\":\n            final_count = len(results)\n            result_type = \"video_timelines\"\n        else:  # progressive\n            final_count = len(results[-1]) if results else 0\n            result_type = \"progressive_events\"\n        \n        return {\n            \"results\": results,\n            \"search_info\": {\n                \"mode\": mode,\n                \"caption_mode\": caption_mode,\n                \"alpha\": alpha if mode == \"hybrid\" else None,\n                \"search_mode\": search_mode,\n                \"duration\": round(duration, 3),\n                \"events_processed\": len(valid_events),\n                \"final_count\": final_count,\n                \"result_type\": result_type,\n                \"description\": f\"{search_mode.title()} temporal search through {len(valid_events)} events\"\n            }\n        }\n    except json.JSONDecodeError:\n        return {\"error\": \"Invalid JSON format for events\"}\n    except Exception as e:\n        return {\"error\": f\"Temporal search failed: {str(e)}\"}\n\n@router.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"device\": DEVICE,\n        \"models_loaded\": {\n            \"clip\": clip_embed_model is not None,\n            \"bge\": bge_embed_model is not None,\n            \"gte\": gte_embed_model is not None,\n            \"translator\": translator is not None\n        },\n        \"collections\": {\n            \"clip\": CLIP_collection,\n            \"bge\": BGE_collection,\n            \"gte\": GTE_collection\n        },\n        \"frame_count\": len(FRAME_NAMES),\n        \"video_count\": len(VIDEO_TO_FRAMES),\n        \"supported_search_modes\": [\"progressive\", \"consolidated\"]\n    }\n\napp.include_router(router)\nprint(\"✅ FastAPI application configured with consolidated mode support\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T00:19:26.073361Z","iopub.execute_input":"2025-08-31T00:19:26.073738Z","iopub.status.idle":"2025-08-31T00:19:26.122254Z","shell.execute_reply.started":"2025-08-31T00:19:26.073714Z","shell.execute_reply":"2025-08-31T00:19:26.121179Z"}},"outputs":[{"name":"stdout","text":"✅ FastAPI application configured with consolidated mode support\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Server setup with ngrok\nimport os, time, threading, socket\nfrom pyngrok import ngrok\nimport uvicorn\n\nPORT = 8000\nHOST = \"0.0.0.0\"\n\n# Set ngrok auth token\nif NGROK_AUTH_TOKEN and NGROK_AUTH_TOKEN != \"YOUR_NGROK_TOKEN_HERE\":\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n    print(\"✅ Ngrok auth token set\")\nelse:\n    print(\"⚠️  NGROK_AUTH_TOKEN not configured. Please update with your token.\")\n    print(\"   Get your token from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n\ndef is_port_in_use(port: int, host=\"127.0.0.1\") -> bool:\n    \"\"\"Check if a local TCP port is already in use.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        return s.connect_ex((host, port)) == 0\n\ndef run_server():\n    \"\"\"Run FastAPI server in background thread\"\"\"\n    uvicorn.run(app, host=HOST, port=PORT, log_level=\"info\")\n\n# Start server only if not already running\nif not is_port_in_use(PORT):\n    print(f\"🚀 Starting FastAPI server on {HOST}:{PORT}\")\n    server_thread = threading.Thread(target=run_server, daemon=True)\n    server_thread.start()\n    time.sleep(3)  # Wait for server startup\n    print(\"✅ Server started successfully\")\nelse:\n    print(f\"🔁 Server already running on http://localhost:{PORT}\")\n\n# Setup ngrok tunnel\ntry:\n    # Clean up existing tunnels\n    for t in ngrok.get_tunnels():\n        addr = (t.config or {}).get(\"addr\", \"\")\n        if str(PORT) in addr:\n            try:\n                ngrok.disconnect(t.public_url)\n            except Exception:\n                pass\n\n    # Kill all tunnels if too many\n    if len(ngrok.get_tunnels()) >= 3:\n        ngrok.kill()\n\n    # Create new tunnel\n    tunnel = ngrok.connect(addr=PORT, proto=\"http\", bind_tls=True)\n    PUBLIC_URL = tunnel.public_url\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"🌐 BACKEND READY!\")\n    print(f\"📡 Public URL: {PUBLIC_URL}\")\n    print(f\"📖 API Docs: {PUBLIC_URL}/docs\")\n    print(f\"🏥 Health Check: {PUBLIC_URL}/health\")\n    print(f\"💻 Local URL: http://localhost:{PORT}\")\n    print(\"\\n🎯 COPY THE PUBLIC URL TO YOUR FRONTEND!\")\n    print(\"=\"*60)\n\n    # Save to global for later use\n    globals()[\"PUBLIC_URL\"] = PUBLIC_URL\n\nexcept Exception as e:\n    print(f\"❌ Ngrok tunnel failed: {e}\")\n    print(f\"🔧 Server still available locally: http://localhost:{PORT}\")\n    print(\"💡 Try restarting the kernel or checking your ngrok auth token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T00:19:32.254671Z","iopub.execute_input":"2025-08-31T00:19:32.255074Z","iopub.status.idle":"2025-08-31T00:19:32.529500Z","shell.execute_reply.started":"2025-08-31T00:19:32.255049Z","shell.execute_reply":"2025-08-31T00:19:32.528418Z"}},"outputs":[{"name":"stdout","text":"✅ Ngrok auth token set\n🔁 Server already running on http://localhost:8000\n\n============================================================\n🌐 BACKEND READY!\n📡 Public URL: https://b120ed32e79a.ngrok-free.app\n📖 API Docs: https://b120ed32e79a.ngrok-free.app/docs\n🏥 Health Check: https://b120ed32e79a.ngrok-free.app/health\n💻 Local URL: http://localhost:8000\n\n🎯 COPY THE PUBLIC URL TO YOUR FRONTEND!\n============================================================\nINFO:     14.234.51.109:0 - \"OPTIONS /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting consolidated temporal search with 3 events...\n  Event 1: Người đầu bếp cho cá vào một tô màu trắng. Hãy lấy...\n    → Found 300 results\n  Event 2: Người đầu bếp đổ bột vào một tô cá để chiên. Hãy l...\n    → Found 300 results\n  Event 3: Tiếp theo, người đầu bếp này dùng đũa để kiểm tra ...\n    → Found 300 results\n  📹 Found 40 videos appearing in all events\n✅ Consolidated temporal search completed! Returning 40 video timelines\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting consolidated temporal search with 3 events...\n  Event 1: Người đầu bếp cho cá vào một tô màu trắng. Hãy lấy...\n    → Found 600 results\n  Event 2: Người đầu bếp đổ bột vào một tô cá để chiên. Hãy l...\n    → Found 600 results\n  Event 3: Tiếp theo, người đầu bếp này dùng đũa để kiểm tra ...\n    → Found 600 results\n  📹 Found 136 videos appearing in all events\n✅ Consolidated temporal search completed! Returning 136 video timelines\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Test the enhanced API\nimport requests\n\ndef test_api_endpoints(use_public_url=True):\n    \"\"\"Test all API endpoints with the new features\"\"\"\n    \n    if use_public_url and 'PUBLIC_URL' in globals():\n        base_url = globals()['PUBLIC_URL']\n        print(f\"🔗 Testing public URL: {base_url}\")\n    else:\n        base_url = \"http://localhost:8000\"\n        print(f\"🔗 Testing local URL: {base_url}\")\n    \n    # Test health endpoint\n    print(\"\\n1. 🏥 Testing health endpoint...\")\n    try:\n        response = requests.get(f\"{base_url}/health\", timeout=10)\n        if response.status_code == 200:\n            health = response.json()\n            print(f\"   ✅ Health check passed\")\n            print(f\"   📊 {health['frame_count']} frames from {health['video_count']} videos\")\n            print(f\"   🤖 Models: {health['models_loaded']}\")\n        else:\n            print(f\"   ❌ Health check failed: {response.status_code}\")\n    except Exception as e:\n        print(f\"   ❌ Health check error: {e}\")\n        return\n    \n    # Test search with BGE\n    print(\"\\n2. 🔍 Testing hybrid search with BGE...\")\n    test_search(base_url, \"người đang nấu ăn\", mode=\"hybrid\", caption_mode=\"bge\")\n    \n    # Test search with GTE\n    print(\"\\n3. 🔍 Testing hybrid search with GTE...\")\n    test_search(base_url, \"người đang nấu ăn\", mode=\"hybrid\", caption_mode=\"gte\")\n    \n    # Test vintern only with GTE\n    print(\"\\n4. 📝 Testing vintern search with GTE...\")\n    test_search(base_url, \"cảnh đẹp thiên nhiên\", mode=\"vintern\", caption_mode=\"gte\")\n    \n    # Test temporal search\n    print(\"\\n5. ⏰ Testing temporal search...\")\n    test_temporal_search(base_url, [\n        \"Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đem nướng trên chảo. Hãy lấy khoảnh khắc chiếc dao cắt qua hoàn toàn chiếc bánh.\",\n        \"Sau đó người này rắc bột lên những miếng thịt, trong quá trình này người đầu bếp lật những miếng thịt để rắc bột đều hai mặt. Hãy lấy khoảnh khắc đầu tiên người đầu bếp này buông tay khỏi miếng thịt sau khi lật miếng thịt đầu tiên.\",\n        \"Các miếng thịt sau đó được đem đi áp chảo cùng với bơ (3 ngang 1 dọc theo chiều của camera). Hãy lấy khoảnh khắc đầu tiên người đầu bếp cầm vào chảo để nhấc lên đảo bơ đều xung quanh\"\n    ])\n\ndef test_search(base_url, query, mode=\"hybrid\", caption_mode=\"bge\", topK=5):\n    \"\"\"Test search endpoint\"\"\"\n    try:\n        data = {\n            \"query\": query,\n            \"topK\": topK,\n            \"mode\": mode,\n            \"caption_mode\": caption_mode,\n            \"alpha\": 0.6\n        }\n        \n        response = requests.post(f\"{base_url}/search\", data=data, timeout=30)\n        \n        if response.status_code == 200:\n            result = response.json()\n            search_info = result.get(\"search_info\", {})\n            print(f\"   ✅ Search successful: {search_info.get('description')}\")\n            print(f\"   ⏱️ Duration: {search_info.get('duration')}s\")\n            print(f\"   📊 Results: {len(result['results'])}\")\n            \n            # Show top results\n            for i, res in enumerate(result['results'][:3]):\n                print(f\"      {i+1}. {res['caption']}\")\n        else:\n            print(f\"   ❌ Search failed: {response.status_code} - {response.text}\")\n    except Exception as e:\n        print(f\"   ❌ Search error: {e}\")\n\ndef test_temporal_search(base_url, events, topK=20):\n    \"\"\"Test temporal search endpoint\"\"\"\n    try:\n        data = {\n            \"events\": json.dumps(events),\n            \"topK\": topK,\n            \"mode\": \"hybrid\",\n            \"caption_mode\": \"gte\",\n            \"alpha\": 0.7\n        }\n        \n        response = requests.post(f\"{base_url}/temporal_search\", data=data, timeout=60)\n        \n        if response.status_code == 200:\n            result = response.json()\n            search_info = result.get(\"search_info\", {})\n            print(f\"   ✅ Temporal search successful: {search_info.get('description')}\")\n            print(f\"   ⏱️ Duration: {search_info.get('duration')}s\")\n            print(f\"   📊 Events processed: {search_info.get('events_processed')}\")\n            print(f\"   🎯 Final results: {search_info.get('final_count')}\")\n            \n            # Show progression\n            for i, event_results in enumerate(result['results']):\n                print(f\"      Event {i+1}: {len(event_results)} results\")\n                for j, res in enumerate(event_results[:2]):\n                    print(f\"        → {res['caption']}\")\n        else:\n            print(f\"   ❌ Temporal search failed: {response.status_code} - {response.text}\")\n    except Exception as e:\n        print(f\"   ❌ Temporal search error: {e}\")\n\n# Run tests\nprint(\"🧪 Testing Enhanced API...\")\ntest_api_endpoints()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:45:19.270410Z","iopub.execute_input":"2025-08-30T23:45:19.270740Z","iopub.status.idle":"2025-08-30T23:45:33.172507Z","shell.execute_reply.started":"2025-08-30T23:45:19.270718Z","shell.execute_reply":"2025-08-30T23:45:33.170827Z"}},"outputs":[{"name":"stdout","text":"🧪 Testing Enhanced API...\n🔗 Testing public URL: https://5d686cb1624e.ngrok-free.app\n\n1. 🏥 Testing health endpoint...\nINFO:     34.34.86.60:0 - \"GET /health HTTP/1.1\" 200 OK\n   ✅ Health check passed\n   📊 96545 frames from 866 videos\n   🤖 Models: {'clip': True, 'bge': True, 'gte': True, 'translator': True}\n\n2. 🔍 Testing hybrid search with BGE...\nINFO:     34.34.86.60:0 - \"POST /search HTTP/1.1\" 200 OK\n   ✅ Search successful: HYBRID mode with BGE model\n   ⏱️ Duration: 2.064s\n   📊 Results: 5\n      1. L26_V272_5537 | Score: 0.42\n      2. L26_V452_4878 | Score: 0.42\n      3. L26_V339_5215 | Score: 0.41\n\n3. 🔍 Testing hybrid search with GTE...\nINFO:     34.34.86.60:0 - \"POST /search HTTP/1.1\" 200 OK\n   ✅ Search successful: HYBRID mode with GTE model\n   ⏱️ Duration: 1.145s\n   📊 Results: 5\n      1. L27_V003_6294 | Score: 0.38\n      2. L26_V367_5958 | Score: 0.38\n      3. L26_V484_3594 | Score: 0.37\n\n4. 📝 Testing vintern search with GTE...\nINFO:     34.34.86.60:0 - \"POST /search HTTP/1.1\" 200 OK\n   ✅ Search successful: VINTERN mode with GTE model\n   ⏱️ Duration: 0.119s\n   📊 Results: 5\n      1. L27_V008_12041 | Score: 0.68\n      2. L28_V016_5421 | Score: 0.66\n      3. L28_V018_13549 | Score: 0.63\n\n5. ⏰ Testing temporal search...\n🔍 Starting progressive temporal search with 3 events...\n  Event 1: Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đe...\n    → Found 20 results from 18 videos\n    → Narrowed search space to 1814 frames\n  Event 2: Sau đó người này rắc bột lên những miếng thịt, tro...\n    → Found 20 results from 7 videos\n    → Narrowed search space to 732 frames\n  Event 3: Các miếng thịt sau đó được đem đi áp chảo cùng với...\n    → Found 20 results from 4 videos\n    → Narrowed search space to 433 frames\n✅ Progressive temporal search completed!\nINFO:     34.34.86.60:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n   ✅ Temporal search successful: Progressive temporal search through 3 events\n   ⏱️ Duration: 8.764s\n   📊 Events processed: 3\n   🎯 Final results: 20\n      Event 1: 20 results\n        → L26_V289_1948 | Score: 0.61\n        → L26_V399_5336 | Score: 0.58\n      Event 2: 20 results\n        → L26_V399_2790 | Score: 0.43\n        → L26_V251_2204 | Score: 0.43\n      Event 3: 20 results\n        → L26_V247_4538 | Score: 0.52\n        → L26_V399_4814 | Score: 0.49\nINFO:     14.234.51.109:0 - \"OPTIONS /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting consolidated temporal search with 3 events...\n  Event 1: Người đầu bếp cho cá vào một tô màu trắng. Hãy lấy...\n    → Found 30 results\n  Event 2: Người đầu bếp đổ bột vào một tô cá để chiên. Hãy l...\n    → Found 30 results\n  Event 3: Tiếp theo, người đầu bếp này dùng đũa để kiểm tra ...\n    → Found 30 results\n  📹 Found 0 videos appearing in all events\n  ⚠️ No common videos found, returning top results from final event\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting consolidated temporal search with 3 events...\n  Event 1: Người đầu bếp cho cá vào một tô màu trắng. Hãy lấy...\n    → Found 30 results\n  Event 2: Người đầu bếp đổ bột vào một tô cá để chiên. Hãy l...\n    → Found 30 results\n  Event 3: Tiếp theo, người đầu bếp này dùng đũa để kiểm tra ...\n    → Found 30 results\n  📹 Found 0 videos appearing in all events\n  ⚠️ No common videos found, returning top results from final event\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting consolidated temporal search with 3 events...\n  Event 1: Người đầu bếp cho cá vào một tô màu trắng. Hãy lấy...\n    → Found 600 results\n  Event 2: Người đầu bếp đổ bột vào một tô cá để chiên. Hãy l...\n    → Found 600 results\n  Event 3: Tiếp theo, người đầu bếp này dùng đũa để kiểm tra ...\n    → Found 600 results\n  📹 Found 136 videos appearing in all events\n✅ Consolidated temporal search completed! Returning 136 video timelines\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting consolidated temporal search with 3 events...\n  Event 1: Người đầu bếp cho cá vào một tô màu trắng. Hãy lấy...\n    → Found 30 results\n  Event 2: Người đầu bếp đổ bột vào một tô cá để chiên. Hãy l...\n    → Found 30 results\n  Event 3: Tiếp theo, người đầu bếp này dùng đũa để kiểm tra ...\n    → Found 30 results\n  📹 Found 0 videos appearing in all events\n  ⚠️ No common videos found, returning top results from final event\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting consolidated temporal search with 3 events...\n  Event 1: Người đầu bếp cho cá vào một tô màu trắng. Hãy lấy...\n    → Found 300 results\n  Event 2: Người đầu bếp đổ bột vào một tô cá để chiên. Hãy l...\n    → Found 300 results\n  Event 3: Tiếp theo, người đầu bếp này dùng đũa để kiểm tra ...\n    → Found 300 results\n  📹 Found 40 videos appearing in all events\n✅ Consolidated temporal search completed! Returning 40 video timelines\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting consolidated temporal search with 3 events...\n  Event 1: Người đầu bếp cho cá vào một tô màu trắng. Hãy lấy...\n    → Found 300 results\n  Event 2: Người đầu bếp đổ bột vào một tô cá để chiên. Hãy l...\n    → Found 300 results\n  Event 3: Tiếp theo, người đầu bếp này dùng đũa để kiểm tra ...\n    → Found 300 results\n  📹 Found 40 videos appearing in all events\n✅ Consolidated temporal search completed! Returning 40 video timelines\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"OPTIONS /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting progressive temporal search with 3 events...\n  Event 1: Người đầu bếp cho cá vào một tô màu trắng. Hãy lấy...\n    → Found 100 results from 76 videos\n    → Narrowed search space to 8141 frames\n  Event 2: Người đầu bếp đổ bột vào một tô cá để chiên. Hãy l...\n    → Found 100 results from 53 videos\n    → Narrowed search space to 5654 frames\n  Event 3: Tiếp theo, người đầu bếp này dùng đũa để kiểm tra ...\n    → Found 100 results from 28 videos\n    → Narrowed search space to 2704 frames\n✅ Progressive temporal search completed!\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting consolidated temporal search with 3 events...\n  Event 1: Người đầu bếp cho cá vào một tô màu trắng. Hãy lấy...\n    → Found 300 results\n  Event 2: Người đầu bếp đổ bột vào một tô cá để chiên. Hãy l...\n    → Found 300 results\n  Event 3: Tiếp theo, người đầu bếp này dùng đũa để kiểm tra ...\n    → Found 300 results\n  📹 Found 40 videos appearing in all events\n✅ Consolidated temporal search completed! Returning 40 video timelines\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}