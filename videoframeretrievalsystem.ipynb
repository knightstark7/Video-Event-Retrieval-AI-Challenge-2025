{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fastapi uvicorn sentence-transformers open-clip-torch qdrant-client \\\n    llama-index llama-index-vector-stores-qdrant llama-index-core pyngrok -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:40:30.777258Z","iopub.execute_input":"2025-08-30T23:40:30.777595Z","iopub.status.idle":"2025-08-30T23:42:32.224622Z","shell.execute_reply.started":"2025-08-30T23:40:30.777571Z","shell.execute_reply":"2025-08-30T23:42:32.223491Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from fastapi import FastAPI, UploadFile, File, Form, Depends, APIRouter\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom qdrant_client import QdrantClient, models\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.core import VectorStoreIndex\nfrom collections import defaultdict\nimport heapq\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nimport re\nfrom pydantic import PrivateAttr\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom typing import List, Optional\nimport open_clip\nfrom pydantic import BaseModel\nimport json\nfrom PIL import Image\nimport hashlib\nimport io\nimport json\nfrom collections import defaultdict\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:42:32.227266Z","iopub.execute_input":"2025-08-30T23:42:32.227699Z","iopub.status.idle":"2025-08-30T23:43:19.579188Z","shell.execute_reply.started":"2025-08-30T23:42:32.227654Z","shell.execute_reply":"2025-08-30T23:43:19.577814Z"}},"outputs":[{"name":"stderr","text":"2025-08-30 23:42:55.868598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756597376.132298      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756597376.206184      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class CLIPEmbedding(BaseEmbedding):\n    _model = PrivateAttr()\n    _preprocess = PrivateAttr()\n    _tokenizer = PrivateAttr()\n    _device = PrivateAttr()\n\n    def __init__(self, model_name: str = \"ViT-H-14-quickgelu\", device: str = \"cpu\"):\n        super().__init__()\n        self._device = device\n        self._model, _, self._preprocess = open_clip.create_model_and_transforms(\n            model_name=model_name,\n            pretrained=\"dfn5b\",\n            device=self._device\n        )\n        self._tokenizer = open_clip.get_tokenizer(model_name)\n        self._model = self._model.to(self._device).eval()\n\n    def _encode_text(self, text: str) -> List[float]:\n        tokens = self._tokenizer([text]).to(self._device)\n        with torch.no_grad():\n            emb = self._model.encode_text(tokens)\n            emb = emb / emb.norm(dim=-1, keepdim=True) \n        return emb[0].cpu().numpy().tolist()\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        return self._encode_text(query)\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        return self._encode_text(text)\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        return self._get_text_embedding(text)\n\n    def _encode_image(self, image: Image.Image) -> List[float]:\n        image_tensor = self._preprocess(image).unsqueeze(0).to(self._device)\n        with torch.no_grad():\n            emb = self._model.encode_image(image_tensor)\n            emb = emb / emb.norm(dim=-1, keepdim=True)\n        return emb[0].cpu().numpy().tolist()\n        \n    def _get_image_embedding(self, image: Image.Image) -> List[float]:\n            return self._encode_image(image)\n    \n    async def _aget_image_embedding(self, image: Image.Image) -> List[float]:\n        return self._get_image_embedding(image)\n\nclass CaptionEmbedding(BaseEmbedding):\n    _model: SentenceTransformer = PrivateAttr()\n\n    def __init__(self, model_name: str = \"BAAI/bge-small-en\", device: str = \"cpu\", trust_remote_code: bool = False):\n        super().__init__()\n        print(f\"Loading model: {model_name}\")\n        self._model = SentenceTransformer(model_name, device=device, \n                                          trust_remote_code=trust_remote_code)\n        self._model = self._model.eval()\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        return self._model.encode(query, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False).tolist()\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        return self._model.encode(text, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False).tolist()\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        return self._get_text_embedding(text)\n\nclass Translator:\n    def __init__(self, model_name: str = \"VietAI/envit5-translation\", device: str = 'cpu'):\n        self.device = device\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n\n    def _clean_prefix(self, text: str) -> str:\n        return re.sub(r\"^(en|vi)\\s*:\\s*\", \"\", text.strip(), flags=re.IGNORECASE)\n    \n    def translate(self, text: str, source_lang: str = \"vi\", max_length: int = 128) -> str:\n        content = f\"{source_lang}: {text}\"\n        inputs = self.tokenizer(\n            content, \n            return_tensors=\"pt\", \n            truncation=True, \n            max_length=max_length).to(self.device)\n        with torch.no_grad():\n            outputs = self.model.generate(**inputs, max_length=max_length)\n        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        return self._clean_prefix(decoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:43:19.580387Z","iopub.execute_input":"2025-08-30T23:43:19.580820Z","iopub.status.idle":"2025-08-30T23:43:19.609185Z","shell.execute_reply.started":"2025-08-30T23:43:19.580785Z","shell.execute_reply":"2025-08-30T23:43:19.608162Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Configuration with Kaggle secrets support\nimport os\n\n# Use Kaggle secrets or fallback to environment variables\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    QDRANT_URL = user_secrets.get_secret(\"QDRANT_URL\")\n    QDRANT_API_KEY = user_secrets.get_secret(\"QDRANT_API_KEY\") \n    NGROK_AUTH_TOKEN = user_secrets.get_secret(\"NGROK_AUTH_TOKEN\")\n    print(\"âœ… Using Kaggle secrets for configuration\")\nexcept Exception as e:\n    print(f\"âš ï¸  Kaggle secrets not available: {e}\")\n    print(\"ğŸ”§ Falling back to hardcoded values (update these with your credentials)\")\n    # Fallback to hardcoded values - UPDATE THESE WITH YOUR ACTUAL CREDENTIALS\n    QDRANT_URL = \"https://09a6d049-00c4-4b77-8e95-1dcc9ea5df34.eu-west-1-0.aws.cloud.qdrant.io:6333\"\n    QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.-ZPZib9FxehqbTuqxsk7QdVjBQd0LlQEq7dpjF1b4PI\"\n    NGROK_AUTH_TOKEN = \"320qV3SQqd4mes6sIS1q9TQTMim_6ruaWyvPqaaWUH7WMkuAB\"\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"ğŸš€ Using device: {DEVICE}\")\n\n# Initialize Qdrant client\nqdrant_client = QdrantClient(\n    url=QDRANT_URL,\n    api_key=QDRANT_API_KEY,\n)\n\nCORS_SETTINGS = {\n    \"allow_origins\": [\"*\"],\n    \"allow_credentials\": True,\n    \"allow_methods\": [\"*\"],\n    \"allow_headers\": [\"*\"],\n}\n\n# Collection names\nCLIP_collection = \"Image\"\nBGE_collection = \"BGE_Caption\"\nGTE_collection = \"GTE_Caption\"\n\nprint(f\"ğŸ“Š Collections: {CLIP_collection}, {BGE_collection}, {GTE_collection}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:43:19.611632Z","iopub.execute_input":"2025-08-30T23:43:19.611981Z","iopub.status.idle":"2025-08-30T23:43:20.046226Z","shell.execute_reply.started":"2025-08-30T23:43:19.611957Z","shell.execute_reply":"2025-08-30T23:43:20.045123Z"}},"outputs":[{"name":"stdout","text":"âš ï¸  Kaggle secrets not available: Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 90800749 and label QDRANT_URL.'], 'error': {'code': 5}, 'wasSuccessful': False}.\nğŸ”§ Falling back to hardcoded values (update these with your credentials)\nğŸš€ Using device: cpu\nğŸ“Š Collections: Image, BGE_Caption, GTE_Caption\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Initialize models\nprint(\"ğŸ”§ Initializing models...\")\n\ntranslator = Translator(device=DEVICE)\nprint(\"âœ… Translator loaded\")\n\nclip_embed_model = CLIPEmbedding(device=DEVICE)\nclip_vector_store = QdrantVectorStore(client=qdrant_client,\n                                      collection_name=CLIP_collection)\nclip_index = VectorStoreIndex.from_vector_store(vector_store=clip_vector_store,\n                                                embed_model=clip_embed_model)\nprint(\"âœ… CLIP model and index loaded\")\n\nbge_embed_model = CaptionEmbedding(model_name=\"AITeamVN/Vietnamese_Embedding_v2\", device=DEVICE)\nbge_vector_store = QdrantVectorStore(client=qdrant_client, \n                                     collection_name=BGE_collection)\nbge_index = VectorStoreIndex.from_vector_store(vector_store=bge_vector_store,\n                                               embed_model=bge_embed_model)\nprint(\"âœ… BGE Vietnamese model loaded\")\n\ngte_embed_model = CaptionEmbedding(model_name=\"dangvantuan/vietnamese-document-embedding\",\n                                   device=DEVICE, trust_remote_code=True)\ngte_vector_store = QdrantVectorStore(client=qdrant_client,\n                                     collection_name=GTE_collection)\ngte_index = VectorStoreIndex.from_vector_store(vector_store=gte_vector_store,\n                                               embed_model=gte_embed_model)\nprint(\"âœ… GTE Document model loaded\")\n\nprint(\"ğŸ‰ All models initialized successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:43:20.047273Z","iopub.execute_input":"2025-08-30T23:43:20.047620Z","iopub.status.idle":"2025-08-30T23:45:06.353591Z","shell.execute_reply.started":"2025-08-30T23:43:20.047589Z","shell.execute_reply":"2025-08-30T23:45:06.352421Z"}},"outputs":[{"name":"stdout","text":"ğŸ”§ Initializing models...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c11e70d246f4705ba57d7ed79d6870a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6092f4925e5542f493e85122f6a4a3c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce36cb1dd45244d096b30690ac196285"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21b7aae99a51473eadc320259df32162"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/721 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22ea78c1328b4a43bd16664a13692a66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"929098a82f694df488b75e0d2eb8e0ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aedbc27ab194727a8cbbd7b73ff81f5"}},"metadata":{}},{"name":"stdout","text":"âœ… Translator loaded\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"open_clip_pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7d86ffa91e84ef0b3ec84f85a06289a"}},"metadata":{}},{"name":"stdout","text":"âœ… CLIP model and index loaded\nLoading model: AITeamVN/Vietnamese_Embedding_v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f1497226a1b4240853d57fa2cf0e066"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/171 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3db007af5884b4bad8d0b1c28da37c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed7de66d3c2a4dfb92460eeb0d54bc7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee0f9c8dfcab4372bcf0b1f7730d6218"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2624da050a1d449e85d4359bf2615d14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2200e955c4c49efb66e395ef16daaf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a168ab6fc4ec49c3ab600a7bd78f336e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0dbeec9325a48188f7fa1fb0def3a44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"664d9d77305540f6a3caa2d4cf0aef02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abf65de67c384dd39529e08bfc976d37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e427afbaaf52444ca61f3ded8635079f"}},"metadata":{}},{"name":"stdout","text":"âœ… BGE Vietnamese model loaded\nLoading model: dangvantuan/vietnamese-document-embedding\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0573c380df5b498db89e1ce9a3ef52ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/171 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f1051360fa64539b45deb9e89934f8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"927cfcd42f1341d881d4feb20af3c04a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"029ffcb9de0f43d99c085d9e805f2740"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c4c7bf84a454100b7173516ddb70b65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61cee9309f6748f38299bc9a2c39ea17"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dangvantuan/Vietnamese_impl:\n- configuration.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e78e852338994406a3b601ebf0127c47"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dangvantuan/Vietnamese_impl:\n- modeling.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c21f12e9119d485dbd3dee96690842dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc2a53e8bf794409867add5f7ff2fa90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a49bfc4f8bd948cda50dce200f3327ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb1766cb92ae4493a187701035269b6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6a46f6034964dc7a983f6a7da9f7191"}},"metadata":{}},{"name":"stdout","text":"âœ… GTE Document model loaded\nğŸ‰ All models initialized successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Build frame mappings for temporal search\nprint(\"ğŸ“Š Building frame mappings for temporal search...\")\n\nFRAME_NAMES = []\noffset = None\nbatch_count = 0\n\nwhile True:\n    result, offset = qdrant_client.scroll(\n        collection_name=CLIP_collection,\n        scroll_filter=None,\n        with_payload=True,\n        limit=1000,\n        offset=offset\n    )\n    \n    batch_count += 1\n    print(f\"  Batch {batch_count}: {len(result)} frames\")\n    \n    for point in result:\n        if \"id\" in point.payload:\n            FRAME_NAMES.append(point.payload[\"id\"])\n    \n    if offset is None:\n        break\n\nFRAME_NAMES = sorted(set(FRAME_NAMES))\n\nVIDEO_TO_FRAMES = defaultdict(list)\nfor f in FRAME_NAMES:\n    vid = \"_\".join(f.split(\"_\")[:2]) \n    VIDEO_TO_FRAMES[vid].append(f)\n\nprint(f\"âœ… Loaded {len(FRAME_NAMES)} frame names from {len(VIDEO_TO_FRAMES)} videos\")\nprint(f\"ğŸ“¹ Sample videos: {list(VIDEO_TO_FRAMES.keys())[:5]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:45:06.355014Z","iopub.execute_input":"2025-08-30T23:45:06.355318Z","iopub.status.idle":"2025-08-30T23:45:13.161022Z","shell.execute_reply.started":"2025-08-30T23:45:06.355284Z","shell.execute_reply":"2025-08-30T23:45:13.159562Z"}},"outputs":[{"name":"stdout","text":"ğŸ“Š Building frame mappings for temporal search...\n  Batch 1: 1000 frames\n  Batch 2: 1000 frames\n  Batch 3: 1000 frames\n  Batch 4: 1000 frames\n  Batch 5: 1000 frames\n  Batch 6: 1000 frames\n  Batch 7: 1000 frames\n  Batch 8: 1000 frames\n  Batch 9: 1000 frames\n  Batch 10: 1000 frames\n  Batch 11: 1000 frames\n  Batch 12: 1000 frames\n  Batch 13: 1000 frames\n  Batch 14: 1000 frames\n  Batch 15: 1000 frames\n  Batch 16: 1000 frames\n  Batch 17: 1000 frames\n  Batch 18: 1000 frames\n  Batch 19: 1000 frames\n  Batch 20: 1000 frames\n  Batch 21: 1000 frames\n  Batch 22: 1000 frames\n  Batch 23: 1000 frames\n  Batch 24: 1000 frames\n  Batch 25: 1000 frames\n  Batch 26: 1000 frames\n  Batch 27: 1000 frames\n  Batch 28: 1000 frames\n  Batch 29: 1000 frames\n  Batch 30: 1000 frames\n  Batch 31: 1000 frames\n  Batch 32: 1000 frames\n  Batch 33: 1000 frames\n  Batch 34: 1000 frames\n  Batch 35: 1000 frames\n  Batch 36: 1000 frames\n  Batch 37: 1000 frames\n  Batch 38: 1000 frames\n  Batch 39: 1000 frames\n  Batch 40: 1000 frames\n  Batch 41: 1000 frames\n  Batch 42: 1000 frames\n  Batch 43: 1000 frames\n  Batch 44: 1000 frames\n  Batch 45: 1000 frames\n  Batch 46: 1000 frames\n  Batch 47: 1000 frames\n  Batch 48: 1000 frames\n  Batch 49: 1000 frames\n  Batch 50: 1000 frames\n  Batch 51: 1000 frames\n  Batch 52: 1000 frames\n  Batch 53: 1000 frames\n  Batch 54: 1000 frames\n  Batch 55: 1000 frames\n  Batch 56: 1000 frames\n  Batch 57: 1000 frames\n  Batch 58: 1000 frames\n  Batch 59: 1000 frames\n  Batch 60: 1000 frames\n  Batch 61: 1000 frames\n  Batch 62: 1000 frames\n  Batch 63: 1000 frames\n  Batch 64: 1000 frames\n  Batch 65: 1000 frames\n  Batch 66: 1000 frames\n  Batch 67: 1000 frames\n  Batch 68: 1000 frames\n  Batch 69: 1000 frames\n  Batch 70: 1000 frames\n  Batch 71: 1000 frames\n  Batch 72: 1000 frames\n  Batch 73: 1000 frames\n  Batch 74: 1000 frames\n  Batch 75: 1000 frames\n  Batch 76: 1000 frames\n  Batch 77: 1000 frames\n  Batch 78: 1000 frames\n  Batch 79: 1000 frames\n  Batch 80: 1000 frames\n  Batch 81: 1000 frames\n  Batch 82: 1000 frames\n  Batch 83: 1000 frames\n  Batch 84: 1000 frames\n  Batch 85: 1000 frames\n  Batch 86: 1000 frames\n  Batch 87: 1000 frames\n  Batch 88: 1000 frames\n  Batch 89: 1000 frames\n  Batch 90: 1000 frames\n  Batch 91: 1000 frames\n  Batch 92: 1000 frames\n  Batch 93: 1000 frames\n  Batch 94: 1000 frames\n  Batch 95: 1000 frames\n  Batch 96: 1000 frames\n  Batch 97: 545 frames\nâœ… Loaded 96545 frame names from 866 videos\nğŸ“¹ Sample videos: ['L21_V001', 'L21_V002', 'L21_V003', 'L21_V005', 'L21_V006']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def retrieve(query: str, topK: int, frame_ids: Optional[List] = None,\n             mode: str = \"clip\", caption_mode: str = \"bge\"):\n    \"\"\"\n    Universal retrieve function with frame filtering support\n    \"\"\"\n    if mode == \"clip\":\n        embed_model = clip_embed_model\n        index = clip_index\n        collection_name = CLIP_collection\n        query_text = translator.translate(query, source_lang=\"vi\")\n    elif mode == \"vintern\":\n        if caption_mode == \"bge\":\n            embed_model = bge_embed_model\n            index = bge_index\n            collection_name = BGE_collection\n            query_text = query\n        else:  # gte\n            embed_model = gte_embed_model\n            index = gte_index\n            collection_name = GTE_collection\n            query_text = query\n        \n    if frame_ids: \n        # Direct query with frame filtering\n        vector_query = embed_model._get_text_embedding(query_text)\n        nodes = qdrant_client.query_points(\n            collection_name=collection_name,\n            query=vector_query,\n            limit=topK,\n            with_payload=True,\n            query_filter=models.Filter(must=[\n                models.FieldCondition(\n                    key=\"id\",\n                    match=models.MatchAny(any=frame_ids)\n                )\n            ])\n        ).points\n        results = [\n            {\"id\": node.payload[\"id\"].strip(), \"score\": node.score}\n            for node in nodes\n        ]\n    else:\n        # Use index for full search\n        retriever = index.as_retriever(similarity_top_k=topK)\n        nodes = retriever.retrieve(query_text)\n        results = [\n            {\"id\": node.metadata.get(\"id\", \"\").strip(), \"score\": node.score}\n            for node in nodes\n        ]\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:45:13.162306Z","iopub.execute_input":"2025-08-30T23:45:13.162733Z","iopub.status.idle":"2025-08-30T23:45:13.175107Z","shell.execute_reply.started":"2025-08-30T23:45:13.162688Z","shell.execute_reply":"2025-08-30T23:45:13.173810Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def retrieve_frame(query: str, topK: int, mode: str = \"hybrid\", caption_mode: str = \"bge\",\n                   alpha: float = 0.5, frame_ids: Optional[List] = None):\n    \"\"\"\n    Enhanced retrieve_frame with caption mode support\n    \"\"\"\n    if mode == \"clip\":\n        clip_nodes = retrieve(query, topK, frame_ids, \"clip\")\n        results = [\n            {\"image\": node[\"id\"], \"caption\": f\"{node['id']} | Score: {node['score']:.2f}\"}\n            for node in clip_nodes\n        ]\n        return results\n    \n    elif mode == \"vintern\":\n        caption_nodes = retrieve(query, topK, frame_ids, \"vintern\", caption_mode)\n        results = [\n            {\"image\": node[\"id\"], \"caption\": f\"{node['id']} | Score: {node['score']:.2f}\"}\n            for node in caption_nodes\n        ]\n        return results\n    \n    else:  # hybrid mode\n        clip_nodes = retrieve(query, topK, frame_ids, \"clip\")\n        caption_nodes = retrieve(query, topK, frame_ids, \"vintern\", caption_mode)\n        \n        combined_scores = defaultdict(float)\n        for node in caption_nodes:\n            combined_scores[node[\"id\"]] += node[\"score\"] * alpha\n\n        for node in clip_nodes:\n            combined_scores[node[\"id\"]] += node[\"score\"] * (1 - alpha)\n\n        top_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:topK]\n\n        return [\n            {\"image\": video_id.strip(), \"caption\": f\"{video_id} | Score: {score:.2f}\"}\n            for video_id, score in top_results\n        ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:45:13.176158Z","iopub.execute_input":"2025-08-30T23:45:13.176525Z","iopub.status.idle":"2025-08-30T23:45:13.579728Z","shell.execute_reply.started":"2025-08-30T23:45:13.176492Z","shell.execute_reply":"2025-08-30T23:45:13.578213Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def retrieve_from_image(contents: bytes, topK: int):\n    \"\"\"\n    Image-based search using CLIP embeddings\n    \"\"\"\n    image = Image.open(io.BytesIO(contents)).convert(\"RGB\")\n    vector_query = clip_embed_model._get_image_embedding(image)\n\n    clip_nodes = qdrant_client.query_points(\n        collection_name=CLIP_collection,\n        query=vector_query,\n        limit=topK,\n        with_payload=True\n    ).points\n\n    results = [\n        {\n            \"image\": node.payload.get(\"id\", \"\").strip(),\n            \"caption\": f\"{node.payload.get('id', '')} | Score: {node.score:.2f}\"\n        }\n        for node in clip_nodes\n    ]\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:45:13.581067Z","iopub.execute_input":"2025-08-30T23:45:13.581488Z","iopub.status.idle":"2025-08-30T23:45:13.609802Z","shell.execute_reply.started":"2025-08-30T23:45:13.581428Z","shell.execute_reply":"2025-08-30T23:45:13.607983Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def get_vid(image_name: str) -> str:\n    \"\"\"Extract video ID from frame name\"\"\"\n    parts = image_name.split(\"_\", 2)\n    return f\"{parts[0]}_{parts[1]}\"\n\ndef temporal_search(events: List[str], topK: int = 100, \n                    mode: str = \"hybrid\", caption_mode: str = \"bge\",\n                    alpha: float = 0.5, search_mode: str = \"progressive\"):\n\n    print(f\"ğŸ” Starting {search_mode} temporal search with {len(events)} events...\")\n    \n    if search_mode == \"progressive\":\n        # Original progressive filtering approach\n        frame_ids = None\n        final_results = []\n        \n        for i, event in enumerate(events):\n            print(f\"  Event {i+1}: {event[:50]}...\")\n            \n            results = retrieve_frame(query=event, topK=topK, mode=mode, \n                                     caption_mode=caption_mode, alpha=alpha, frame_ids=frame_ids)\n            final_results.append(results)\n            \n            # Extract video IDs to narrow search space\n            video_ids = {\"_\".join(item['image'].split(\"_\")[:2]) for item in results}\n            print(f\"    â†’ Found {len(results)} results from {len(video_ids)} videos\")\n            \n            # Update frame_ids for next iteration\n            frame_ids = [f for vid in video_ids for f in VIDEO_TO_FRAMES[vid]]\n            print(f\"    â†’ Narrowed search space to {len(frame_ids)} frames\")\n        \n        print(f\"âœ… Progressive temporal search completed!\")\n        return final_results\n    \n    else:  # consolidated mode\n        # New consolidated approach - find common videos with best frames per event\n        event_results = []\n        \n        # Get results for each event independently\n        for i, event in enumerate(events):\n            print(f\"  Event {i+1}: {event[:50]}...\")\n            results = retrieve_frame(query=event, topK=topK*3, mode=mode,\n                                     caption_mode=caption_mode, alpha=alpha)\n            event_results.append(results)\n            print(f\"    â†’ Found {len(results)} results\")\n        \n        # Find videos that appear in ALL events\n        video_sets = []\n        for results in event_results:\n            video_set = {get_vid(item['image']) for item in results}\n            video_sets.append(video_set)\n        \n        common_videos = set.intersection(*video_sets) if video_sets else set()\n        print(f\"  ğŸ“¹ Found {len(common_videos)} videos appearing in all events\")\n        \n        if not common_videos:\n            print(\"  âš ï¸ No common videos found, returning top results from final event\")\n            return event_results[-1] if event_results else []\n        \n        # For each common video, get best frame per event\n        video_timeline_results = []\n        \n        for video_id in sorted(common_videos):\n            video_frames_per_event = []\n            total_score = 0\n            \n            for event_idx, results in enumerate(event_results):\n                # Find best frame for this video in this event\n                video_frames = [item for item in results if get_vid(item['image']) == video_id]\n                if video_frames:\n                    best_frame = max(video_frames, key=lambda x: float(x['caption'].split('Score: ')[1]))\n                    video_frames_per_event.append(best_frame['image'])\n                    total_score += float(best_frame['caption'].split('Score: ')[1])\n                else:\n                    video_frames_per_event.append(f\"{video_id}_0000\")\n            \n            avg_score = total_score / len(events)\n            video_timeline_results.append({\n                \"video_id\": video_id,\n                \"image\": video_frames_per_event,\n                \"caption\": [f\"Event {i+1}\" for i in range(len(events))],\n                \"score\": total_score\n            })\n        \n        # Sort by average score and return top results\n        video_timeline_results.sort(key=lambda x: x['score'], reverse=True)\n        final_consolidated = video_timeline_results[:topK]\n        \n        print(f\"âœ… Consolidated temporal search completed! Returning {len(final_consolidated)} video timelines\")\n        return final_consolidated","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T00:19:12.269195Z","iopub.execute_input":"2025-08-31T00:19:12.271081Z","iopub.status.idle":"2025-08-31T00:19:12.293539Z","shell.execute_reply.started":"2025-08-31T00:19:12.271026Z","shell.execute_reply":"2025-08-31T00:19:12.291901Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def visualize_video_results(results, root_dir=\"/kaggle/input/aic-batch-1/keyframes/keyframes\", top_k=10):\n    \"\"\"\n    Enhanced visualization function for TRAKE mode temporal search results\n    Displays video timelines with frames from each event in sequence\n    \n    Args:\n        results: Results from temporal_search with search_mode=\"consolidated\"\n        root_dir: Root directory containing keyframe images\n        top_k: Number of top video timelines to display\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import matplotlib.image as mpimg\n    from matplotlib.patches import Rectangle\n    import numpy as np\n    import os\n    \n    if not results:\n        print(\"âŒ No results to visualize\")\n        return\n    \n    # Handle both progressive and consolidated result formats\n    if isinstance(results[0], dict) and 'video_id' in results[0]:\n        # Consolidated format - video timelines\n        video_results = results[:top_k]\n        print(f\"ğŸ¬ Visualizing {len(video_results)} video timelines for TRAKE mode\")\n        \n        fig, axes = plt.subplots(len(video_results), len(video_results[0]['image']), \n                                figsize=(4 * len(video_results[0]['image']), 3 * len(video_results)))\n        fig.suptitle(f'TRAKE Mode: Top {len(video_results)} Video Event Sequences', fontsize=16, fontweight='bold')\n        \n        for video_idx, video_info in enumerate(video_results):\n            frames = video_info[\"image\"]\n            video_id = video_info[\"video_id\"] \n            total_score = video_info[\"score\"]\n            \n            for event_idx, frame_name in enumerate(frames):\n                if len(video_results) == 1:\n                    ax = axes[event_idx] if len(frames) > 1 else axes\n                else:\n                    ax = axes[video_idx, event_idx] if len(frames) > 1 else axes[video_idx]\n                \n                # Construct image path\n                img_path = os.path.join(root_dir, frame_name + '.jpg')\n                \n                try:\n                    if os.path.exists(img_path):\n                        img = mpimg.imread(img_path)\n                        ax.imshow(img)\n                    else:\n                        # Create placeholder if image not found\n                        ax.text(0.5, 0.5, f'Image not found:\\\\n{frame_name}', \n                               ha='center', va='center', transform=ax.transAxes,\n                               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n                        \n                except Exception as e:\n                    ax.text(0.5, 0.5, f'Error loading:\\\\n{frame_name}', \n                           ha='center', va='center', transform=ax.transAxes,\n                           bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"))\n                \n                # Add frame info and event marker\n                ax.set_title(f'Event {event_idx + 1}\\\\n{frame_name}', fontsize=10)\n                ax.axis('off')\n                \n                # Add colored border for event sequence\n                colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n                border_color = colors[event_idx % len(colors)]\n                rect = Rectangle((0, 0), 1, 1, transform=ax.transAxes, \n                               linewidth=3, edgecolor=border_color, facecolor='none')\n                ax.add_patch(rect)\n            \n            # Add video info as row label\n            if len(video_results) > 1:\n                axes[video_idx, 0].text(-0.1, 0.5, \n                                       f'{video_id}\\\\nScore: {total_score:.3f}', \n                                       ha='right', va='center', transform=axes[video_idx, 0].transAxes,\n                                       fontsize=12, fontweight='bold',\n                                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n        \n        plt.tight_layout()\n        plt.show()\n        \n       \n        \n    else:\n        # Progressive format - convert to simple grid visualization\n        print(f\"ğŸ“‹ Visualizing progressive search results ({len(results)} events)\")\n        \n        # Take top frames from final event for visualization\n        final_event_results = results[-1][:top_k] if results else []\n        \n        if not final_event_results:\n            print(\"âŒ No final results to visualize\")\n            return\n            \n        cols = min(5, len(final_event_results))\n        rows = (len(final_event_results) + cols - 1) // cols\n        \n        fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n        fig.suptitle(f'Progressive Search: Final Results ({len(results)} events)', fontsize=14)\n        \n        if rows == 1 and cols == 1:\n            axes = [axes]\n        elif rows == 1:\n            axes = [axes]\n        else:\n            axes = axes.flatten() if rows > 1 else axes\n            \n        for idx, result in enumerate(final_event_results):\n            if idx >= len(axes):\n                break\n                \n            ax = axes[idx]\n            frame_name = result['image']\n            img_path = os.path.join(root_dir, frame_name + '.jpg')\n            \n            try:\n                if os.path.exists(img_path):\n                    img = mpimg.imread(img_path)\n                    ax.imshow(img)\n                else:\n                    ax.text(0.5, 0.5, f'Image not found:\\\\n{frame_name}', \n                           ha='center', va='center', transform=ax.transAxes)\n            except Exception as e:\n                ax.text(0.5, 0.5, f'Error loading:\\\\n{frame_name}', \n                       ha='center', va='center', transform=ax.transAxes)\n            \n            ax.set_title(f'{frame_name}\\\\n{result[\"caption\"].split(\"|\")[1]}', fontsize=9)\n            ax.axis('off')\n        \n        # Hide unused subplots\n        for idx in range(len(final_event_results), len(axes)):\n            axes[idx].set_visible(False)\n        \n        plt.tight_layout()\n        plt.show()\n\nprint(\"âœ… Video visualization function\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T00:19:23.453426Z","iopub.execute_input":"2025-08-31T00:19:23.453834Z","iopub.status.idle":"2025-08-31T00:19:23.482010Z","shell.execute_reply.started":"2025-08-31T00:19:23.453808Z","shell.execute_reply":"2025-08-31T00:19:23.480307Z"}},"outputs":[{"name":"stdout","text":"âœ… Video visualization function added for enhanced TRAKE mode\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# FastAPI Application\napp = FastAPI(title=\"Video Event Retrieval API v2.0\", \n              description=\"Enhanced multimodal search with temporal capabilities\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=CORS_SETTINGS[\"allow_origins\"],\n    allow_credentials=CORS_SETTINGS[\"allow_credentials\"],\n    allow_methods=CORS_SETTINGS[\"allow_methods\"],\n    allow_headers=CORS_SETTINGS[\"allow_headers\"],\n)\n\nrouter = APIRouter()\n\n@router.post(\"/search\")\nasync def api_search(\n    query: Optional[str] = Form(None),\n    topK: int = Form(...),\n    mode: str = Form(\"hybrid\"),\n    caption_mode: str = Form(\"bge\"),\n    alpha: float = Form(0.5),\n    file: UploadFile = File(None)\n):\n    \"\"\"\n    Enhanced search API with caption mode support\n    - mode: hybrid, clip, vintern, image\n    - caption_mode: bge, gte (for vintern and hybrid modes)\n    - alpha: text/visual balance for hybrid mode (0.1-0.9)\n    \"\"\"\n    start_time = time.time()\n    \n    try:\n        if mode == \"image\":\n            if file is None:\n                return {\"error\": \"No file uploaded for image mode\"}\n            contents = await file.read()\n            results = retrieve_from_image(contents=contents, topK=topK)\n            search_info = f\"IMAGE search\"\n        else:\n            if query is None or query.strip() == \"\":\n                return {\"error\": \"No query provided for text mode\"}\n            results = retrieve_frame(query=query, topK=topK, mode=mode, \n                                    caption_mode=caption_mode, alpha=alpha)\n            search_info = f\"{mode.upper()} mode with {caption_mode.upper()} model\"\n        \n        duration = time.time() - start_time\n        \n        return {\n            \"results\": results,\n            \"search_info\": {\n                \"mode\": mode,\n                \"caption_mode\": caption_mode if mode in [\"hybrid\", \"vintern\"] else None,\n                \"alpha\": alpha if mode == \"hybrid\" else None,\n                \"duration\": round(duration, 3),\n                \"count\": len(results),\n                \"description\": search_info\n            }\n        }\n    except Exception as e:\n        return {\"error\": f\"Search failed: {str(e)}\"}\n\n@router.post(\"/temporal_search\")\nasync def api_temporal_search(\n    events: str = Form(...),  # JSON string of event list\n    topK: int = Form(100),\n    mode: str = Form(\"hybrid\"),\n    caption_mode: str = Form(\"bge\"),\n    alpha: float = Form(0.5),\n    search_mode: str = Form(\"progressive\")  # New parameter: \"progressive\" or \"consolidated\"\n):\n    \"\"\"\n    Enhanced Temporal search API for TRAKE mode\n    - events: JSON array of sequential event descriptions\n    - search_mode: \"progressive\" (frontend compatible) or \"consolidated\" (TRAKE visualization)\n    - Returns different result formats based on search_mode\n    \"\"\"\n    start_time = time.time()\n    \n    try:\n        events_list = json.loads(events)\n        if not isinstance(events_list, list) or len(events_list) == 0:\n            return {\"error\": \"Events must be a non-empty list\"}\n        \n        # Filter out empty events\n        valid_events = [e.strip() for e in events_list if e.strip()]\n        if len(valid_events) == 0:\n            return {\"error\": \"No valid events provided\"}\n        \n        results = temporal_search(events=valid_events, topK=topK, mode=mode,\n                                 caption_mode=caption_mode, alpha=alpha, \n                                 search_mode=search_mode)\n        \n        duration = time.time() - start_time\n        \n        # Different response format based on search mode\n        if search_mode == \"consolidated\":\n            final_count = len(results)\n            result_type = \"video_timelines\"\n        else:  # progressive\n            final_count = len(results[-1]) if results else 0\n            result_type = \"progressive_events\"\n        \n        return {\n            \"results\": results,\n            \"search_info\": {\n                \"mode\": mode,\n                \"caption_mode\": caption_mode,\n                \"alpha\": alpha if mode == \"hybrid\" else None,\n                \"search_mode\": search_mode,\n                \"duration\": round(duration, 3),\n                \"events_processed\": len(valid_events),\n                \"final_count\": final_count,\n                \"result_type\": result_type,\n                \"description\": f\"{search_mode.title()} temporal search through {len(valid_events)} events\"\n            }\n        }\n    except json.JSONDecodeError:\n        return {\"error\": \"Invalid JSON format for events\"}\n    except Exception as e:\n        return {\"error\": f\"Temporal search failed: {str(e)}\"}\n\n@router.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"device\": DEVICE,\n        \"models_loaded\": {\n            \"clip\": clip_embed_model is not None,\n            \"bge\": bge_embed_model is not None,\n            \"gte\": gte_embed_model is not None,\n            \"translator\": translator is not None\n        },\n        \"collections\": {\n            \"clip\": CLIP_collection,\n            \"bge\": BGE_collection,\n            \"gte\": GTE_collection\n        },\n        \"frame_count\": len(FRAME_NAMES),\n        \"video_count\": len(VIDEO_TO_FRAMES),\n        \"supported_search_modes\": [\"progressive\", \"consolidated\"]\n    }\n\napp.include_router(router)\nprint(\"âœ… FastAPI application configured with consolidated mode support\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T00:19:26.073361Z","iopub.execute_input":"2025-08-31T00:19:26.073738Z","iopub.status.idle":"2025-08-31T00:19:26.122254Z","shell.execute_reply.started":"2025-08-31T00:19:26.073714Z","shell.execute_reply":"2025-08-31T00:19:26.121179Z"}},"outputs":[{"name":"stdout","text":"âœ… FastAPI application configured with consolidated mode support\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Server setup with ngrok\nimport os, time, threading, socket\nfrom pyngrok import ngrok\nimport uvicorn\n\nPORT = 8000\nHOST = \"0.0.0.0\"\n\n# Set ngrok auth token\nif NGROK_AUTH_TOKEN and NGROK_AUTH_TOKEN != \"YOUR_NGROK_TOKEN_HERE\":\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n    print(\"âœ… Ngrok auth token set\")\nelse:\n    print(\"âš ï¸  NGROK_AUTH_TOKEN not configured. Please update with your token.\")\n    print(\"   Get your token from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n\ndef is_port_in_use(port: int, host=\"127.0.0.1\") -> bool:\n    \"\"\"Check if a local TCP port is already in use.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        return s.connect_ex((host, port)) == 0\n\ndef run_server():\n    \"\"\"Run FastAPI server in background thread\"\"\"\n    uvicorn.run(app, host=HOST, port=PORT, log_level=\"info\")\n\n# Start server only if not already running\nif not is_port_in_use(PORT):\n    print(f\"ğŸš€ Starting FastAPI server on {HOST}:{PORT}\")\n    server_thread = threading.Thread(target=run_server, daemon=True)\n    server_thread.start()\n    time.sleep(3)  # Wait for server startup\n    print(\"âœ… Server started successfully\")\nelse:\n    print(f\"ğŸ” Server already running on http://localhost:{PORT}\")\n\n# Setup ngrok tunnel\ntry:\n    # Clean up existing tunnels\n    for t in ngrok.get_tunnels():\n        addr = (t.config or {}).get(\"addr\", \"\")\n        if str(PORT) in addr:\n            try:\n                ngrok.disconnect(t.public_url)\n            except Exception:\n                pass\n\n    # Kill all tunnels if too many\n    if len(ngrok.get_tunnels()) >= 3:\n        ngrok.kill()\n\n    # Create new tunnel\n    tunnel = ngrok.connect(addr=PORT, proto=\"http\", bind_tls=True)\n    PUBLIC_URL = tunnel.public_url\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"ğŸŒ BACKEND READY!\")\n    print(f\"ğŸ“¡ Public URL: {PUBLIC_URL}\")\n    print(f\"ğŸ“– API Docs: {PUBLIC_URL}/docs\")\n    print(f\"ğŸ¥ Health Check: {PUBLIC_URL}/health\")\n    print(f\"ğŸ’» Local URL: http://localhost:{PORT}\")\n    print(\"\\nğŸ¯ COPY THE PUBLIC URL TO YOUR FRONTEND!\")\n    print(\"=\"*60)\n\n    # Save to global for later use\n    globals()[\"PUBLIC_URL\"] = PUBLIC_URL\n\nexcept Exception as e:\n    print(f\"âŒ Ngrok tunnel failed: {e}\")\n    print(f\"ğŸ”§ Server still available locally: http://localhost:{PORT}\")\n    print(\"ğŸ’¡ Try restarting the kernel or checking your ngrok auth token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T00:19:32.254671Z","iopub.execute_input":"2025-08-31T00:19:32.255074Z","iopub.status.idle":"2025-08-31T00:19:32.529500Z","shell.execute_reply.started":"2025-08-31T00:19:32.255049Z","shell.execute_reply":"2025-08-31T00:19:32.528418Z"}},"outputs":[{"name":"stdout","text":"âœ… Ngrok auth token set\nğŸ” Server already running on http://localhost:8000\n\n============================================================\nğŸŒ BACKEND READY!\nğŸ“¡ Public URL: https://b120ed32e79a.ngrok-free.app\nğŸ“– API Docs: https://b120ed32e79a.ngrok-free.app/docs\nğŸ¥ Health Check: https://b120ed32e79a.ngrok-free.app/health\nğŸ’» Local URL: http://localhost:8000\n\nğŸ¯ COPY THE PUBLIC URL TO YOUR FRONTEND!\n============================================================\nINFO:     14.234.51.109:0 - \"OPTIONS /temporal_search HTTP/1.1\" 200 OK\nğŸ” Starting consolidated temporal search with 3 events...\n  Event 1: NgÆ°á»i Ä‘áº§u báº¿p cho cÃ¡ vÃ o má»™t tÃ´ mÃ u tráº¯ng. HÃ£y láº¥y...\n    â†’ Found 300 results\n  Event 2: NgÆ°á»i Ä‘áº§u báº¿p Ä‘á»• bá»™t vÃ o má»™t tÃ´ cÃ¡ Ä‘á»ƒ chiÃªn. HÃ£y l...\n    â†’ Found 300 results\n  Event 3: Tiáº¿p theo, ngÆ°á»i Ä‘áº§u báº¿p nÃ y dÃ¹ng Ä‘Å©a Ä‘á»ƒ kiá»ƒm tra ...\n    â†’ Found 300 results\n  ğŸ“¹ Found 40 videos appearing in all events\nâœ… Consolidated temporal search completed! Returning 40 video timelines\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nğŸ” Starting consolidated temporal search with 3 events...\n  Event 1: NgÆ°á»i Ä‘áº§u báº¿p cho cÃ¡ vÃ o má»™t tÃ´ mÃ u tráº¯ng. HÃ£y láº¥y...\n    â†’ Found 600 results\n  Event 2: NgÆ°á»i Ä‘áº§u báº¿p Ä‘á»• bá»™t vÃ o má»™t tÃ´ cÃ¡ Ä‘á»ƒ chiÃªn. HÃ£y l...\n    â†’ Found 600 results\n  Event 3: Tiáº¿p theo, ngÆ°á»i Ä‘áº§u báº¿p nÃ y dÃ¹ng Ä‘Å©a Ä‘á»ƒ kiá»ƒm tra ...\n    â†’ Found 600 results\n  ğŸ“¹ Found 136 videos appearing in all events\nâœ… Consolidated temporal search completed! Returning 136 video timelines\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Test the enhanced API\nimport requests\n\ndef test_api_endpoints(use_public_url=True):\n    \"\"\"Test all API endpoints with the new features\"\"\"\n    \n    if use_public_url and 'PUBLIC_URL' in globals():\n        base_url = globals()['PUBLIC_URL']\n        print(f\"ğŸ”— Testing public URL: {base_url}\")\n    else:\n        base_url = \"http://localhost:8000\"\n        print(f\"ğŸ”— Testing local URL: {base_url}\")\n    \n    # Test health endpoint\n    print(\"\\n1. ğŸ¥ Testing health endpoint...\")\n    try:\n        response = requests.get(f\"{base_url}/health\", timeout=10)\n        if response.status_code == 200:\n            health = response.json()\n            print(f\"   âœ… Health check passed\")\n            print(f\"   ğŸ“Š {health['frame_count']} frames from {health['video_count']} videos\")\n            print(f\"   ğŸ¤– Models: {health['models_loaded']}\")\n        else:\n            print(f\"   âŒ Health check failed: {response.status_code}\")\n    except Exception as e:\n        print(f\"   âŒ Health check error: {e}\")\n        return\n    \n    # Test search with BGE\n    print(\"\\n2. ğŸ” Testing hybrid search with BGE...\")\n    test_search(base_url, \"ngÆ°á»i Ä‘ang náº¥u Äƒn\", mode=\"hybrid\", caption_mode=\"bge\")\n    \n    # Test search with GTE\n    print(\"\\n3. ğŸ” Testing hybrid search with GTE...\")\n    test_search(base_url, \"ngÆ°á»i Ä‘ang náº¥u Äƒn\", mode=\"hybrid\", caption_mode=\"gte\")\n    \n    # Test vintern only with GTE\n    print(\"\\n4. ğŸ“ Testing vintern search with GTE...\")\n    test_search(base_url, \"cáº£nh Ä‘áº¹p thiÃªn nhiÃªn\", mode=\"vintern\", caption_mode=\"gte\")\n    \n    # Test temporal search\n    print(\"\\n5. â° Testing temporal search...\")\n    test_temporal_search(base_url, [\n        \"Má»™t ngÆ°á»i  Ä‘ang cáº¯t Ä‘Ã´i á»• bÃ¡nh mÃ¬ cÃ³ ráº¯c mÃ¨ rá»“i Ä‘em nÆ°á»›ng trÃªn cháº£o. HÃ£y láº¥y khoáº£nh kháº¯c chiáº¿c dao cáº¯t qua hoÃ n toÃ n chiáº¿c bÃ¡nh.\",\n        \"Sau Ä‘Ã³ ngÆ°á»i nÃ y ráº¯c bá»™t lÃªn nhá»¯ng miáº¿ng thá»‹t, trong quÃ¡ trÃ¬nh nÃ y ngÆ°á»i Ä‘áº§u báº¿p láº­t nhá»¯ng miáº¿ng thá»‹t Ä‘á»ƒ ráº¯c bá»™t Ä‘á»u hai máº·t. HÃ£y láº¥y khoáº£nh kháº¯c Ä‘áº§u tiÃªn ngÆ°á»i Ä‘áº§u báº¿p nÃ y buÃ´ng tay khá»i miáº¿ng thá»‹t sau khi láº­t miáº¿ng thá»‹t Ä‘áº§u tiÃªn.\",\n        \"CÃ¡c miáº¿ng thá»‹t sau Ä‘Ã³ Ä‘Æ°á»£c Ä‘em Ä‘i Ã¡p cháº£o cÃ¹ng vá»›i bÆ¡ (3 ngang 1 dá»c theo chiá»u cá»§a camera). HÃ£y láº¥y khoáº£nh kháº¯c Ä‘áº§u tiÃªn ngÆ°á»i Ä‘áº§u báº¿p cáº§m vÃ o cháº£o Ä‘á»ƒ nháº¥c lÃªn Ä‘áº£o bÆ¡ Ä‘á»u xung quanh\"\n    ])\n\ndef test_search(base_url, query, mode=\"hybrid\", caption_mode=\"bge\", topK=5):\n    \"\"\"Test search endpoint\"\"\"\n    try:\n        data = {\n            \"query\": query,\n            \"topK\": topK,\n            \"mode\": mode,\n            \"caption_mode\": caption_mode,\n            \"alpha\": 0.6\n        }\n        \n        response = requests.post(f\"{base_url}/search\", data=data, timeout=30)\n        \n        if response.status_code == 200:\n            result = response.json()\n            search_info = result.get(\"search_info\", {})\n            print(f\"   âœ… Search successful: {search_info.get('description')}\")\n            print(f\"   â±ï¸ Duration: {search_info.get('duration')}s\")\n            print(f\"   ğŸ“Š Results: {len(result['results'])}\")\n            \n            # Show top results\n            for i, res in enumerate(result['results'][:3]):\n                print(f\"      {i+1}. {res['caption']}\")\n        else:\n            print(f\"   âŒ Search failed: {response.status_code} - {response.text}\")\n    except Exception as e:\n        print(f\"   âŒ Search error: {e}\")\n\ndef test_temporal_search(base_url, events, topK=20):\n    \"\"\"Test temporal search endpoint\"\"\"\n    try:\n        data = {\n            \"events\": json.dumps(events),\n            \"topK\": topK,\n            \"mode\": \"hybrid\",\n            \"caption_mode\": \"gte\",\n            \"alpha\": 0.7\n        }\n        \n        response = requests.post(f\"{base_url}/temporal_search\", data=data, timeout=60)\n        \n        if response.status_code == 200:\n            result = response.json()\n            search_info = result.get(\"search_info\", {})\n            print(f\"   âœ… Temporal search successful: {search_info.get('description')}\")\n            print(f\"   â±ï¸ Duration: {search_info.get('duration')}s\")\n            print(f\"   ğŸ“Š Events processed: {search_info.get('events_processed')}\")\n            print(f\"   ğŸ¯ Final results: {search_info.get('final_count')}\")\n            \n            # Show progression\n            for i, event_results in enumerate(result['results']):\n                print(f\"      Event {i+1}: {len(event_results)} results\")\n                for j, res in enumerate(event_results[:2]):\n                    print(f\"        â†’ {res['caption']}\")\n        else:\n            print(f\"   âŒ Temporal search failed: {response.status_code} - {response.text}\")\n    except Exception as e:\n        print(f\"   âŒ Temporal search error: {e}\")\n\n# Run tests\nprint(\"ğŸ§ª Testing Enhanced API...\")\ntest_api_endpoints()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T23:45:19.270410Z","iopub.execute_input":"2025-08-30T23:45:19.270740Z","iopub.status.idle":"2025-08-30T23:45:33.172507Z","shell.execute_reply.started":"2025-08-30T23:45:19.270718Z","shell.execute_reply":"2025-08-30T23:45:33.170827Z"}},"outputs":[{"name":"stdout","text":"ğŸ§ª Testing Enhanced API...\nğŸ”— Testing public URL: https://5d686cb1624e.ngrok-free.app\n\n1. ğŸ¥ Testing health endpoint...\nINFO:     34.34.86.60:0 - \"GET /health HTTP/1.1\" 200 OK\n   âœ… Health check passed\n   ğŸ“Š 96545 frames from 866 videos\n   ğŸ¤– Models: {'clip': True, 'bge': True, 'gte': True, 'translator': True}\n\n2. ğŸ” Testing hybrid search with BGE...\nINFO:     34.34.86.60:0 - \"POST /search HTTP/1.1\" 200 OK\n   âœ… Search successful: HYBRID mode with BGE model\n   â±ï¸ Duration: 2.064s\n   ğŸ“Š Results: 5\n      1. L26_V272_5537 | Score: 0.42\n      2. L26_V452_4878 | Score: 0.42\n      3. L26_V339_5215 | Score: 0.41\n\n3. ğŸ” Testing hybrid search with GTE...\nINFO:     34.34.86.60:0 - \"POST /search HTTP/1.1\" 200 OK\n   âœ… Search successful: HYBRID mode with GTE model\n   â±ï¸ Duration: 1.145s\n   ğŸ“Š Results: 5\n      1. L27_V003_6294 | Score: 0.38\n      2. L26_V367_5958 | Score: 0.38\n      3. L26_V484_3594 | Score: 0.37\n\n4. ğŸ“ Testing vintern search with GTE...\nINFO:     34.34.86.60:0 - \"POST /search HTTP/1.1\" 200 OK\n   âœ… Search successful: VINTERN mode with GTE model\n   â±ï¸ Duration: 0.119s\n   ğŸ“Š Results: 5\n      1. L27_V008_12041 | Score: 0.68\n      2. L28_V016_5421 | Score: 0.66\n      3. L28_V018_13549 | Score: 0.63\n\n5. â° Testing temporal search...\nğŸ” Starting progressive temporal search with 3 events...\n  Event 1: Má»™t ngÆ°á»i  Ä‘ang cáº¯t Ä‘Ã´i á»• bÃ¡nh mÃ¬ cÃ³ ráº¯c mÃ¨ rá»“i Ä‘e...\n    â†’ Found 20 results from 18 videos\n    â†’ Narrowed search space to 1814 frames\n  Event 2: Sau Ä‘Ã³ ngÆ°á»i nÃ y ráº¯c bá»™t lÃªn nhá»¯ng miáº¿ng thá»‹t, tro...\n    â†’ Found 20 results from 7 videos\n    â†’ Narrowed search space to 732 frames\n  Event 3: CÃ¡c miáº¿ng thá»‹t sau Ä‘Ã³ Ä‘Æ°á»£c Ä‘em Ä‘i Ã¡p cháº£o cÃ¹ng vá»›i...\n    â†’ Found 20 results from 4 videos\n    â†’ Narrowed search space to 433 frames\nâœ… Progressive temporal search completed!\nINFO:     34.34.86.60:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n   âœ… Temporal search successful: Progressive temporal search through 3 events\n   â±ï¸ Duration: 8.764s\n   ğŸ“Š Events processed: 3\n   ğŸ¯ Final results: 20\n      Event 1: 20 results\n        â†’ L26_V289_1948 | Score: 0.61\n        â†’ L26_V399_5336 | Score: 0.58\n      Event 2: 20 results\n        â†’ L26_V399_2790 | Score: 0.43\n        â†’ L26_V251_2204 | Score: 0.43\n      Event 3: 20 results\n        â†’ L26_V247_4538 | Score: 0.52\n        â†’ L26_V399_4814 | Score: 0.49\nINFO:     14.234.51.109:0 - \"OPTIONS /temporal_search HTTP/1.1\" 200 OK\nğŸ” Starting consolidated temporal search with 3 events...\n  Event 1: NgÆ°á»i Ä‘áº§u báº¿p cho cÃ¡ vÃ o má»™t tÃ´ mÃ u tráº¯ng. HÃ£y láº¥y...\n    â†’ Found 30 results\n  Event 2: NgÆ°á»i Ä‘áº§u báº¿p Ä‘á»• bá»™t vÃ o má»™t tÃ´ cÃ¡ Ä‘á»ƒ chiÃªn. HÃ£y l...\n    â†’ Found 30 results\n  Event 3: Tiáº¿p theo, ngÆ°á»i Ä‘áº§u báº¿p nÃ y dÃ¹ng Ä‘Å©a Ä‘á»ƒ kiá»ƒm tra ...\n    â†’ Found 30 results\n  ğŸ“¹ Found 0 videos appearing in all events\n  âš ï¸ No common videos found, returning top results from final event\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nğŸ” Starting consolidated temporal search with 3 events...\n  Event 1: NgÆ°á»i Ä‘áº§u báº¿p cho cÃ¡ vÃ o má»™t tÃ´ mÃ u tráº¯ng. HÃ£y láº¥y...\n    â†’ Found 30 results\n  Event 2: NgÆ°á»i Ä‘áº§u báº¿p Ä‘á»• bá»™t vÃ o má»™t tÃ´ cÃ¡ Ä‘á»ƒ chiÃªn. HÃ£y l...\n    â†’ Found 30 results\n  Event 3: Tiáº¿p theo, ngÆ°á»i Ä‘áº§u báº¿p nÃ y dÃ¹ng Ä‘Å©a Ä‘á»ƒ kiá»ƒm tra ...\n    â†’ Found 30 results\n  ğŸ“¹ Found 0 videos appearing in all events\n  âš ï¸ No common videos found, returning top results from final event\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nğŸ” Starting consolidated temporal search with 3 events...\n  Event 1: NgÆ°á»i Ä‘áº§u báº¿p cho cÃ¡ vÃ o má»™t tÃ´ mÃ u tráº¯ng. HÃ£y láº¥y...\n    â†’ Found 600 results\n  Event 2: NgÆ°á»i Ä‘áº§u báº¿p Ä‘á»• bá»™t vÃ o má»™t tÃ´ cÃ¡ Ä‘á»ƒ chiÃªn. HÃ£y l...\n    â†’ Found 600 results\n  Event 3: Tiáº¿p theo, ngÆ°á»i Ä‘áº§u báº¿p nÃ y dÃ¹ng Ä‘Å©a Ä‘á»ƒ kiá»ƒm tra ...\n    â†’ Found 600 results\n  ğŸ“¹ Found 136 videos appearing in all events\nâœ… Consolidated temporal search completed! Returning 136 video timelines\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nğŸ” Starting consolidated temporal search with 3 events...\n  Event 1: NgÆ°á»i Ä‘áº§u báº¿p cho cÃ¡ vÃ o má»™t tÃ´ mÃ u tráº¯ng. HÃ£y láº¥y...\n    â†’ Found 30 results\n  Event 2: NgÆ°á»i Ä‘áº§u báº¿p Ä‘á»• bá»™t vÃ o má»™t tÃ´ cÃ¡ Ä‘á»ƒ chiÃªn. HÃ£y l...\n    â†’ Found 30 results\n  Event 3: Tiáº¿p theo, ngÆ°á»i Ä‘áº§u báº¿p nÃ y dÃ¹ng Ä‘Å©a Ä‘á»ƒ kiá»ƒm tra ...\n    â†’ Found 30 results\n  ğŸ“¹ Found 0 videos appearing in all events\n  âš ï¸ No common videos found, returning top results from final event\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nğŸ” Starting consolidated temporal search with 3 events...\n  Event 1: NgÆ°á»i Ä‘áº§u báº¿p cho cÃ¡ vÃ o má»™t tÃ´ mÃ u tráº¯ng. HÃ£y láº¥y...\n    â†’ Found 300 results\n  Event 2: NgÆ°á»i Ä‘áº§u báº¿p Ä‘á»• bá»™t vÃ o má»™t tÃ´ cÃ¡ Ä‘á»ƒ chiÃªn. HÃ£y l...\n    â†’ Found 300 results\n  Event 3: Tiáº¿p theo, ngÆ°á»i Ä‘áº§u báº¿p nÃ y dÃ¹ng Ä‘Å©a Ä‘á»ƒ kiá»ƒm tra ...\n    â†’ Found 300 results\n  ğŸ“¹ Found 40 videos appearing in all events\nâœ… Consolidated temporal search completed! Returning 40 video timelines\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nğŸ” Starting consolidated temporal search with 3 events...\n  Event 1: NgÆ°á»i Ä‘áº§u báº¿p cho cÃ¡ vÃ o má»™t tÃ´ mÃ u tráº¯ng. HÃ£y láº¥y...\n    â†’ Found 300 results\n  Event 2: NgÆ°á»i Ä‘áº§u báº¿p Ä‘á»• bá»™t vÃ o má»™t tÃ´ cÃ¡ Ä‘á»ƒ chiÃªn. HÃ£y l...\n    â†’ Found 300 results\n  Event 3: Tiáº¿p theo, ngÆ°á»i Ä‘áº§u báº¿p nÃ y dÃ¹ng Ä‘Å©a Ä‘á»ƒ kiá»ƒm tra ...\n    â†’ Found 300 results\n  ğŸ“¹ Found 40 videos appearing in all events\nâœ… Consolidated temporal search completed! Returning 40 video timelines\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"OPTIONS /temporal_search HTTP/1.1\" 200 OK\nğŸ” Starting progressive temporal search with 3 events...\n  Event 1: NgÆ°á»i Ä‘áº§u báº¿p cho cÃ¡ vÃ o má»™t tÃ´ mÃ u tráº¯ng. HÃ£y láº¥y...\n    â†’ Found 100 results from 76 videos\n    â†’ Narrowed search space to 8141 frames\n  Event 2: NgÆ°á»i Ä‘áº§u báº¿p Ä‘á»• bá»™t vÃ o má»™t tÃ´ cÃ¡ Ä‘á»ƒ chiÃªn. HÃ£y l...\n    â†’ Found 100 results from 53 videos\n    â†’ Narrowed search space to 5654 frames\n  Event 3: Tiáº¿p theo, ngÆ°á»i Ä‘áº§u báº¿p nÃ y dÃ¹ng Ä‘Å©a Ä‘á»ƒ kiá»ƒm tra ...\n    â†’ Found 100 results from 28 videos\n    â†’ Narrowed search space to 2704 frames\nâœ… Progressive temporal search completed!\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nğŸ” Starting consolidated temporal search with 3 events...\n  Event 1: NgÆ°á»i Ä‘áº§u báº¿p cho cÃ¡ vÃ o má»™t tÃ´ mÃ u tráº¯ng. HÃ£y láº¥y...\n    â†’ Found 300 results\n  Event 2: NgÆ°á»i Ä‘áº§u báº¿p Ä‘á»• bá»™t vÃ o má»™t tÃ´ cÃ¡ Ä‘á»ƒ chiÃªn. HÃ£y l...\n    â†’ Found 300 results\n  Event 3: Tiáº¿p theo, ngÆ°á»i Ä‘áº§u báº¿p nÃ y dÃ¹ng Ä‘Å©a Ä‘á»ƒ kiá»ƒm tra ...\n    â†’ Found 300 results\n  ğŸ“¹ Found 40 videos appearing in all events\nâœ… Consolidated temporal search completed! Returning 40 video timelines\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}