{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fastapi uvicorn sentence-transformers open-clip-torch qdrant-client \\\n    llama-index llama-index-vector-stores-qdrant llama-index-core pyngrok -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T14:56:30.980544Z","iopub.execute_input":"2025-08-29T14:56:30.980862Z","iopub.status.idle":"2025-08-29T14:59:05.915003Z","shell.execute_reply.started":"2025-08-29T14:56:30.980836Z","shell.execute_reply":"2025-08-29T14:59:05.912447Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from fastapi import FastAPI, UploadFile, File, Form, Depends, APIRouter\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom qdrant_client import QdrantClient, models\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.core import VectorStoreIndex\nfrom collections import defaultdict\nimport heapq\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nimport re\nfrom pydantic import PrivateAttr\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom typing import List, Optional\nimport open_clip\nfrom pydantic import BaseModel\nimport json\nfrom PIL import Image\nimport hashlib\nimport io\nimport json\nfrom collections import defaultdict\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T14:59:05.919165Z","iopub.execute_input":"2025-08-29T14:59:05.919627Z","iopub.status.idle":"2025-08-29T15:00:04.059809Z","shell.execute_reply.started":"2025-08-29T14:59:05.919579Z","shell.execute_reply":"2025-08-29T15:00:04.058031Z"}},"outputs":[{"name":"stderr","text":"2025-08-29 14:59:35.383267: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756479575.716340      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756479575.815630      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class CLIPEmbedding(BaseEmbedding):\n    _model = PrivateAttr()\n    _preprocess = PrivateAttr()\n    _tokenizer = PrivateAttr()\n    _device = PrivateAttr()\n\n    def __init__(self, model_name: str = \"ViT-H-14-quickgelu\", device: str = \"cpu\"):\n        super().__init__()\n        self._device = device\n        self._model, _, self._preprocess = open_clip.create_model_and_transforms(\n            model_name=model_name,\n            pretrained=\"dfn5b\",\n            device=self._device\n        )\n        self._tokenizer = open_clip.get_tokenizer(model_name)\n        self._model = self._model.to(self._device).eval()\n\n    def _encode_text(self, text: str) -> List[float]:\n        tokens = self._tokenizer([text]).to(self._device)\n        with torch.no_grad():\n            emb = self._model.encode_text(tokens)\n            emb = emb / emb.norm(dim=-1, keepdim=True) \n        return emb[0].cpu().numpy().tolist()\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        return self._encode_text(query)\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        return self._encode_text(text)\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        return self._get_text_embedding(text)\n\n    def _encode_image(self, image: Image.Image) -> List[float]:\n        image_tensor = self._preprocess(image).unsqueeze(0).to(self._device)\n        with torch.no_grad():\n            emb = self._model.encode_image(image_tensor)\n            emb = emb / emb.norm(dim=-1, keepdim=True)\n        return emb[0].cpu().numpy().tolist()\n        \n    def _get_image_embedding(self, image: Image.Image) -> List[float]:\n            return self._encode_image(image)\n    \n    async def _aget_image_embedding(self, image: Image.Image) -> List[float]:\n        return self._get_image_embedding(image)\n\nclass CaptionEmbedding(BaseEmbedding):\n    _model: SentenceTransformer = PrivateAttr()\n\n    def __init__(self, model_name: str = \"BAAI/bge-small-en\", device: str = \"cpu\", trust_remote_code: bool = False):\n        super().__init__()\n        print(f\"Loading model: {model_name}\")\n        self._model = SentenceTransformer(model_name, device=device, \n                                          trust_remote_code=trust_remote_code)\n        self._model = self._model.eval()\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        return self._model.encode(query, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False).tolist()\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        return self._model.encode(text, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False).tolist()\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        return self._get_text_embedding(text)\n\nclass Translator:\n    def __init__(self, model_name: str = \"VietAI/envit5-translation\", device: str = 'cpu'):\n        self.device = device\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n\n    def _clean_prefix(self, text: str) -> str:\n        return re.sub(r\"^(en|vi)\\s*:\\s*\", \"\", text.strip(), flags=re.IGNORECASE)\n    \n    def translate(self, text: str, source_lang: str = \"vi\", max_length: int = 128) -> str:\n        content = f\"{source_lang}: {text}\"\n        inputs = self.tokenizer(\n            content, \n            return_tensors=\"pt\", \n            truncation=True, \n            max_length=max_length).to(self.device)\n        with torch.no_grad():\n            outputs = self.model.generate(**inputs, max_length=max_length)\n        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        return self._clean_prefix(decoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T15:00:04.060909Z","iopub.execute_input":"2025-08-29T15:00:04.061252Z","iopub.status.idle":"2025-08-29T15:00:04.093343Z","shell.execute_reply.started":"2025-08-29T15:00:04.061227Z","shell.execute_reply":"2025-08-29T15:00:04.091824Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Configuration with Kaggle secrets support\nimport os\n\n# Use Kaggle secrets or fallback to environment variables\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    QDRANT_URL = user_secrets.get_secret(\"QDRANT_URL\")\n    QDRANT_API_KEY = user_secrets.get_secret(\"QDRANT_API_KEY\") \n    NGROK_AUTH_TOKEN = user_secrets.get_secret(\"NGROK_AUTH_TOKEN\")\n    print(\"✅ Using Kaggle secrets for configuration\")\nexcept Exception as e:\n    print(f\"⚠️  Kaggle secrets not available: {e}\")\n    print(\"🔧 Falling back to hardcoded values (update these with your credentials)\")\n    # Fallback to hardcoded values - UPDATE THESE WITH YOUR ACTUAL CREDENTIALS\n    QDRANT_URL = \"https://09a6d049-00c4-4b77-8e95-1dcc9ea5df34.eu-west-1-0.aws.cloud.qdrant.io:6333\"\n    QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.-ZPZib9FxehqbTuqxsk7QdVjBQd0LlQEq7dpjF1b4PI\"\n    NGROK_AUTH_TOKEN = \"28k6uZmtZlrKVCzyVQTfjtRSIDd_6GHtfHcwNEojEk9WjkTmv\"\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"🚀 Using device: {DEVICE}\")\n\n# Initialize Qdrant client\nqdrant_client = QdrantClient(\n    url=QDRANT_URL,\n    api_key=QDRANT_API_KEY,\n)\n\nCORS_SETTINGS = {\n    \"allow_origins\": [\"*\"],\n    \"allow_credentials\": True,\n    \"allow_methods\": [\"*\"],\n    \"allow_headers\": [\"*\"],\n}\n\n# Collection names\nCLIP_collection = \"Image\"\nBGE_collection = \"BGE_Caption\"\nGTE_collection = \"GTE_Caption\"\n\nprint(f\"📊 Collections: {CLIP_collection}, {BGE_collection}, {GTE_collection}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T15:04:09.465696Z","iopub.execute_input":"2025-08-29T15:04:09.467004Z","iopub.status.idle":"2025-08-29T15:04:09.882588Z","shell.execute_reply.started":"2025-08-29T15:04:09.466963Z","shell.execute_reply":"2025-08-29T15:04:09.881442Z"}},"outputs":[{"name":"stdout","text":"⚠️  Kaggle secrets not available: Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 90800749 and label QDRANT_URL.'], 'error': {'code': 5}, 'wasSuccessful': False}.\n🔧 Falling back to hardcoded values (update these with your credentials)\n🚀 Using device: cpu\n📊 Collections: Image, BGE_Caption, GTE_Caption\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Initialize models\nprint(\"🔧 Initializing models...\")\n\ntranslator = Translator(device=DEVICE)\nprint(\"✅ Translator loaded\")\n\nclip_embed_model = CLIPEmbedding(device=DEVICE)\nclip_vector_store = QdrantVectorStore(client=qdrant_client,\n                                      collection_name=CLIP_collection)\nclip_index = VectorStoreIndex.from_vector_store(vector_store=clip_vector_store,\n                                                embed_model=clip_embed_model)\nprint(\"✅ CLIP model and index loaded\")\n\nbge_embed_model = CaptionEmbedding(model_name=\"AITeamVN/Vietnamese_Embedding_v2\", device=DEVICE)\nbge_vector_store = QdrantVectorStore(client=qdrant_client, \n                                     collection_name=BGE_collection)\nbge_index = VectorStoreIndex.from_vector_store(vector_store=bge_vector_store,\n                                               embed_model=bge_embed_model)\nprint(\"✅ BGE Vietnamese model loaded\")\n\ngte_embed_model = CaptionEmbedding(model_name=\"dangvantuan/vietnamese-document-embedding\",\n                                   device=DEVICE, trust_remote_code=True)\ngte_vector_store = QdrantVectorStore(client=qdrant_client,\n                                     collection_name=GTE_collection)\ngte_index = VectorStoreIndex.from_vector_store(vector_store=gte_vector_store,\n                                               embed_model=gte_embed_model)\nprint(\"✅ GTE Document model loaded\")\n\nprint(\"🎉 All models initialized successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T15:00:04.619304Z","iopub.execute_input":"2025-08-29T15:00:04.619699Z","iopub.status.idle":"2025-08-29T15:01:48.160263Z","shell.execute_reply.started":"2025-08-29T15:00:04.619674Z","shell.execute_reply":"2025-08-29T15:01:48.159073Z"}},"outputs":[{"name":"stdout","text":"🔧 Initializing models...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a883cf5bc3a4087b4a5b76bd7472304"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e2d3fd55a5d42ae8c32070c53e5570a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f1df27f32d14094866c55fe1e2e4169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70e60bfdb17a4a07bf720d97500f8f22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/721 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dcf867cc7864844b8099ddc531b5058"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90cea119301f4114b6abc4f85f6555b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cafa3c8111946fab0c78c8d933e9d5b"}},"metadata":{}},{"name":"stdout","text":"✅ Translator loaded\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"open_clip_pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7acf45c800745f9a199814894bf5282"}},"metadata":{}},{"name":"stdout","text":"✅ CLIP model and index loaded\nLoading model: AITeamVN/Vietnamese_Embedding_v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a946ef1b1ea49efac4c8efed175e250"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/171 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cc849d3f8724ed1913f97f0a8e0e781"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5c9045e07914540887fcf4aeee7cbb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f929a0c28da409ca86129da600f834b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf11f9ec553342cd99fb5f9d52c856b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aa64cc338814bdb8d69ff9fce481109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"094b4c386a51444e8307ec92a1a57e19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11aec47e7cc448319e7bf55dc80db2ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e9d8a55e9e14e33a14cac5105685e93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"917b8acd276d4080890594a963ec3eb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88251048574a4a4b97168f68c9197a41"}},"metadata":{}},{"name":"stdout","text":"✅ BGE Vietnamese model loaded\nLoading model: dangvantuan/vietnamese-document-embedding\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"787cf305071144bd9f01bcf918ae6c9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/171 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b86c758e954436ca18da5d6a6197d61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa95b411cd3142e6b61d15c4dc2b91cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbf212661fcf4d96b9296a57385704a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1048f3e9cbe6492aa929cd2c9b4ccf38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e355711751264d1b9d4c6528085a32a4"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dangvantuan/Vietnamese_impl:\n- configuration.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aa8d934b2474206a15a4f318a099e03"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dangvantuan/Vietnamese_impl:\n- modeling.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a899307e4fcc49a5b97d8e895bf7759e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2a4014f01ed4522861dab0a84a8c951"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c046e61848244f2ab22f2180cd326b94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3f5ffe7a77a4c44a63b1e63f1cb961d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3c2fda5b01341ac89af8a8b2a3d1025"}},"metadata":{}},{"name":"stdout","text":"✅ GTE Document model loaded\n🎉 All models initialized successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Build frame mappings for temporal search\nprint(\"📊 Building frame mappings for temporal search...\")\n\nFRAME_NAMES = []\noffset = None\nbatch_count = 0\n\nwhile True:\n    result, offset = qdrant_client.scroll(\n        collection_name=CLIP_collection,\n        scroll_filter=None,\n        with_payload=True,\n        limit=1000,\n        offset=offset\n    )\n    \n    batch_count += 1\n    print(f\"  Batch {batch_count}: {len(result)} frames\")\n    \n    for point in result:\n        if \"id\" in point.payload:\n            FRAME_NAMES.append(point.payload[\"id\"])\n    \n    if offset is None:\n        break\n\nFRAME_NAMES = sorted(set(FRAME_NAMES))\n\nVIDEO_TO_FRAMES = defaultdict(list)\nfor f in FRAME_NAMES:\n    vid = \"_\".join(f.split(\"_\")[:2]) \n    VIDEO_TO_FRAMES[vid].append(f)\n\nprint(f\"✅ Loaded {len(FRAME_NAMES)} frame names from {len(VIDEO_TO_FRAMES)} videos\")\nprint(f\"📹 Sample videos: {list(VIDEO_TO_FRAMES.keys())[:5]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T15:01:48.161180Z","iopub.execute_input":"2025-08-29T15:01:48.161518Z","iopub.status.idle":"2025-08-29T15:01:53.900261Z","shell.execute_reply.started":"2025-08-29T15:01:48.161488Z","shell.execute_reply":"2025-08-29T15:01:53.899017Z"}},"outputs":[{"name":"stdout","text":"📊 Building frame mappings for temporal search...\n  Batch 1: 1000 frames\n  Batch 2: 1000 frames\n  Batch 3: 1000 frames\n  Batch 4: 1000 frames\n  Batch 5: 1000 frames\n  Batch 6: 1000 frames\n  Batch 7: 1000 frames\n  Batch 8: 1000 frames\n  Batch 9: 1000 frames\n  Batch 10: 1000 frames\n  Batch 11: 1000 frames\n  Batch 12: 1000 frames\n  Batch 13: 1000 frames\n  Batch 14: 1000 frames\n  Batch 15: 1000 frames\n  Batch 16: 1000 frames\n  Batch 17: 1000 frames\n  Batch 18: 1000 frames\n  Batch 19: 1000 frames\n  Batch 20: 1000 frames\n  Batch 21: 1000 frames\n  Batch 22: 1000 frames\n  Batch 23: 1000 frames\n  Batch 24: 1000 frames\n  Batch 25: 1000 frames\n  Batch 26: 1000 frames\n  Batch 27: 1000 frames\n  Batch 28: 1000 frames\n  Batch 29: 1000 frames\n  Batch 30: 1000 frames\n  Batch 31: 1000 frames\n  Batch 32: 1000 frames\n  Batch 33: 1000 frames\n  Batch 34: 1000 frames\n  Batch 35: 1000 frames\n  Batch 36: 1000 frames\n  Batch 37: 1000 frames\n  Batch 38: 1000 frames\n  Batch 39: 1000 frames\n  Batch 40: 1000 frames\n  Batch 41: 1000 frames\n  Batch 42: 1000 frames\n  Batch 43: 1000 frames\n  Batch 44: 1000 frames\n  Batch 45: 1000 frames\n  Batch 46: 1000 frames\n  Batch 47: 1000 frames\n  Batch 48: 1000 frames\n  Batch 49: 1000 frames\n  Batch 50: 1000 frames\n  Batch 51: 1000 frames\n  Batch 52: 1000 frames\n  Batch 53: 1000 frames\n  Batch 54: 1000 frames\n  Batch 55: 1000 frames\n  Batch 56: 1000 frames\n  Batch 57: 1000 frames\n  Batch 58: 1000 frames\n  Batch 59: 1000 frames\n  Batch 60: 1000 frames\n  Batch 61: 1000 frames\n  Batch 62: 1000 frames\n  Batch 63: 1000 frames\n  Batch 64: 1000 frames\n  Batch 65: 1000 frames\n  Batch 66: 1000 frames\n  Batch 67: 1000 frames\n  Batch 68: 1000 frames\n  Batch 69: 1000 frames\n  Batch 70: 1000 frames\n  Batch 71: 1000 frames\n  Batch 72: 1000 frames\n  Batch 73: 1000 frames\n  Batch 74: 1000 frames\n  Batch 75: 1000 frames\n  Batch 76: 1000 frames\n  Batch 77: 1000 frames\n  Batch 78: 1000 frames\n  Batch 79: 1000 frames\n  Batch 80: 1000 frames\n  Batch 81: 1000 frames\n  Batch 82: 1000 frames\n  Batch 83: 1000 frames\n  Batch 84: 1000 frames\n  Batch 85: 1000 frames\n  Batch 86: 1000 frames\n  Batch 87: 1000 frames\n  Batch 88: 1000 frames\n  Batch 89: 1000 frames\n  Batch 90: 1000 frames\n  Batch 91: 1000 frames\n  Batch 92: 1000 frames\n  Batch 93: 1000 frames\n  Batch 94: 1000 frames\n  Batch 95: 1000 frames\n  Batch 96: 1000 frames\n  Batch 97: 545 frames\n✅ Loaded 96545 frame names from 866 videos\n📹 Sample videos: ['L21_V001', 'L21_V002', 'L21_V003', 'L21_V005', 'L21_V006']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def retrieve(query: str, topK: int, frame_ids: Optional[List] = None,\n             mode: str = \"clip\", caption_mode: str = \"bge\"):\n    \"\"\"\n    Universal retrieve function with frame filtering support\n    \"\"\"\n    if mode == \"clip\":\n        embed_model = clip_embed_model\n        index = clip_index\n        collection_name = CLIP_collection\n        query_text = translator.translate(query, source_lang=\"vi\")\n    elif mode == \"vintern\":\n        if caption_mode == \"bge\":\n            embed_model = bge_embed_model\n            index = bge_index\n            collection_name = BGE_collection\n            query_text = query\n        else:  # gte\n            embed_model = gte_embed_model\n            index = gte_index\n            collection_name = GTE_collection\n            query_text = query\n        \n    if frame_ids: \n        # Direct query with frame filtering\n        vector_query = embed_model._get_text_embedding(query_text)\n        nodes = qdrant_client.query_points(\n            collection_name=collection_name,\n            query=vector_query,\n            limit=topK,\n            with_payload=True,\n            query_filter=models.Filter(must=[\n                models.FieldCondition(\n                    key=\"id\",\n                    match=models.MatchAny(any=frame_ids)\n                )\n            ])\n        ).points\n        results = [\n            {\"id\": node.payload[\"id\"].strip(), \"score\": node.score}\n            for node in nodes\n        ]\n    else:\n        # Use index for full search\n        retriever = index.as_retriever(similarity_top_k=topK)\n        nodes = retriever.retrieve(query_text)\n        results = [\n            {\"id\": node.metadata.get(\"id\", \"\").strip(), \"score\": node.score}\n            for node in nodes\n        ]\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T15:01:53.901521Z","iopub.execute_input":"2025-08-29T15:01:53.901839Z","iopub.status.idle":"2025-08-29T15:01:53.916415Z","shell.execute_reply.started":"2025-08-29T15:01:53.901815Z","shell.execute_reply":"2025-08-29T15:01:53.915210Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def retrieve_frame(query: str, topK: int, mode: str = \"hybrid\", caption_mode: str = \"bge\",\n                   alpha: float = 0.5, frame_ids: Optional[List] = None):\n    \"\"\"\n    Enhanced retrieve_frame with caption mode support\n    \"\"\"\n    if mode == \"clip\":\n        clip_nodes = retrieve(query, topK, frame_ids, \"clip\")\n        results = [\n            {\"image\": node[\"id\"], \"caption\": f\"{node['id']} | Score: {node['score']:.2f}\"}\n            for node in clip_nodes\n        ]\n        return results\n    \n    elif mode == \"vintern\":\n        caption_nodes = retrieve(query, topK, frame_ids, \"vintern\", caption_mode)\n        results = [\n            {\"image\": node[\"id\"], \"caption\": f\"{node['id']} | Score: {node['score']:.2f}\"}\n            for node in caption_nodes\n        ]\n        return results\n    \n    else:  # hybrid mode\n        clip_nodes = retrieve(query, topK, frame_ids, \"clip\")\n        caption_nodes = retrieve(query, topK, frame_ids, \"vintern\", caption_mode)\n        \n        combined_scores = defaultdict(float)\n        for node in caption_nodes:\n            combined_scores[node[\"id\"]] += node[\"score\"] * alpha\n\n        for node in clip_nodes:\n            combined_scores[node[\"id\"]] += node[\"score\"] * (1 - alpha)\n\n        top_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:topK]\n\n        return [\n            {\"image\": video_id.strip(), \"caption\": f\"{video_id} | Score: {score:.2f}\"}\n            for video_id, score in top_results\n        ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T15:01:53.918169Z","iopub.execute_input":"2025-08-29T15:01:53.918603Z","iopub.status.idle":"2025-08-29T15:01:58.562195Z","shell.execute_reply.started":"2025-08-29T15:01:53.918572Z","shell.execute_reply":"2025-08-29T15:01:58.560159Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def retrieve_from_image(contents: bytes, topK: int):\n    \"\"\"\n    Image-based search using CLIP embeddings\n    \"\"\"\n    image = Image.open(io.BytesIO(contents)).convert(\"RGB\")\n    vector_query = clip_embed_model._get_image_embedding(image)\n\n    clip_nodes = qdrant_client.query_points(\n        collection_name=CLIP_collection,\n        query=vector_query,\n        limit=topK,\n        with_payload=True\n    ).points\n\n    results = [\n        {\n            \"image\": node.payload.get(\"id\", \"\").strip(),\n            \"caption\": f\"{node.payload.get('id', '')} | Score: {node.score:.2f}\"\n        }\n        for node in clip_nodes\n    ]\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T15:01:58.564190Z","iopub.execute_input":"2025-08-29T15:01:58.566319Z","iopub.status.idle":"2025-08-29T15:01:58.604824Z","shell.execute_reply.started":"2025-08-29T15:01:58.566265Z","shell.execute_reply":"2025-08-29T15:01:58.603151Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def temporal_search(events: List[str], topK: int = 100, \n                    mode: str = \"hybrid\", caption_mode: str = \"bge\",\n                    alpha: float = 0.5):\n    \"\"\"\n    Temporal search for TRAKE mode - progressive filtering through video sequences\n    \"\"\"\n    frame_ids = None\n    final_results = []  \n    \n    print(f\"🔍 Starting temporal search with {len(events)} events...\")\n\n    for i, event in enumerate(events):\n        print(f\"  Event {i+1}: {event[:50]}...\")\n        \n        results = retrieve_frame(query=event, topK=topK, mode=mode, \n                                 caption_mode=caption_mode, alpha=alpha, frame_ids=frame_ids)\n        final_results.append(results)\n        \n        # Extract video IDs from current results to narrow search space\n        video_ids = {\"_\".join(item['image'].split(\"_\")[:2]) for item in results}\n        print(f\"    → Found {len(results)} results from {len(video_ids)} videos\")\n\n        # Update frame_ids to frames from videos in current results\n        frame_ids = [f for vid in video_ids for f in VIDEO_TO_FRAMES[vid]]\n        print(f\"    → Narrowed search space to {len(frame_ids)} frames\")\n    \n    print(f\"✅ Temporal search completed!\")\n    return final_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T15:01:58.606528Z","iopub.execute_input":"2025-08-29T15:01:58.606983Z","iopub.status.idle":"2025-08-29T15:01:58.646014Z","shell.execute_reply.started":"2025-08-29T15:01:58.606950Z","shell.execute_reply":"2025-08-29T15:01:58.644213Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# FastAPI Application\napp = FastAPI(title=\"Video Event Retrieval API v2.0\", \n              description=\"Enhanced multimodal search with temporal capabilities\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=CORS_SETTINGS[\"allow_origins\"],\n    allow_credentials=CORS_SETTINGS[\"allow_credentials\"],\n    allow_methods=CORS_SETTINGS[\"allow_methods\"],\n    allow_headers=CORS_SETTINGS[\"allow_headers\"],\n)\n\nrouter = APIRouter()\n\n@router.post(\"/search\")\nasync def api_search(\n    query: Optional[str] = Form(None),\n    topK: int = Form(...),\n    mode: str = Form(\"hybrid\"),\n    caption_mode: str = Form(\"bge\"),\n    alpha: float = Form(0.5),\n    file: UploadFile = File(None)\n):\n    \"\"\"\n    Enhanced search API with caption mode support\n    - mode: hybrid, clip, vintern, image\n    - caption_mode: bge, gte (for vintern and hybrid modes)\n    - alpha: text/visual balance for hybrid mode (0.1-0.9)\n    \"\"\"\n    start_time = time.time()\n    \n    try:\n        if mode == \"image\":\n            if file is None:\n                return {\"error\": \"No file uploaded for image mode\"}\n            contents = await file.read()\n            results = retrieve_from_image(contents=contents, topK=topK)\n            search_info = f\"IMAGE search\"\n        else:\n            if query is None or query.strip() == \"\":\n                return {\"error\": \"No query provided for text mode\"}\n            results = retrieve_frame(query=query, topK=topK, mode=mode, \n                                    caption_mode=caption_mode, alpha=alpha)\n            search_info = f\"{mode.upper()} mode with {caption_mode.upper()} model\"\n        \n        duration = time.time() - start_time\n        \n        return {\n            \"results\": results,\n            \"search_info\": {\n                \"mode\": mode,\n                \"caption_mode\": caption_mode if mode in [\"hybrid\", \"vintern\"] else None,\n                \"alpha\": alpha if mode == \"hybrid\" else None,\n                \"duration\": round(duration, 3),\n                \"count\": len(results),\n                \"description\": search_info\n            }\n        }\n    except Exception as e:\n        return {\"error\": f\"Search failed: {str(e)}\"}\n\n@router.post(\"/temporal_search\")\nasync def api_temporal_search(\n    events: str = Form(...),  # JSON string of event list\n    topK: int = Form(100),\n    mode: str = Form(\"hybrid\"),\n    caption_mode: str = Form(\"bge\"),\n    alpha: float = Form(0.5)\n):\n    \"\"\"\n    Temporal search API for TRAKE mode\n    - events: JSON array of sequential event descriptions\n    - Returns progressive results for each event\n    \"\"\"\n    start_time = time.time()\n    \n    try:\n        events_list = json.loads(events)\n        if not isinstance(events_list, list) or len(events_list) == 0:\n            return {\"error\": \"Events must be a non-empty list\"}\n        \n        # Filter out empty events\n        valid_events = [e.strip() for e in events_list if e.strip()]\n        if len(valid_events) == 0:\n            return {\"error\": \"No valid events provided\"}\n        \n        results = temporal_search(events=valid_events, topK=topK, mode=mode,\n                                 caption_mode=caption_mode, alpha=alpha)\n        \n        duration = time.time() - start_time\n        \n        return {\n            \"results\": results,\n            \"search_info\": {\n                \"mode\": mode,\n                \"caption_mode\": caption_mode,\n                \"alpha\": alpha if mode == \"hybrid\" else None,\n                \"duration\": round(duration, 3),\n                \"events_processed\": len(valid_events),\n                \"final_count\": len(results[-1]) if results else 0,\n                \"description\": f\"Temporal search through {len(valid_events)} events\"\n            }\n        }\n    except json.JSONDecodeError:\n        return {\"error\": \"Invalid JSON format for events\"}\n    except Exception as e:\n        return {\"error\": f\"Temporal search failed: {str(e)}\"}\n\n@router.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"device\": DEVICE,\n        \"models_loaded\": {\n            \"clip\": clip_embed_model is not None,\n            \"bge\": bge_embed_model is not None,\n            \"gte\": gte_embed_model is not None,\n            \"translator\": translator is not None\n        },\n        \"collections\": {\n            \"clip\": CLIP_collection,\n            \"bge\": BGE_collection,\n            \"gte\": GTE_collection\n        },\n        \"frame_count\": len(FRAME_NAMES),\n        \"video_count\": len(VIDEO_TO_FRAMES)\n    }\n\napp.include_router(router)\nprint(\"✅ FastAPI application configured\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T15:01:58.647347Z","iopub.execute_input":"2025-08-29T15:01:58.648233Z","iopub.status.idle":"2025-08-29T15:01:58.702187Z","shell.execute_reply.started":"2025-08-29T15:01:58.648160Z","shell.execute_reply":"2025-08-29T15:01:58.700267Z"}},"outputs":[{"name":"stdout","text":"✅ FastAPI application configured\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Server setup with ngrok\nimport os, time, threading, socket\nfrom pyngrok import ngrok\nimport uvicorn\n\nPORT = 8000\nHOST = \"0.0.0.0\"\n\n# Set ngrok auth token\nif NGROK_AUTH_TOKEN and NGROK_AUTH_TOKEN != \"YOUR_NGROK_TOKEN_HERE\":\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n    print(\"✅ Ngrok auth token set\")\nelse:\n    print(\"⚠️  NGROK_AUTH_TOKEN not configured. Please update with your token.\")\n    print(\"   Get your token from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n\ndef is_port_in_use(port: int, host=\"127.0.0.1\") -> bool:\n    \"\"\"Check if a local TCP port is already in use.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        return s.connect_ex((host, port)) == 0\n\ndef run_server():\n    \"\"\"Run FastAPI server in background thread\"\"\"\n    uvicorn.run(app, host=HOST, port=PORT, log_level=\"info\")\n\n# Start server only if not already running\nif not is_port_in_use(PORT):\n    print(f\"🚀 Starting FastAPI server on {HOST}:{PORT}\")\n    server_thread = threading.Thread(target=run_server, daemon=True)\n    server_thread.start()\n    time.sleep(3)  # Wait for server startup\n    print(\"✅ Server started successfully\")\nelse:\n    print(f\"🔁 Server already running on http://localhost:{PORT}\")\n\n# Setup ngrok tunnel\ntry:\n    # Clean up existing tunnels\n    for t in ngrok.get_tunnels():\n        addr = (t.config or {}).get(\"addr\", \"\")\n        if str(PORT) in addr:\n            try:\n                ngrok.disconnect(t.public_url)\n            except Exception:\n                pass\n\n    # Kill all tunnels if too many\n    if len(ngrok.get_tunnels()) >= 3:\n        ngrok.kill()\n\n    # Create new tunnel\n    tunnel = ngrok.connect(addr=PORT, proto=\"http\", bind_tls=True)\n    PUBLIC_URL = tunnel.public_url\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"🌐 BACKEND READY!\")\n    print(f\"📡 Public URL: {PUBLIC_URL}\")\n    print(f\"📖 API Docs: {PUBLIC_URL}/docs\")\n    print(f\"🏥 Health Check: {PUBLIC_URL}/health\")\n    print(f\"💻 Local URL: http://localhost:{PORT}\")\n    print(\"\\n🎯 COPY THE PUBLIC URL TO YOUR FRONTEND!\")\n    print(\"=\"*60)\n\n    # Save to global for later use\n    globals()[\"PUBLIC_URL\"] = PUBLIC_URL\n\nexcept Exception as e:\n    print(f\"❌ Ngrok tunnel failed: {e}\")\n    print(f\"🔧 Server still available locally: http://localhost:{PORT}\")\n    print(\"💡 Try restarting the kernel or checking your ngrok auth token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T15:04:31.123959Z","iopub.execute_input":"2025-08-29T15:04:31.124330Z","iopub.status.idle":"2025-08-29T15:04:31.783554Z","shell.execute_reply.started":"2025-08-29T15:04:31.124307Z","shell.execute_reply":"2025-08-29T15:04:31.782084Z"}},"outputs":[{"name":"stdout","text":"✅ Ngrok auth token set\n🔁 Server already running on http://localhost:8000\n\n============================================================\n🌐 BACKEND READY!\n📡 Public URL: https://d6dd4a9b06c5.ngrok-free.app\n📖 API Docs: https://d6dd4a9b06c5.ngrok-free.app/docs\n🏥 Health Check: https://d6dd4a9b06c5.ngrok-free.app/health\n💻 Local URL: http://localhost:8000\n\n🎯 COPY THE PUBLIC URL TO YOUR FRONTEND!\n============================================================\nINFO:     14.234.51.109:0 - \"OPTIONS /search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"POST /search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"POST /search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"POST /search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"POST /search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"POST /search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"POST /search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"OPTIONS /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting temporal search with 3 events...\n  Event 1: Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đe...\n    → Found 10 results from 10 videos\n    → Narrowed search space to 1068 frames\n  Event 2: Sau đó người này rắc bột lên những miếng thịt, tro...\n    → Found 10 results from 4 videos\n    → Narrowed search space to 414 frames\n  Event 3: Các miếng thịt sau đó được đem đi áp chảo cùng với...\n    → Found 10 results from 3 videos\n    → Narrowed search space to 301 frames\n✅ Temporal search completed!\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"GET / HTTP/1.1\" 404 Not Found\nINFO:     14.234.51.109:0 - \"GET / HTTP/1.1\" 404 Not Found\nINFO:     14.234.51.109:0 - \"GET / HTTP/1.1\" 404 Not Found\n🔍 Starting temporal search with 3 events...\n  Event 1: Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đe...\n    → Found 100 results from 78 videos\n    → Narrowed search space to 8098 frames\n  Event 2: Sau đó người này rắc bột lên những miếng thịt, tro...\n    → Found 100 results from 48 videos\n    → Narrowed search space to 5010 frames\n  Event 3: Các miếng thịt sau đó được đem đi áp chảo cùng với...\n    → Found 100 results from 17 videos\n    → Narrowed search space to 1579 frames\n✅ Temporal search completed!\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting temporal search with 3 events...\n  Event 1: Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đe...\n    → Found 10 results from 10 videos\n    → Narrowed search space to 1068 frames\n  Event 2: Sau đó người này rắc bột lên những miếng thịt, tro...\n    → Found 10 results from 4 videos\n    → Narrowed search space to 414 frames\n  Event 3: Các miếng thịt sau đó được đem đi áp chảo cùng với...\n    → Found 10 results from 3 videos\n    → Narrowed search space to 301 frames\n✅ Temporal search completed!\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"OPTIONS /search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"POST /search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"POST /search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"OPTIONS /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting temporal search with 3 events...\n  Event 1: Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đe...\n    → Found 10 results from 10 videos\n    → Narrowed search space to 1068 frames\n  Event 2: Sau đó người này rắc bột lên những miếng thịt, tro...\n    → Found 10 results from 4 videos\n    → Narrowed search space to 414 frames\n  Event 3: Các miếng thịt sau đó được đem đi áp chảo cùng với...\n    → Found 10 results from 3 videos\n    → Narrowed search space to 301 frames\n✅ Temporal search completed!\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Test the enhanced API\nimport requests\n\ndef test_api_endpoints(use_public_url=True):\n    \"\"\"Test all API endpoints with the new features\"\"\"\n    \n    if use_public_url and 'PUBLIC_URL' in globals():\n        base_url = globals()['PUBLIC_URL']\n        print(f\"🔗 Testing public URL: {base_url}\")\n    else:\n        base_url = \"http://localhost:8000\"\n        print(f\"🔗 Testing local URL: {base_url}\")\n    \n    # Test health endpoint\n    print(\"\\n1. 🏥 Testing health endpoint...\")\n    try:\n        response = requests.get(f\"{base_url}/health\", timeout=10)\n        if response.status_code == 200:\n            health = response.json()\n            print(f\"   ✅ Health check passed\")\n            print(f\"   📊 {health['frame_count']} frames from {health['video_count']} videos\")\n            print(f\"   🤖 Models: {health['models_loaded']}\")\n        else:\n            print(f\"   ❌ Health check failed: {response.status_code}\")\n    except Exception as e:\n        print(f\"   ❌ Health check error: {e}\")\n        return\n    \n    # Test search with BGE\n    print(\"\\n2. 🔍 Testing hybrid search with BGE...\")\n    test_search(base_url, \"người đang nấu ăn\", mode=\"hybrid\", caption_mode=\"bge\")\n    \n    # Test search with GTE\n    print(\"\\n3. 🔍 Testing hybrid search with GTE...\")\n    test_search(base_url, \"người đang nấu ăn\", mode=\"hybrid\", caption_mode=\"gte\")\n    \n    # Test vintern only with GTE\n    print(\"\\n4. 📝 Testing vintern search with GTE...\")\n    test_search(base_url, \"cảnh đẹp thiên nhiên\", mode=\"vintern\", caption_mode=\"gte\")\n    \n    # Test temporal search\n    print(\"\\n5. ⏰ Testing temporal search...\")\n    test_temporal_search(base_url, [\n        \"Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đem nướng trên chảo. Hãy lấy khoảnh khắc chiếc dao cắt qua hoàn toàn chiếc bánh.\",\n        \"Sau đó người này rắc bột lên những miếng thịt, trong quá trình này người đầu bếp lật những miếng thịt để rắc bột đều hai mặt. Hãy lấy khoảnh khắc đầu tiên người đầu bếp này buông tay khỏi miếng thịt sau khi lật miếng thịt đầu tiên.\",\n        \"Các miếng thịt sau đó được đem đi áp chảo cùng với bơ (3 ngang 1 dọc theo chiều của camera). Hãy lấy khoảnh khắc đầu tiên người đầu bếp cầm vào chảo để nhấc lên đảo bơ đều xung quanh\"\n    ])\n\ndef test_search(base_url, query, mode=\"hybrid\", caption_mode=\"bge\", topK=5):\n    \"\"\"Test search endpoint\"\"\"\n    try:\n        data = {\n            \"query\": query,\n            \"topK\": topK,\n            \"mode\": mode,\n            \"caption_mode\": caption_mode,\n            \"alpha\": 0.6\n        }\n        \n        response = requests.post(f\"{base_url}/search\", data=data, timeout=30)\n        \n        if response.status_code == 200:\n            result = response.json()\n            search_info = result.get(\"search_info\", {})\n            print(f\"   ✅ Search successful: {search_info.get('description')}\")\n            print(f\"   ⏱️ Duration: {search_info.get('duration')}s\")\n            print(f\"   📊 Results: {len(result['results'])}\")\n            \n            # Show top results\n            for i, res in enumerate(result['results'][:3]):\n                print(f\"      {i+1}. {res['caption']}\")\n        else:\n            print(f\"   ❌ Search failed: {response.status_code} - {response.text}\")\n    except Exception as e:\n        print(f\"   ❌ Search error: {e}\")\n\ndef test_temporal_search(base_url, events, topK=20):\n    \"\"\"Test temporal search endpoint\"\"\"\n    try:\n        data = {\n            \"events\": json.dumps(events),\n            \"topK\": topK,\n            \"mode\": \"hybrid\",\n            \"caption_mode\": \"gte\",\n            \"alpha\": 0.7\n        }\n        \n        response = requests.post(f\"{base_url}/temporal_search\", data=data, timeout=60)\n        \n        if response.status_code == 200:\n            result = response.json()\n            search_info = result.get(\"search_info\", {})\n            print(f\"   ✅ Temporal search successful: {search_info.get('description')}\")\n            print(f\"   ⏱️ Duration: {search_info.get('duration')}s\")\n            print(f\"   📊 Events processed: {search_info.get('events_processed')}\")\n            print(f\"   🎯 Final results: {search_info.get('final_count')}\")\n            \n            # Show progression\n            for i, event_results in enumerate(result['results']):\n                print(f\"      Event {i+1}: {len(event_results)} results\")\n                for j, res in enumerate(event_results[:2]):\n                    print(f\"        → {res['caption']}\")\n        else:\n            print(f\"   ❌ Temporal search failed: {response.status_code} - {response.text}\")\n    except Exception as e:\n        print(f\"   ❌ Temporal search error: {e}\")\n\n# Run tests\nprint(\"🧪 Testing Enhanced API...\")\ntest_api_endpoints()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T15:53:34.815578Z","iopub.execute_input":"2025-08-29T15:53:34.816496Z","iopub.status.idle":"2025-08-29T15:53:49.563322Z","shell.execute_reply.started":"2025-08-29T15:53:34.816456Z","shell.execute_reply":"2025-08-29T15:53:49.562154Z"}},"outputs":[{"name":"stdout","text":"🧪 Testing Enhanced API...\n🔗 Testing public URL: https://d6dd4a9b06c5.ngrok-free.app\n\n1. 🏥 Testing health endpoint...\nINFO:     34.91.46.191:0 - \"GET /health HTTP/1.1\" 200 OK\n   ✅ Health check passed\n   📊 96545 frames from 866 videos\n   🤖 Models: {'clip': True, 'bge': True, 'gte': True, 'translator': True}\n\n2. 🔍 Testing hybrid search with BGE...\nINFO:     34.91.46.191:0 - \"POST /search HTTP/1.1\" 200 OK\n   ✅ Search successful: HYBRID mode with BGE model\n   ⏱️ Duration: 1.57s\n   📊 Results: 5\n      1. L26_V272_5537 | Score: 0.42\n      2. L26_V452_4878 | Score: 0.42\n      3. L26_V339_5215 | Score: 0.41\n\n3. 🔍 Testing hybrid search with GTE...\nINFO:     34.91.46.191:0 - \"POST /search HTTP/1.1\" 200 OK\n   ✅ Search successful: HYBRID mode with GTE model\n   ⏱️ Duration: 1.286s\n   📊 Results: 5\n      1. L27_V003_6294 | Score: 0.38\n      2. L26_V367_5958 | Score: 0.38\n      3. L26_V484_3594 | Score: 0.37\n\n4. 📝 Testing vintern search with GTE...\nINFO:     34.91.46.191:0 - \"POST /search HTTP/1.1\" 200 OK\n   ✅ Search successful: VINTERN mode with GTE model\n   ⏱️ Duration: 0.113s\n   📊 Results: 5\n      1. L27_V008_12041 | Score: 0.68\n      2. L28_V016_5421 | Score: 0.66\n      3. L28_V018_13549 | Score: 0.63\n\n5. ⏰ Testing temporal search...\n🔍 Starting temporal search with 3 events...\n  Event 1: Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đe...\n    → Found 20 results from 18 videos\n    → Narrowed search space to 1814 frames\n  Event 2: Sau đó người này rắc bột lên những miếng thịt, tro...\n    → Found 20 results from 7 videos\n    → Narrowed search space to 732 frames\n  Event 3: Các miếng thịt sau đó được đem đi áp chảo cùng với...\n    → Found 20 results from 4 videos\n    → Narrowed search space to 433 frames\n✅ Temporal search completed!\nINFO:     34.91.46.191:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n   ✅ Temporal search successful: Temporal search through 3 events\n   ⏱️ Duration: 9.977s\n   📊 Events processed: 3\n   🎯 Final results: 20\n      Event 1: 20 results\n        → L26_V289_1948 | Score: 0.61\n        → L26_V399_5336 | Score: 0.58\n      Event 2: 20 results\n        → L26_V399_2790 | Score: 0.43\n        → L26_V251_2204 | Score: 0.43\n      Event 3: 20 results\n        → L26_V247_4538 | Score: 0.52\n        → L26_V399_4814 | Score: 0.49\nINFO:     14.234.51.109:0 - \"OPTIONS /search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"POST /search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"OPTIONS /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting temporal search with 1 events...\n  Event 1: Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đe...\n    → Found 10 results from 10 videos\n    → Narrowed search space to 1068 frames\n✅ Temporal search completed!\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"POST /search HTTP/1.1\" 200 OK\n🔍 Starting temporal search with 3 events...\n  Event 1: Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đe...\n    → Found 10 results from 10 videos\n    → Narrowed search space to 1068 frames\n  Event 2: Sau đó người này rắc bột lên những miếng thịt, tro...\n    → Found 10 results from 4 videos\n    → Narrowed search space to 414 frames\n  Event 3: Các miếng thịt sau đó được đem đi áp chảo cùng với...\n    → Found 10 results from 3 videos\n    → Narrowed search space to 301 frames\n✅ Temporal search completed!\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting temporal search with 3 events...\n  Event 1: Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đe...\n    → Found 10 results from 10 videos\n    → Narrowed search space to 1068 frames\n  Event 2: Sau đó người này rắc bột lên những miếng thịt, tro...\n    → Found 10 results from 4 videos\n    → Narrowed search space to 414 frames\n  Event 3: Các miếng thịt sau đó được đem đi áp chảo cùng với...\n    → Found 10 results from 3 videos\n    → Narrowed search space to 301 frames\n✅ Temporal search completed!\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"OPTIONS /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting temporal search with 3 events...\n  Event 1: Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đe...\n    → Found 100 results from 78 videos\n    → Narrowed search space to 8098 frames\n  Event 2: Sau đó người này rắc bột lên những miếng thịt, tro...\n    → Found 100 results from 48 videos\n    → Narrowed search space to 5010 frames\n  Event 3: Các miếng thịt sau đó được đem đi áp chảo cùng với...\n    → Found 100 results from 17 videos\n    → Narrowed search space to 1579 frames\n✅ Temporal search completed!\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\nINFO:     14.234.51.109:0 - \"OPTIONS /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting temporal search with 3 events...\n  Event 1: Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đe...\n    → Found 10 results from 10 videos\n    → Narrowed search space to 1068 frames\n  Event 2: Sau đó người này rắc bột lên những miếng thịt, tro...\n    → Found 10 results from 4 videos\n    → Narrowed search space to 414 frames\n  Event 3: Các miếng thịt sau đó được đem đi áp chảo cùng với...\n    → Found 10 results from 3 videos\n    → Narrowed search space to 301 frames\n✅ Temporal search completed!\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n🔍 Starting temporal search with 3 events...\n  Event 1: Một người  đang cắt đôi ổ bánh mì có rắc mè rồi đe...\n    → Found 10 results from 10 videos\n    → Narrowed search space to 1068 frames\n  Event 2: Sau đó người này rắc bột lên những miếng thịt, tro...\n    → Found 10 results from 4 videos\n    → Narrowed search space to 414 frames\n  Event 3: Các miếng thịt sau đó được đem đi áp chảo cùng với...\n    → Found 10 results from 3 videos\n    → Narrowed search space to 301 frames\n✅ Temporal search completed!\nINFO:     14.234.51.109:0 - \"POST /temporal_search HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}