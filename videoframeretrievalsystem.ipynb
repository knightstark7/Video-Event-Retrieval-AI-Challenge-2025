{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fastapi uvicorn sentence-transformers open-clip-torch qdrant-client \\\n    llama-index llama-index-vector-stores-qdrant llama-index-core pyngrok -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:47:23.777867Z","iopub.execute_input":"2025-08-25T15:47:23.778410Z","iopub.status.idle":"2025-08-25T15:49:02.602771Z","shell.execute_reply.started":"2025-08-25T15:47:23.778385Z","shell.execute_reply":"2025-08-25T15:49:02.602064Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom qdrant_client import QdrantClient\nimport torch\nfrom fastapi import APIRouter\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.core import VectorStoreIndex\nfrom collections import defaultdict\nimport heapq\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nimport re\nfrom pydantic import PrivateAttr\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom typing import List\nimport open_clip\nfrom pydantic import BaseModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:49:05.470473Z","iopub.execute_input":"2025-08-25T15:49:05.471125Z","iopub.status.idle":"2025-08-25T15:49:39.315086Z","shell.execute_reply.started":"2025-08-25T15:49:05.471094Z","shell.execute_reply":"2025-08-25T15:49:39.314285Z"}},"outputs":[{"name":"stderr","text":"2025-08-25 15:49:21.953490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756136962.184429      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756136962.248683      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class CLIPEmbedding(BaseEmbedding):\n    _model = PrivateAttr()\n    _preprocess = PrivateAttr()\n    _tokenizer = PrivateAttr()\n    _device = PrivateAttr()\n\n    def __init__(self, model_name: str):\n        super().__init__()\n        self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self._model, _, self._preprocess = open_clip.create_model_and_transforms(\n            model_name='ViT-H-14-quickgelu',\n            pretrained='dfn5b',\n            device=self._device\n        )\n        self._tokenizer = open_clip.get_tokenizer('ViT-H-14-quickgelu')\n        self._model = self._model.to(self._device).eval()\n\n    def _encode_text(self, text: str) -> List[float]:\n        tokens = self._tokenizer([text]).to(self._device)\n        with torch.no_grad():\n            emb = self._model.encode_text(tokens)\n            emb = emb / emb.norm(dim=-1, keepdim=True) \n        return emb[0].cpu().numpy().tolist()\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        return self._encode_text(query)\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        return self._encode_text(text)\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        return self._get_text_embedding(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:49:54.340342Z","iopub.execute_input":"2025-08-25T15:49:54.340629Z","iopub.status.idle":"2025-08-25T15:49:54.350258Z","shell.execute_reply.started":"2025-08-25T15:49:54.340608Z","shell.execute_reply":"2025-08-25T15:49:54.349644Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class BGEEmbedding(BaseEmbedding):\n    _model: SentenceTransformer = PrivateAttr()\n\n    def __init__(self, model_name: str = \"BAAI/bge-small-en\", device: str = \"cpu\"):\n        super().__init__()\n        self._model = SentenceTransformer(model_name, device=device, trust_remote_code=True)\n        self._model = self._model.eval()\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        return self._model.encode(query).tolist()\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        return self._model.encode(text).tolist()\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        return self._get_text_embedding(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:49:57.783223Z","iopub.execute_input":"2025-08-25T15:49:57.783833Z","iopub.status.idle":"2025-08-25T15:49:57.790628Z","shell.execute_reply.started":"2025-08-25T15:49:57.783811Z","shell.execute_reply":"2025-08-25T15:49:57.789991Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Translator:\n    def __init__(self, model_name: str = \"VietAI/envit5-translation\", device: str = 'cpu'):\n        self.device = device\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n\n    def _clean_prefix(self, text: str) -> str:\n        return re.sub(r\"^(en|vi)\\s*:\\s*\", \"\", text.strip(), flags=re.IGNORECASE)\n    \n    def translate(self, text: str, source_lang: str = \"en\", max_length: int = 128) -> str:\n        content = f\"{source_lang}: {text}\"\n        inputs = self.tokenizer(\n            content, \n            return_tensors=\"pt\", \n            truncation=True, \n            max_length=max_length).to(self.device)\n        with torch.no_grad():\n            outputs = self.model.generate(**inputs, max_length=max_length)\n        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        return self._clean_prefix(decoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:50:07.921462Z","iopub.execute_input":"2025-08-25T15:50:07.922300Z","iopub.status.idle":"2025-08-25T15:50:07.928023Z","shell.execute_reply.started":"2025-08-25T15:50:07.922268Z","shell.execute_reply":"2025-08-25T15:50:07.927397Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Configuration with Kaggle secrets support\nimport os\n\n# Use Kaggle secrets or fallback to hardcoded values\nQDRANT_URL = os.getenv('QDRANT_URL', \"https://09a6d049-00c4-4b77-8e95-1dcc9ea5df34.eu-west-1-0.aws.cloud.qdrant.io:6333\")\nQDRANT_API_KEY = os.getenv('QDRANT_API_KEY', \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.-ZPZib9FxehqbTuqxsk7QdVjBQd0LlQEq7dpjF1b4PI\")\nNGROK_AUTH_TOKEN = os.getenv('NGROK_AUTH_TOKEN', \"28k6uZmtZlrKVCzyVQTfjtRSIDd_6GHtfHcwNEojEk9WjkTmv\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")\n\nqdrant_client = QdrantClient(\n    url=QDRANT_URL,\n    api_key=QDRANT_API_KEY,\n)\n\nCORS_SETTINGS = {\n    \"allow_origins\": [\"*\"],\n    \"allow_credentials\": True,\n    \"allow_methods\": [\"*\"],\n    \"allow_headers\": [\"*\"],\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:50:10.752404Z","iopub.execute_input":"2025-08-25T15:50:10.753209Z","iopub.status.idle":"2025-08-25T15:50:10.953588Z","shell.execute_reply.started":"2025-08-25T15:50:10.753183Z","shell.execute_reply":"2025-08-25T15:50:10.952908Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class SearchRequest(BaseModel):\n    query: str\n    topK: int\n    mode: str = \"hybrid\"  # \"hybrid\", \"clip\", \"vintern\"\n\nclip_embed_model = CLIPEmbedding(model_name='ViT-H-14-quickgelu')\nclip_vector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"Image\")\nclip_index = VectorStoreIndex.from_vector_store(vector_store=clip_vector_store, embed_model=clip_embed_model)\n\n# bge_embed_model = BGEEmbedding(model_name=\"AITeamVN/Vietnamese_Embedding_v2\", device=DEVICE)\nbge_embed_model = BGEEmbedding(model_name=\"dangvantuan/vietnamese-document-embedding\", device=DEVICE)\nbge_vector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"Demo\")\nbge_index = VectorStoreIndex.from_vector_store(vector_store=bge_vector_store, embed_model=bge_embed_model)\n\ntranslator = Translator(device=DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:50:13.186614Z","iopub.execute_input":"2025-08-25T15:50:13.187212Z","iopub.status.idle":"2025-08-25T15:51:22.868753Z","shell.execute_reply.started":"2025-08-25T15:50:13.187189Z","shell.execute_reply":"2025-08-25T15:51:22.867935Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"open_clip_pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a3fa8d1f4474ad1b791114fb181d78d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de0dbad4fe56465db688c518bdaef28d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/171 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d900416fcf284f1dbba72cdf75f22983"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a92a1bcbbd848bf83ab465f86552570"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15591b81f8a6492986794ae4b28feb20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95ddbe0a712640028fdd15a385e659dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b65d671b55e4f46b1a0aaa8a80cbac2"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dangvantuan/Vietnamese_impl:\n- configuration.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b0f221c896e4396915ee307433186f4"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dangvantuan/Vietnamese_impl:\n- modeling.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94b9caca134b46c7af8d5e880aa4d3a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8242eee1c174832a22bc1a008b4c307"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3272f23d88624a5cb945e8fa7e122cc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"954d57c50381492cb2b427e7a543c31b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"923846d797924e8ab6cc5f3b1069f9a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60454fb4a2964932b4d2b0880696f3a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82956054a1654b70a4433dd5aed9546f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e87c57935346441597a89b82844c8fbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f42e7d6c03e4807b77991a0cf84caaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/721 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc1766e65ed44474b5dd1038350ece74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62d17009b70c48b191036a64026912f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"133b046c68384d0e8e910649ae10a88f"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def retrieve_frame(query: str, topK: int, mode: str = \"hybrid\", alpha: float = 0.5):\n\n    \n    if mode == \"clip\":\n        # CLIP-only search\n        prompt = translator.translate(query, source_lang=\"vi\")\n        clip_nodes = clip_index.as_retriever(similarity_top_k=topK).retrieve(prompt)\n        results = [\n            {\"image\": node.metadata[\"id\"].strip(), \"caption\": f\"{node.metadata['id']} | Score: {node.score:.2f}\"}\n            for node in clip_nodes\n        ]\n        return results\n    \n    elif mode == \"vintern\":\n        # Vintern-only search (using Demo collection)\n        bge_nodes = bge_index.as_retriever(similarity_top_k=topK).retrieve(query)\n        results = [\n            {\"image\": node.metadata[\"id\"].strip(), \"caption\": f\"{node.metadata['id']} | Score: {node.score:.2f}\"}\n            for node in bge_nodes\n        ]\n        return results\n    \n    else:\n        prompt = translator.translate(query, source_lang=\"vi\")\n        # Hybrid search (original logic)\n        bge_nodes = bge_index.as_retriever(similarity_top_k=topK).retrieve(query)\n        clip_nodes = clip_index.as_retriever(similarity_top_k=topK).retrieve(prompt)\n        \n        combined_scores = defaultdict(float)\n        for node in bge_nodes:\n            combined_scores[node.metadata[\"id\"]] += node.score * alpha\n\n        for node in clip_nodes:\n            combined_scores[node.metadata[\"id\"]] += node.score * (1 - alpha)\n\n        top_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:topK]\n\n        return [\n            {\"image\": video_id.strip(), \"caption\": f\"{video_id} | Score: {score:.2f}\"}\n            for video_id, score in top_results\n        ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:51:37.770734Z","iopub.execute_input":"2025-08-25T15:51:37.771019Z","iopub.status.idle":"2025-08-25T15:51:37.778477Z","shell.execute_reply.started":"2025-08-25T15:51:37.770998Z","shell.execute_reply":"2025-08-25T15:51:37.777827Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"app = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=CORS_SETTINGS[\"allow_origins\"],\n    allow_credentials=CORS_SETTINGS[\"allow_credentials\"],\n    allow_methods=CORS_SETTINGS[\"allow_methods\"],\n    allow_headers=CORS_SETTINGS[\"allow_headers\"],\n)\n\nrouter = APIRouter()\n\n@router.post(\"/search\")\ndef api_search(req: SearchRequest):\n    results = retrieve_frame(req.query, req.topK, req.mode)\n    return {\"results\": results}\n\napp.include_router(router)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:51:41.922920Z","iopub.execute_input":"2025-08-25T15:51:41.923632Z","iopub.status.idle":"2025-08-25T15:51:41.930401Z","shell.execute_reply.started":"2025-08-25T15:51:41.923607Z","shell.execute_reply":"2025-08-25T15:51:41.929579Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import os, time, threading, socket\nfrom pyngrok import ngrok\nimport uvicorn\n\n# === Config ===\nPORT = 8000\nHOST = \"0.0.0.0\"\n\nNGROK_AUTH_TOKEN = os.getenv(\"NGROK_AUTH_TOKEN\", globals().get(\"NGROK_AUTH_TOKEN\"))\n\nif NGROK_AUTH_TOKEN:\n    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\nelse:\n    print(\"⚠️  NGROK_AUTH_TOKEN chưa được thiết lập.\")\n\ndef is_port_in_use(port: int, host=\"127.0.0.1\") -> bool:\n    \"\"\"Check if a local TCP port is already in use.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        return s.connect_ex((host, port)) == 0\n\ndef run_server():\n    \"\"\"Run FastAPI server in background thread\"\"\"\n    uvicorn.run(app, host=HOST, port=PORT, log_level=\"info\")\n\n# 1) Start server only if not already running\nif not is_port_in_use(PORT):\n    server_thread = threading.Thread(target=run_server, daemon=True)\n    server_thread.start()\n    # Đợi server khởi động\n    time.sleep(1)\nelse:\n    print(f\"🔁 Server đã chạy trên http://localhost:{PORT}, bỏ qua bước khởi động.\")\n\n# 2) Reuse/clean tunnels\nfor t in ngrok.get_tunnels():\n    addr = (t.config or {}).get(\"addr\", \"\")\n    if str(PORT) in addr:\n        try:\n            ngrok.disconnect(t.public_url)\n        except Exception:\n            pass\n\ntry:\n    if len(ngrok.get_tunnels()) >= 3:\n        ngrok.kill()\n\n    tunnel = ngrok.connect(addr=PORT, proto=\"http\", bind_tls=True)  # sẽ trả về URL https\n    PUBLIC_URL = tunnel.public_url  # https://...\n    print(f\"🌐 Public URL: {PUBLIC_URL}\")\n    print(f\"📝 API Documentation: {PUBLIC_URL}/docs\")\n    print(f\"💡 Local URL: http://localhost:{PORT}\")\n\n    # Lưu vào biến global để dùng sau\n    globals()[\"PUBLIC_URL\"] = PUBLIC_URL\n\nexcept Exception as e:\n    print(f\"❌ ngrok tunnel failed: {e}\")\n    print(f\"🔧 Falling back to local access only: http://localhost:{PORT}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:51:44.547976Z","iopub.execute_input":"2025-08-25T15:51:44.548285Z","iopub.status.idle":"2025-08-25T15:51:47.730461Z","shell.execute_reply.started":"2025-08-25T15:51:44.548264Z","shell.execute_reply":"2025-08-25T15:51:47.729577Z"}},"outputs":[{"name":"stdout","text":"                                                                                                    \r","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [36]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"🌐 Public URL: https://b25f692fb561.ngrok-free.app\n📝 API Documentation: https://b25f692fb561.ngrok-free.app/docs\n💡 Local URL: http://localhost:8000\nINFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"OPTIONS /search HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"438059e621504027baf1ff02f01f57e0"}},"metadata":{}},{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Test the API with public URL\nimport requests\nimport json\n\ndef test_search_api(query=\"con chó màu đen\", topK=10, mode=\"hybrid\", use_public_url=True):\n    \"\"\"Test the search API with public or local URL\"\"\"\n    \n    # Use public URL if available, otherwise fallback to localhost\n    if use_public_url and 'PUBLIC_URL' in globals():\n        base_url = globals()['PUBLIC_URL']\n    else:\n        base_url = \"http://localhost:8000\"\n    \n    url = f\"{base_url}/search\"\n    payload = {\n        \"query\": query,\n        \"topK\": topK,\n        \"mode\": mode\n    }\n    \n    print(f\"🔍 Testing endpoint: {url}\")\n    print(f\"🎯 Search mode: {mode}\")\n    \n    try:\n        response = requests.post(url, json=payload, timeout=30)\n        if response.status_code == 200:\n            results = response.json()\n            print(f\"✅ Search successful for query: '{query}'\")\n            print(f\"📊 Found {len(results['results'])} results:\")\n            for i, result in enumerate(results['results'][:5], 1):\n                print(f\"  {i}. {result['caption']}\")\n            return results\n        else:\n            print(f\"❌ Error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"❌ Request failed: {str(e)}\")\n        return None\n\n# Test all search modes\nprint(\"🧪 Testing different search modes...\")\n\nprint(\"\\n1. Testing HYBRID mode:\")\ntest_results = test_search_api(mode=\"hybrid\")\n\nprint(\"\\n2. Testing CLIP-only mode:\")\ntest_results = test_search_api(mode=\"clip\")\n\nprint(\"\\n3. Testing VINTERN-only mode:\")\ntest_results = test_search_api(mode=\"vintern\")\n\n# Also provide curl commands for external testing\nif 'PUBLIC_URL' in globals():\n    public_url = globals()['PUBLIC_URL']\n    print(f\"\\n🔧 External curl test commands:\")\n    print(f'# Hybrid mode:')\n    print(f'curl -X POST \"{public_url}/search\" -H \"Content-Type: application/json\" -d \\'{{\"query\": \"khung hình bóng đá\", \"topK\": 10, \"mode\": \"hybrid\"}}\\'')\n    print(f'# CLIP-only mode:')\n    print(f'curl -X POST \"{public_url}/search\" -H \"Content-Type: application/json\" -d \\'{{\"query\": \"khung hình bóng đá\", \"topK\": 10, \"mode\": \"clip\"}}\\'')\n    print(f'# Vintern-only mode:')\n    print(f'curl -X POST \"{public_url}/search\" -H \"Content-Type: application/json\" -d \\'{{\"query\": \"khung hình bóng đá\", \"topK\": 10, \"mode\": \"vintern\"}}\\'')\nelse:\n    print(\"\\n⚠️  No public URL available. Use local testing only.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T13:01:58.896165Z","iopub.execute_input":"2025-08-25T13:01:58.896464Z","iopub.status.idle":"2025-08-25T13:02:02.011954Z","shell.execute_reply.started":"2025-08-25T13:01:58.896442Z","shell.execute_reply":"2025-08-25T13:02:02.011428Z"}},"outputs":[{"name":"stdout","text":"🧪 Testing different search modes...\n\n1. Testing HYBRID mode:\n🔍 Testing endpoint: https://a4260991a951.ngrok-free.app/search\n🎯 Search mode: hybrid\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e27b370f374245e5ac3e2cd3a6afcf0a"}},"metadata":{}},{"name":"stdout","text":"INFO:     34.139.164.154:0 - \"POST /search HTTP/1.1\" 200 OK\n✅ Search successful for query: 'con chó màu đen'\n📊 Found 10 results:\n  1. L29_V016_24169 | Score: 0.39\n  2. L22_V004_38469 | Score: 0.27\n  3. L22_V009_10832 | Score: 0.26\n  4. L22_V009_18681 | Score: 0.26\n  5. L29_V016_14919 | Score: 0.26\n\n2. Testing CLIP-only mode:\n🔍 Testing endpoint: https://a4260991a951.ngrok-free.app/search\n🎯 Search mode: clip\nINFO:     34.139.164.154:0 - \"POST /search HTTP/1.1\" 200 OK\n✅ Search successful for query: 'con chó màu đen'\n📊 Found 10 results:\n  1. L29_V016_24169 | Score: 0.27\n  2. L29_V016_12645 | Score: 0.25\n  3. L29_V023_11472 | Score: 0.24\n  4. L28_V017_9496 | Score: 0.24\n  5. L29_V016_19155 | Score: 0.24\n\n3. Testing VINTERN-only mode:\n🔍 Testing endpoint: https://a4260991a951.ngrok-free.app/search\n🎯 Search mode: vintern\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abe3d1ac25e4404fafb229aa6eda9b4b"}},"metadata":{}},{"name":"stdout","text":"INFO:     34.139.164.154:0 - \"POST /search HTTP/1.1\" 200 OK\n✅ Search successful for query: 'con chó màu đen'\n📊 Found 10 results:\n  1. L22_V004_38469 | Score: 0.54\n  2. L22_V009_10832 | Score: 0.53\n  3. L22_V009_18681 | Score: 0.52\n  4. L29_V016_14919 | Score: 0.51\n  5. L29_V016_24169 | Score: 0.51\n\n🔧 External curl test commands:\n# Hybrid mode:\ncurl -X POST \"https://a4260991a951.ngrok-free.app/search\" -H \"Content-Type: application/json\" -d '{\"query\": \"khung hình bóng đá\", \"topK\": 10, \"mode\": \"hybrid\"}'\n# CLIP-only mode:\ncurl -X POST \"https://a4260991a951.ngrok-free.app/search\" -H \"Content-Type: application/json\" -d '{\"query\": \"khung hình bóng đá\", \"topK\": 10, \"mode\": \"clip\"}'\n# Vintern-only mode:\ncurl -X POST \"https://a4260991a951.ngrok-free.app/search\" -H \"Content-Type: application/json\" -d '{\"query\": \"khung hình bóng đá\", \"topK\": 10, \"mode\": \"vintern\"}'\nINFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"OPTIONS /search HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eed316e48c24f0f86e90aafec7256cd"}},"metadata":{}},{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\nINFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a86028081024c4bb64b73256658bf26"}},"metadata":{}},{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34ee8cf1831248328b33c5082e317f6b"}},"metadata":{}},{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e73ecef77cc43c9ba515b7e669c83d6"}},"metadata":{}},{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\nINFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"898e38f42688402784c73950b38e100d"}},"metadata":{}},{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"733c518c3d4343a6b095749eb07847dd"}},"metadata":{}},{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afd9cfceb0bc40c0856caf08d837eba7"}},"metadata":{}},{"name":"stdout","text":"INFO:     2405:4802:93d3:fce0:c145:8747:88b3:25d6:0 - \"POST /search HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}